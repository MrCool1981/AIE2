,question,contexts,ground_truth,evolution_type,metadata,episode_done
0,What is the purpose of implementing a use-case based supplier risk assessment framework in relation to content provenance standards?,"[""\\begin{center}\n\\begin{tabular}{|c|c|c|}\n\\hline\nGV-6.1-005 & \\begin{tabular}{l}\nImplement a use-cased based supplier risk assessment framework to evaluate and \\\\\nmonitor third-party entities' performance and adherence to content provenance \\\\\nstandards and technologies to detect anomalies and unauthorized changes; \\\\\nservices acquisition and value chain risk management; and legal compliance. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nData Privacy; Information \\\\\nIntegrity; Information Security; \\\\\nIntellectual Property; Value Chain \\\\\nand Component Integration \\\\\n\\end{tabular} \\\\\n\\hline\nGV-6.1-006 & \\begin{tabular}{l}\nInclude clauses in contracts which allow an organization to evaluate third-party \\\\\nGAI processes and standards. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nGV-6.1-007 & \\begin{tabular}{l}\nInventory all third-party entities with access to organizational content and \\\\\nestablish approved GAI technology and service provider lists. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration \\\\\n\\end{tabular} \\\\\n\\hline\nGV-6.1-008 & \\begin{tabular}{l}\nMaintain records of changes to content made by third parties to promote content \\\\\nprovenance, including sources, timestamps, metadata. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Value Chain \\\\\nand Component Integration; \\\\\nIntellectual Property \\\\\n\\end{tabular} \\\\\n\\hline\nGV-6.1-009 & \\begin{tabular}{l}\nUpdate and integrate due diligence processes for GAI acquisition and \\\\\nprocurement vendor assessments to include intellectual property, data privacy, \\\\\nsecurity, and other risks. For example, update processes to: Address solutions that \\\\\nmay rely on embedded GAI technologies; Address ongoing monitoring, \\\\\nassessments, and alerting, dynamic risk assessments, and real-time reporting \\\\\ntools for monitoring third-party GAI risks; Consider policy adjustments across GAI \\\\\nmodeling libraries, tools and APIs, fine-tuned models, and embedded tools; \\\\""]",The purpose of implementing a use-case based supplier risk assessment framework is to evaluate and monitor third-party entities' performance and adherence to content provenance standards and technologies to detect anomalies and unauthorized changes.,simple,[{'source': 'data/nist_ai.tex'}],True
1,What actions are suggested to address the risks of bias and homogenization in AI-generated content?,"['Al Actor Tasks: Al Impact Assessment, Domain Experts, Operation and Monitoring, TEVV\n\nMEASURE 3.3: Feedback processes for end users and impacted communities to report problems and appeal system outcomes are established and integrated into Al system evaluation metrics.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-3.3-001 & \\begin{tabular}{l}\nConduct impact assessments on how Al-generated content might affect \\\\\ndifferent social, economic, and cultural groups. \\\\\n\\end{tabular} & Harmful Bias and Homogenization \\\\\n\\hline\nMS-3.3-002 & \\begin{tabular}{l}\nConduct studies to understand how end users perceive and interact with GAI \\\\\ncontent and accompanying content provenance within context of use. Assess \\\\\nwhether the content aligns with their expectations and how they may act upon \\\\\nthe information presented. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-Al Configuration; \\\\\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-3.3-003 & \\begin{tabular}{l}\nEvaluate potential biases and stereotypes that could emerge from the Al- \\\\\ngenerated content using appropriate methodologies including computational \\\\\ntesting methods as well as evaluating structured feedback input. \\\\\n\\end{tabular} & Harmful Bias and Homogenization \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","The suggested actions to address the risks of bias and homogenization in AI-generated content include: 1. Conducting impact assessments on how AI-generated content might affect different social, economic, and cultural groups (Action ID: MS-3.3-001). 2. Evaluating potential biases and stereotypes that could emerge from the AI-generated content using appropriate methodologies including computational testing methods as well as evaluating structured feedback input (Action ID: MS-3.3-003).",simple,[{'source': 'data/nist_ai.tex'}],True
2,How are AI systems resourced according to organizational risk priorities?,"[""Al Actor Tasks: Governance and Oversight, Operation and Monitoring\n\nGOVERN 1.6: Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities.\n\n\\begin{center}\n\\begin{tabular}{|c|c|c|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nGV-1.6-001 & \\begin{tabular}{l}\nEnumerate organizational GAI systems for incorporation into AI system inventory \\\\\nand adjust AI system inventory requirements to account for GAI risks. \\\\\n\\end{tabular} & Information Security \\\\\n\\hline\nGV-1.6-002 & \\begin{tabular}{l}\nDefine any inventory exemptions in organizational policies for GAI systems \\\\\nembedded into application software. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration \\\\\n\\end{tabular} \\\\\n\\hline\nGV-1.6-003 & \\begin{tabular}{l}\nIn addition to general model, governance, and risk information, consider the \\\\\nfollowing items in GAI system inventory entries: Data provenance information \\\\\n(e.g., source, signatures, versioning, watermarks); Known issues reported from \\\\\ninternal bug tracking or external information sharing resources (e.g., Al incident \\\\\ndatabase, AVID, CVE, NVD, or OECD AI incident monitor); Human oversight roles \\\\\nand responsibilities; Special rights and considerations for intellectual property, \\\\\nlicensed works, or personal, privileged, proprietary or sensitive data; Underlying \\\\\nfoundation models, versions of underlying models, and access modes. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nData Privacy; Human-AI \\\\\nConfiguration; Information \\\\\nIntegrity; Intellectual Property; \\\\\nValue Chain and Component \\\\\nIntegration \\\\\n\\end{tabular} \\\\\n\\hline\n\\multicolumn{3}{|l|}{AI Actor Tasks: Governance and Oversight} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nGOVERN 1.7: Processes and procedures are in place for decommissioning and phasing out Al systems safely and in a manner that does not increase risks or decrease the organization's trustworthiness.""]",Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities.,simple,[{'source': 'data/nist_ai.tex'}],True
3,What are the risks associated with integrating GAI into applications involving consequential decision making?,"[""Risks from confabulations may arise when users believe false content - often due to the confident nature of the response - leading users to act upon or promote the false information. This poses a challenge for many real-world applications, such as in healthcare, where a confabulated summary of patient information reports could cause doctors to make incorrect diagnoses and/or recommend the wrong treatments. Risks of confabulated content may be especially important to monitor when integrating GAI into applications involving consequential decision making.\n\nGAI outputs may also include confabulated logic or citations that purport to justify or explain the system's answer, which may further mislead humans into inappropriately trusting the system's output. For instance, LLMs sometimes provide logical steps for how they arrived at an answer even when the answer itself is incorrect. Similarly, an LLM could falsely assert that it is human or has human traits, potentially deceiving humans into believing they are speaking with another human.\n\nThe extent to which humans can be deceived by LLMs, the mechanisms by which this may occur, and the potential risks from adversarial prompting of such behavior are emerging areas of study. Given the wide range of downstream impacts of GAI, it is difficult to estimate the downstream scale and impact of confabulations.\n\nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Valid and Reliable, Explainable and Interpretable\n\\subsection*{2.3. Dangerous, Violent, or Hateful Content}\nGAI systems can produce content that is inciting, radicalizing, or threatening, or that glorifies violence, with greater ease and scale than other technologies. LLMs have been reported to generate dangerous or violent recommendations, and some models have generated actionable instructions for dangerous or""]","Risks from confabulations may arise when users believe false content, leading users to act upon or promote the false information. This poses a challenge for many real-world applications, such as in healthcare, where a confabulated summary of patient information reports could cause doctors to make incorrect diagnoses and/or recommend the wrong treatments. Risks of confabulated content may be especially important to monitor when integrating GAI into applications involving consequential decision making.",simple,[{'source': 'data/nist_ai.tex'}],True
4,What actions are suggested to address data privacy risks in AI-generated content?,"['Security; Harmful Bias and \\\\\nHomogenization; Dangerous, \\\\\nViolent, or Hateful Content; Data \\\\\nPrivacy \\\\\n\\end{tabular} \\\\\n\\hline\nMP-4.1-006 & \\begin{tabular}{l}\nImplement policies and practices defining how third-party intellectual property and \\\\\ntraining data will be used, stored, and protected. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nIntellectual Property; Value Chain \\\\\nand Component Integration \\\\\n\\end{tabular} \\\\\n\\hline\nMP-4.1-007 & \\begin{tabular}{l}\nRe-evaluate models that were fine-tuned or enhanced on top of third-party \\\\\nmodels. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration \\\\\n\\end{tabular} \\\\\n\\hline\nMP-4.1-008 & \\begin{tabular}{l}\nRe-evaluate risks when adapting GAI models to new domains. Additionally, \\\\\nestablish warning systems to determine if a GAI system is being used in a new \\\\\ndomain where previous assumptions (relating to context of use or mapped risks \\\\\nsuch as security, and safety) may no longer hold. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nIntellectual Property; Harmful Bias \\\\\nand Homogenization; Dangerous, \\\\\nViolent, or Hateful Content; Data \\\\\nPrivacy \\\\\n\\end{tabular} \\\\\n\\hline\nMP-4.1-009 & \\begin{tabular}{l}\nLeverage approaches to detect the presence of PII or sensitive data in generated \\\\\noutput text, image, video, or audio. \\\\\n\\end{tabular} & Data Privacy \\\\\n\\hline\n\\end{tabular}\n\\end{center}', '\\begin{center}\n\\begin{tabular}{|c|c|c|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMP-4.1-001 & \\begin{tabular}{l}\nConduct periodic monitoring of AI-generated content for privacy risks; address any \\\\\npossible instances of PII or sensitive data exposure. \\\\\n\\end{tabular} & Data Privacy \\\\\n\\hline\nMP-4.1-002 & \\begin{tabular}{l}\nImplement processes for responding to potential intellectual property infringement \\\\\nclaims or other rights. \\\\\n\\end{tabular} & Intellectual Property \\\\\n\\hline\nMP-4.1-003 & \\begin{tabular}{l}\nConnect new GAI policies, procedures, and processes to existing model, data, \\\\\nsoftware development, and IT governance and to legal, compliance, and risk \\\\\nmanagement activities. \\\\\n\\end{tabular} & Information Security; Data Privacy \\\\\n\\hline\nMP-4.1-004 & \\begin{tabular}{l}\nDocument training data curation policies, to the extent possible and according to \\\\\napplicable laws and policies. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nIntellectual Property; Data Privacy; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content \\\\\n\\end{tabular} \\\\\n\\hline\nMP-4.1-005 & \\begin{tabular}{l}\nEstablish policies for collection, retention, and minimum quality of data, in \\\\\nconsideration of the following risks: Disclosure of inappropriate CBRN information; \\\\\nUse of Illegal or dangerous content; Offensive cyber capabilities; Training data \\\\\nimbalances that could give rise to harmful biases; Leak of personally identifiable \\\\\ninformation, including facial likenesses of individuals. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nIntellectual Property; Information \\\\\nSecurity; Harmful Bias and \\\\\nHomogenization; Dangerous, \\\\\nViolent, or Hateful Content; Data \\\\\nPrivacy \\\\\n\\end{tabular} \\\\\n\\hline\nMP-4.1-006 & \\begin{tabular}{l}\nImplement policies and practices defining how third-party intellectual property and \\\\\ntraining data will be used, stored, and protected. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nIntellectual Property; Value Chain \\\\']","Conduct periodic monitoring of AI-generated content for privacy risks; address any possible instances of PII or sensitive data exposure. Leverage approaches to detect the presence of PII or sensitive data in generated output text, image, video, or audio.",simple,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
5,What is the purpose of evaluating feedback loops between GAI system content provenance and human reviewers?,"['MANAGE 2.2: Mechanisms are in place and applied to sustain the value of deployed Al systems.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMG-2.2-001 & \\begin{tabular}{l}\nCompare GAI system outputs against pre-defined organization risk tolerance, \\\\\nguidelines, and principles, and review and test Al-generated content against \\\\\nthese guidelines. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content; Harmful Bias and \\\\\nHomogenization; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\nMG-2.2-002 & \\begin{tabular}{l}\nDocument training data sources to trace the origin and provenance of AI- \\\\\ngenerated content. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMG-2.2-003 & \\begin{tabular}{l}\nEvaluate feedback loops between GAI system content provenance and human \\\\\nreviewers, and update where needed. Implement real-time monitoring systems \\\\\nto affirm that content provenance protocols remain effective. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMG-2.2-004 & \\begin{tabular}{l}\nEvaluate GAI content and data for representational biases and employ \\\\\ntechniques such as re-sampling, re-ranking, or adversarial training to mitigate \\\\\nbiases in the generated content. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Security; Harmful Bias \\\\\nand Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\n & \\begin{tabular}{l}\nEngage in due diligence to analyze GAI output for harmful content, potential \\\\\nmisinformation, and CBRN-related or NCII content. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content; Harmful Bias and \\\\\nHomogenization; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}']",The purpose of evaluating feedback loops between GAI system content provenance and human reviewers is to update where needed and implement real-time monitoring systems to affirm that content provenance protocols remain effective.,simple,[{'source': 'data/nist_ai.tex'}],True
6,How might organizational governance need to be adjusted for generative AI systems?,"[""Acknowledgments: These considerations could not have been surfaced without the helpful analysis and contributions from the community and NIST staff GAI PWG leads: George Awad, Luca Belli, Harold Booth, Mat Heyman, Yooyoung Lee, Mark Pryzbocki, Reva Schwartz, Martin Stanley, and Kyra Yee.\n\\section*{A.1. Governance}\n\\section*{A.1.1. Overview}\nLike any other technology system, governance principles and techniques can be used to manage risks related to generative AI models, capabilities, and applications. Organizations may choose to apply their existing risk tiering to GAI systems, or they may opt to revise or update Al system risk levels to address these unique GAI risks. This section describes how organizational governance regimes may be reevaluated and adjusted for GAI contexts. It also addresses third-party considerations for governing across the Al value chain.\n\\section*{A.1.2. Organizational Governance}\nGAI opportunities, risks and long-term performance characteristics are typically less well-understood than non-generative Al tools and may be perceived and acted upon by humans in ways that vary greatly. Accordingly, GAI may call for different levels of oversight from AI Actors or different human-AI configurations in order to manage their risks effectively. Organizations' use of GAl systems may also warrant additional human review, tracking and documentation, and greater management oversight.\n\nAl technology can produce varied outputs in multiple modalities and present many classes of user interfaces. This leads to a broader set of AI Actors interacting with GAI systems for widely differing applications and contexts of use. These can include data labeling and preparation, development of GAI models, content moderation, code generation and review, text generation and editing, image and video generation, summarization, search, and chat. These activities can take place within organizational settings or in the public domain.""]","Organizational governance regimes may need to be reevaluated and adjusted for GAI contexts. This includes applying existing risk tiering to GAI systems or revising AI system risk levels to address unique GAI risks. It also involves different levels of oversight from AI Actors or different human-AI configurations, additional human review, tracking and documentation, and greater management oversight.",simple,[{'source': 'data/nist_ai.tex'}],True
7,What measures should be implemented to prevent or flag outputs that reproduce particular training data?,"['\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration \\\\\n\\end{tabular} \\\\\n\\hline\nMG-3.1-004 & \\begin{tabular}{l}\nTake reasonable measures to review training data for CBRN information, and \\\\\nintellectual property, and where appropriate, remove it. Implement reasonable \\\\\nmeasures to prevent, flag, or take other action in response to outputs that \\\\\nreproduce particular training data (e.g., plagiarized, trademarked, patented, \\\\\nlicensed content or trade secret material). \\\\\n\\end{tabular} & \\begin{tabular}{l}\nIntellectual Property; CBRN \\\\\nInformation or Capabilities \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","Reasonable measures should be implemented to prevent, flag, or take other action in response to outputs that reproduce particular training data, such as plagiarized, trademarked, patented, licensed content, or trade secret material.",simple,[{'source': 'data/nist_ai.tex'}],True
8,What are the primary information security risks associated with GAI-based systems?,"['Trustworthy AI Characteristics: Accountable and Transparent, Safe, Valid and Reliable, Interpretable and Explainable\n\\subsection*{2.9. Information Security}\nInformation security for computer systems and data is a mature field with widely accepted and standardized practices for offensive and defensive cyber capabilities. GAI-based systems present two primary information security risks: GAI could potentially discover or enable new cybersecurity risks by lowering the barriers for or easing automated exercise of offensive capabilities; simultaneously, it expands the available attack surface, as GAI itself is vulnerable to attacks like prompt injection or data poisoning.\n\nOffensive cyber capabilities advanced by GAI systems may augment cybersecurity attacks such as hacking, malware, and phishing. Reports have indicated that LLMs are already able to discover some vulnerabilities in systems (hardware, software, data) and write code to exploit them. Sophisticated threat actors might further these risks by developing GAI-powered security co-pilots for use in several parts of the attack chain, including informing attackers on how to proactively evade threat detection and escalate privileges after gaining system access.\n\nInformation security for GAI models and systems also includes maintaining availability of the GAI system and the integrity and (when applicable) the confidentiality of the GAI code, training data, and model weights. To identify and secure potential attack points in Al systems or specific components of the AI\\\\\n${ }^{12}$ See also \\href{https://doi.org/10.6028/NIST.AI.100-4}{https://doi.org/10.6028/NIST.AI.100-4}, to be published.\\\\\nvalue chain (e.g., data inputs, processing, GAI training, or deployment environments), conventional cybersecurity practices may need to adapt or evolve.']","GAI-based systems present two primary information security risks: they could potentially discover or enable new cybersecurity risks by lowering the barriers for or easing automated exercise of offensive capabilities; simultaneously, they expand the available attack surface, as GAI itself is vulnerable to attacks like prompt injection or data poisoning.",simple,[{'source': 'data/nist_ai.tex'}],True
9,What is confabulation in the context of GAI systems?,"['While some of these described capabilities lie beyond the reach of existing GAI tools, ongoing assessments of this risk would be enhanced by monitoring both the ability of AI tools to facilitate CBRN weapons planning and GAI systems\' connection or access to relevant data and tools.\n\nTrustworthy AI Characteristic: Safe, Explainable and Interpretable\n\\subsection*{2.2. Confabulation}\n""Confabulation"" refers to a phenomenon in which GAI systems generate and confidently present erroneous or false content in response to prompts. Confabulations also include generated outputs that diverge from the prompts or other input or that contradict previously generated statements in the same context. These phenomena are colloquially also referred to as ""hallucinations"" or ""fabrications.""\n\nConfabulations can occur across GAI outputs and contexts. ${ }^{9,10}$ Confabulations are a natural result of the way generative models are designed: they generate outputs that approximate the statistical distribution of their training data; for example, LLMs predict the next token or word in a sentence or phrase. While such statistical prediction can produce factually accurate and consistent outputs, it can also produce outputs that are factually inaccurate or internally inconsistent. This dynamic is particularly relevant when it comes to open-ended prompts for long-form responses and in domains which require highly contextual and/or domain expertise.']","""Confabulation"" refers to a phenomenon in which GAI systems generate and confidently present erroneous or false content in response to prompts. Confabulations also include generated outputs that diverge from the prompts or other input or that contradict previously generated statements in the same context. These phenomena are colloquially also referred to as ""hallucinations"" or ""fabrications.""",simple,[{'source': 'data/nist_ai.tex'}],True
10,"What is the purpose of collaborating with external researchers, industry experts, and community representatives in the context of managing identified risks?","['\\begin{center}\n\\begin{tabular}{|c|c|c|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMG-4.1-001 & \\begin{tabular}{l}\nCollaborate with external researchers, industry experts, and community \\\\\nrepresentatives to maintain awareness of emerging best practices and \\\\\ntechnologies in measuring and managing identified risks. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Harmful Bias \\\\\nand Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMG-4.1-002 & \\begin{tabular}{l}\nEstablish, maintain, and evaluate effectiveness of organizational processes and \\\\\nprocedures for post-deployment monitoring of GAI systems, particularly for \\\\\npotential confabulation, CBRN, or cyber risks. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nConfabulation; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMG-4.1-003 & \\begin{tabular}{l}\nEvaluate the use of sentiment analysis to gauge user sentiment regarding GAI \\\\\ncontent performance and impact, and work in collaboration with AI Actors \\\\\nexperienced in user research and experience. \\\\\n\\end{tabular} & Human-Al Configuration \\\\\n\\hline\nMG-4.1-004 & \\begin{tabular}{l}\nImplement active learning techniques to identify instances where the model fails \\\\\nor produces unexpected outputs. \\\\\n\\end{tabular} & Confabulation \\\\\n\\hline\nMG-4.1-005 & \\begin{tabular}{l}\nShare transparency reports with internal and external stakeholders that detail \\\\\nsteps taken to update the GAI system to enhance transparency and \\\\\naccountability. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; Harmful \\\\\nBias and Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMG-4.1-006 & \\begin{tabular}{l}\nTrack dataset modifications for provenance by monitoring data deletions, \\\\\nrectification requests, and other changes that may impact the verifiability of \\\\\ncontent origins. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","The purpose of collaborating with external researchers, industry experts, and community representatives is to maintain awareness of emerging best practices and technologies in measuring and managing identified risks.",simple,[{'source': 'data/nist_ai.tex'}],True
11,What risks are associated with GAI models in relation to CSAM?,"['CSAM. Even when trained on ""clean"" data, increasingly capable GAI models can synthesize or produce synthetic NCII and CSAM. Websites, mobile apps, and custom-built models that generate synthetic NCII have moved from niche internet forums to mainstream, automated, and scaled online businesses.\n\nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Privacy Enhanced\n\\subsection*{2.12. Value Chain and Component Integration}\nGAI value chains involve many third-party components such as procured datasets, pre-trained models, and software libraries. These components might be improperly obtained or not properly vetted, leading to diminished transparency or accountability for downstream users. While this is a risk for traditional AI systems and some other digital technologies, the risk is exacerbated for GAI due to the scale of the training data, which may be too large for humans to vet; the difficulty of training foundation models, which leads to extensive reuse of limited numbers of models; and the extent to which GAI may be integrated into other devices and services. As GAI systems often involve many distinct third-party components and data sources, it may be difficult to attribute issues in a system\'s behavior to any one of these sources.\n\nErrors in third-party GAI components can also have downstream impacts on accuracy and robustness. For example, test datasets commonly used to benchmark or validate models can contain label errors. Inaccuracies in these labels can impact the ""stability"" or robustness of these benchmarks, which many GAI practitioners consider during the model selection process.']","GAI models, even when trained on 'clean' data, can synthesize or produce synthetic NCII and CSAM. Websites, mobile apps, and custom-built models that generate synthetic NCII have moved from niche internet forums to mainstream, automated, and scaled online businesses.",simple,[{'source': 'data/nist_ai.tex'}],True
12,What policies and procedures should be established for the continuous monitoring of third-party GAI systems in deployment?,"['\\begin{center}\n\\begin{tabular}{|c|c|c|}\n\\hline\nGV-6.2-003 & \\begin{tabular}{l}\nEstablish incident response plans for third-party GAI technologies: Align incident \\\\\nresponse plans with impacts enumerated in MAP 5.1; Communicate third-party \\\\\nGAI incident response plans to all relevant AI Actors; Define ownership of GAI \\\\\nincident response functions; Rehearse third-party GAI incident response plans at \\\\\na regular cadence; Improve incident response plans based on retrospective \\\\\nlearning; Review incident response plans for alignment with relevant breach \\\\\nreporting, data protection, data privacy, or other laws. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nData Privacy; Human-AI \\\\\nConfiguration; Information \\\\\nSecurity; Value Chain and \\\\\nComponent Integration; Harmful \\\\\nBias and Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\nGV-6.2-004 & \\begin{tabular}{l}\nEstablish policies and procedures for continuous monitoring of third-party GAI \\\\\nsystems in deployment. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration \\\\\n\\end{tabular} \\\\\n\\hline\nGV-6.2-005 & \\begin{tabular}{l}\nEstablish policies and procedures that address GAI data redundancy, including \\\\\nmodel weights and other system artifacts. \\\\\n\\end{tabular} & Harmful Bias and Homogenization \\\\\n\\hline\nGV-6.2-006 & \\begin{tabular}{l}\nEstablish policies and procedures to test and manage risks related to rollover and \\\\\nfallback technologies for GAI systems, acknowledging that rollover and fallback \\\\\nmay include manual processing. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nGV-6.2-007 & \\begin{tabular}{l}\nReview vendor contracts and avoid arbitrary or capricious termination of critical \\\\\nGAI technologies or vendor services and non-standard terms that may amplify or \\\\\ndefer liability in unexpected ways and/or contribute to unauthorized data \\\\\ncollection by vendors or third-parties (e.g., secondary data use). Consider: Clear \\\\']",Policies and procedures should be established for the continuous monitoring of third-party GAI systems in deployment.,simple,[{'source': 'data/nist_ai.tex'}],True
13,What measures are suggested to address security concerns when adapting GAI models to new domains?,"['\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nMP-4.1-010 & \\begin{tabular}{l}\nConduct appropriate diligence on training data use to assess intellectual property, \\\\\nand privacy, risks, including to examine whether use of proprietary or sensitive \\\\\ntraining data is consistent with applicable laws. \\\\\n\\end{tabular} & Intellectual Property; Data Privacy \\\\\n\\hline\nAI Actor Tasks: Governance and Oversight, Operation and Monitoring, Procurement, Third-party entities &  &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nMAP 5.1: Likelihood and magnitude of each identified impact (both potentially beneficial and harmful) based on expected use, past uses of Al systems in similar contexts, public incident reports, feedback from those external to the team that developed or deployed the Al system, or other data are identified and documented.', 'Security; Harmful Bias and \\\\\nHomogenization; Dangerous, \\\\\nViolent, or Hateful Content; Data \\\\\nPrivacy \\\\\n\\end{tabular} \\\\\n\\hline\nMP-4.1-006 & \\begin{tabular}{l}\nImplement policies and practices defining how third-party intellectual property and \\\\\ntraining data will be used, stored, and protected. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nIntellectual Property; Value Chain \\\\\nand Component Integration \\\\\n\\end{tabular} \\\\\n\\hline\nMP-4.1-007 & \\begin{tabular}{l}\nRe-evaluate models that were fine-tuned or enhanced on top of third-party \\\\\nmodels. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration \\\\\n\\end{tabular} \\\\\n\\hline\nMP-4.1-008 & \\begin{tabular}{l}\nRe-evaluate risks when adapting GAI models to new domains. Additionally, \\\\\nestablish warning systems to determine if a GAI system is being used in a new \\\\\ndomain where previous assumptions (relating to context of use or mapped risks \\\\\nsuch as security, and safety) may no longer hold. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nIntellectual Property; Harmful Bias \\\\\nand Homogenization; Dangerous, \\\\\nViolent, or Hateful Content; Data \\\\\nPrivacy \\\\\n\\end{tabular} \\\\\n\\hline\nMP-4.1-009 & \\begin{tabular}{l}\nLeverage approaches to detect the presence of PII or sensitive data in generated \\\\\noutput text, image, video, or audio. \\\\\n\\end{tabular} & Data Privacy \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","Establish warning systems to determine if a GAI system is being used in a new domain where previous assumptions (relating to context of use or mapped risks such as security, and safety) may no longer hold.",simple,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
14,"What processes are defined, assessed, and documented to ensure AI system performance and trustworthiness?","['MAP 3.4: Processes for operator and practitioner proficiency with Al system performance and trustworthiness - and relevant technical standards and certifications - are defined, assessed, and documented.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMP-3.4-001 & \\begin{tabular}{l}\nEvaluate whether GAI operators and end-users can accurately understand \\\\\ncontent lineage and origin. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMP-3.4-002 & \\begin{tabular}{l}\nAdapt existing training programs to include modules on digital content \\\\\ntransparency. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMP-3.4-003 & \\begin{tabular}{l}\nDevelop certification programs that test proficiency in managing GAI risks and \\\\\ninterpreting content provenance, relevant to specific industry and context. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMP-3.4-004 & Delineate human proficiency tests from tests of GAI capabilities. & Human-AI Configuration \\\\\n\\hline\nMP-3.4-005 & \\begin{tabular}{l}\nImplement systems to continually monitor and track the outcomes of human-GAI \\\\\nconfigurations for future refinement and improvements. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMP-3.4-006 & \\begin{tabular}{l}\nInvolve the end-users, practitioners, and operators in GAI system in prototyping \\\\\nand testing activities. Make sure these tests cover various scenarios, such as crisis \\\\\nsituations or ethically sensitive contexts. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Integrity; Harmful Bias \\\\\nand Homogenization; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","Processes for operator and practitioner proficiency with AI system performance and trustworthiness - and relevant technical standards and certifications - are defined, assessed, and documented.",simple,[{'source': 'data/nist_ai.tex'}],True
15,Why is it important for AI red teams to be demographically and interdisciplinarily diverse?,"['Organizations may also collect feedback on outcomes, harms, and user experience directly from users in the production environment after a model has been released, in accordance with human subject standards such as informed consent and compensation. Organizations should follow applicable human subjects research requirements, and best practices such as informed consent and subject compensation, when implementing feedback activities.\n\\section*{Al Red-teaming}\nAl red-teaming is an evolving practice that references exercises often conducted in a controlled environment and in collaboration with AI developers building AI models to identify potential adverse behavior or outcomes of a GAI model or system, how they could occur, and stress test safeguards"". AI red-teaming can be performed before or after AI models or systems are made available to the broader public; this section focuses on red-teaming in pre-deployment contexts.\n\nThe quality of AI red-teaming outputs is related to the background and expertise of the AI red team itself. Demographically and interdisciplinarily diverse AI red teams can be used to identify flaws in the varying contexts where GAI will be used. For best results, Al red teams should demonstrate domain expertise, and awareness of socio-cultural aspects within the deployment context. Al red-teaming results should be given additional analysis before they are incorporated into organizational governance and decision making, policy and procedural updates, and Al risk management efforts.']",Demographically and interdisciplinarily diverse AI red teams can be used to identify flaws in the varying contexts where GAI will be used.,simple,[{'source': 'data/nist_ai.tex'}],True
16,What actions are suggested to address AI risks related to intellectual property?,"[""Al Actor Tasks: AI Design, Al Impact Assessment, Affected Individuals and Communities, Governance and Oversight\n\nGOVERN 6.1: Policies and procedures are in place that address AI risks associated with third-party entities, including risks of infringement of a third-party's intellectual property or other rights.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nGV-6.1-001 & \\begin{tabular}{l}\nCategorize different types of GAI content with associated third-party rights (e.g., \\\\\ncopyright, intellectual property, data privacy). \\\\\n\\end{tabular} & \\begin{tabular}{l}\nData Privacy; Intellectual \\\\\nProperty; Value Chain and \\\\\nComponent Integration \\\\\n\\end{tabular} \\\\\n\\hline\nGV-6.1-002 & \\begin{tabular}{l}\nConduct joint educational activities and events in collaboration with third parties \\\\\nto promote best practices for managing GAI risks. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration \\\\\n\\end{tabular} \\\\\n\\hline\nGV-6.1-003 & \\begin{tabular}{l}\nDevelop and validate approaches for measuring the success of content \\\\\nprovenance management efforts with third parties (e.g., incidents detected and \\\\\nresponse times). \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Value Chain \\\\\nand Component Integration \\\\\n\\end{tabular} \\\\\n\\hline\nGV-6.1-004 & \\begin{tabular}{l}\nDraft and maintain well-defined contracts and service level agreements (SLAs) \\\\\nthat specify content ownership, usage rights, quality standards, security \\\\\nrequirements, and content provenance expectations for GAI systems. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity; Intellectual Property \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}""]","The suggested actions to address AI risks related to intellectual property include: 1. Categorizing different types of GAI content with associated third-party rights (e.g., copyright, intellectual property, data privacy). 2. Drafting and maintaining well-defined contracts and service level agreements (SLAs) that specify content ownership, usage rights, quality standards, security requirements, and content provenance expectations for GAI systems.",simple,[{'source': 'data/nist_ai.tex'}],True
17,"What policies and mechanisms should be established to prevent GAI systems from generating CSAM, NCII, or content that violates the law?","['GOVERN 1.4: The risk management process and its outcomes are established through transparent policies, procedures, and other controls based on organizational risk priorities.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nGV-1.4-001 & \\begin{tabular}{l}\nEstablish policies and mechanisms to prevent GAI systems from generating \\\\\nCSAM, NCII or content that violates the law. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nObscene, Degrading, and/or \\\\\nAbusive Content; Harmful Bias \\\\\nand Homogenization; \\\\\nDangerous, Violent, or Hateful \\\\\nContent \\\\\n\\end{tabular} \\\\\n\\hline\nGV-1.4-002 & \\begin{tabular}{l}\nEstablish transparent acceptable use policies for GAI that address illegal use or \\\\\napplications of GAI. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or \\\\\nCapabilities; Obscene, \\\\\nDegrading, and/or Abusive \\\\\nContent; Data Privacy; Civil \\\\\nRights violations \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nAI Actor Tasks: AI Development, AI Deployment, Governance and Oversight\n\nGOVERN 1.5: Ongoing monitoring and periodic review of the risk management process and its outcomes are planned, and organizational roles and responsibilities are clearly defined, including determining the frequency of periodic review.']","Policies and mechanisms should be established to prevent GAI systems from generating CSAM, NCII, or content that violates the law.",simple,[{'source': 'data/nist_ai.tex'}],True
18,What is the purpose of implementing interpretability and explainability methods in evaluating GAI system decisions?,"['\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-4.2-001 & \\begin{tabular}{l}\nConduct adversarial testing at a regular cadence to map and measure GAI risks, \\\\\nincluding tests to address attempts to deceive or manipulate the application of \\\\\nprovenance techniques or other misuses. Identify vulnerabilities and \\\\\nunderstand potential misuse scenarios and unintended outputs. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-002 & \\begin{tabular}{l}\nEvaluate GAI system performance in real-world scenarios to observe its \\\\\nbehavior in practical environments and reveal issues that might not surface in \\\\\ncontrolled and optimized testing environments. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nConfabulation; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-003 & \\begin{tabular}{l}\nImplement interpretability and explainability methods to evaluate GAI system \\\\\ndecisions and verify alignment with intended purpose. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Harmful Bias \\\\\nand Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-004 & \\begin{tabular}{l}\nMonitor and document instances where human operators or other systems \\\\\noverride the GAI\'s decisions. Evaluate these cases to understand if the overrides \\\\\nare linked to issues related to content provenance. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-005 & \\begin{tabular}{l}\nVerify and document the incorporation of results of structured public feedback \\\\\nexercises into design, implementation, deployment approval (""go""/""no-go"" \\\\\ndecisions), monitoring, and decommission decisions. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\nAI Actor Tasks: Al Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV &  &  \\\\\n\\hline\n\\end{tabular}']",The purpose of implementing interpretability and explainability methods is to evaluate GAI system decisions and verify alignment with intended purpose.,simple,[{'source': 'data/nist_ai.tex'}],True
19,How can field testing help in evaluating AI system risks and impacts?,"['\\item Al Red-teaming: A structured testing exercise used to probe an AI system to find flaws and vulnerabilities such as inaccurate, harmful, or discriminatory outputs, often in a controlled environment and in collaboration with system developers.\n\\end{itemize}\nInformation gathered from structured public feedback can inform design, implementation, deployment approval, maintenance, or decommissioning decisions. Results and insights gleaned from these exercises can serve multiple purposes, including improving data quality and preprocessing, bolstering governance decision making, and enhancing system documentation and debugging practices. When implementing feedback activities, organizations should follow human subjects research requirements and best practices such as informed consent and subject compensation.\n\\section*{Participatory Engagement Methods}\nOn an ad hoc or more structured basis, organizations can design and use a variety of channels to engage external stakeholders in product development or review. Focus groups with select experts can provide feedback on a range of issues. Small user studies can provide feedback from representative groups or populations. Anonymous surveys can be used to poll or gauge reactions to specific features. Participatory engagement methods are often less structured than field testing or red teaming, and are more commonly used in early stages of AI or product development.\n\\section*{Field Testing}\nField testing involves structured settings to evaluate risks and impacts and to simulate the conditions under which the GAI system will be deployed. Field style tests can be adapted from a focus on user preferences and experiences towards Al risks and impacts - both negative and positive. When carried out with large groups of users, these tests can provide estimations of the likelihood of risks and impacts in real world interactions.']","Field testing involves structured settings to evaluate risks and impacts and to simulate the conditions under which the GAI system will be deployed. Field style tests can be adapted from a focus on user preferences and experiences towards AI risks and impacts - both negative and positive. When carried out with large groups of users, these tests can provide estimations of the likelihood of risks and impacts in real world interactions.",simple,[{'source': 'data/nist_ai.tex'}],True
20,What are the potential harms and legal implications of GAI-generated obscene and abusive content?,"['How GAI relates to copyright, including the status of generated content that is similar to but does not strictly copy work protected by copyright, is currently being debated in legal fora. Similar discussions are taking place regarding the use or emulation of personal identity, likeness, or voice without permission.\n\nTrustworthy Al Characteristics: Accountable and Transparent, Fair with Harmful Bias Managed, Privacy Enhanced\n\\subsection*{2.11. Obscene, Degrading, and/or Abusive Content}\nGAI can ease the production of and access to illegal non-consensual intimate imagery (NCII) of adults, and/or child sexual abuse material (CSAM). GAI-generated obscene, abusive or degrading content can create privacy, psychological and emotional, and even physical harms, and in some cases may be illegal.\n\nGenerated explicit or obscene AI content may include highly realistic ""deepfakes"" of real individuals, including children. The spread of this kind of material can have downstream negative consequences: in the context of CSAM, even if the generated images do not resemble specific individuals, the prevalence of such images can divert time and resources from efforts\\_to find real-world victims. Outside of CSAM, the creation and spread of NCII disproportionately impacts women and sexual minorities, and can have subsequent negative consequences including decline in overall mental health, substance abuse, and even suicidal thoughts.\n\nData used for training GAI models may unintentionally include CSAM and NCII. A recent report noted that several commonly used GAI training datasets were found to contain hundreds of known images of\n\nCSAM. Even when trained on ""clean"" data, increasingly capable GAI models can synthesize or produce synthetic NCII and CSAM. Websites, mobile apps, and custom-built models that generate synthetic NCII have moved from niche internet forums to mainstream, automated, and scaled online businesses.']","GAI-generated obscene, abusive, or degrading content can create privacy, psychological, emotional, and even physical harms, and in some cases may be illegal. Generated explicit or obscene AI content may include highly realistic 'deepfakes' of real individuals, including children. The spread of this kind of material can have downstream negative consequences: in the context of CSAM, even if the generated images do not resemble specific individuals, the prevalence of such images can divert time and resources from efforts to find real-world victims. Outside of CSAM, the creation and spread of NCII disproportionately impacts women and sexual minorities, and can have subsequent negative consequences including decline in overall mental health, substance abuse, and even suicidal thoughts.",simple,[{'source': 'data/nist_ai.tex'}],True
21,What are the different stages of the AI lifecycle where risks can arise?,"['Al risks can differ from or intensify traditional software risks. Likewise, GAI can exacerbate existing AI risks, and creates unique risks. GAI risks can vary along many dimensions:\n\\begin{itemize}\n  \\item Stage of the AI lifecycle: Risks can arise during design, development, deployment, operation, and/or decommissioning.\n  \\item Scope: Risks may exist at individual model or system levels, at the application or implementation levels (i.e., for a specific use case), or at the ecosystem level - that is, beyond a single system or organizational context. Examples of the latter include the expansion of ""algorithmic monocultures, ${ }^{3 \\prime \\prime}$ resulting from repeated use of the same model, or impacts on access to opportunity, labor markets, and the creative economies. ${ }^{4}$\n  \\item Source of risk: Risks may emerge from factors related to the design, training, or operation of the GAI model itself, stemming in some cases from GAI model or system inputs, and in other cases, from GAI system outputs. Many GAI risks, however, originate from human behavior, including\n\\end{itemize}\n\\footnotetext{3 ""Algorithmic monocultures"" refers to the phenomenon in which repeated use of the same model or algorithm in consequential decision-making settings like employment and lending can result in increased susceptibility by systems to correlated failures (like unexpected shocks), due to multiple actors relying on the same algorithm. ${ }^{4}$ Many studies have projected the impact of AI on the workforce and labor markets. Fewer studies have examined the impact of GAI on the labor market, though some industry surveys indicate that that both employees and employers are pondering this disruption.\n}\nthe abuse, misuse, and unsafe repurposing by humans (adversarial or not), and others result from interactions between a human and an Al system.\n\\begin{itemize}']","Risks can arise during design, development, deployment, operation, and/or decommissioning.",simple,[{'source': 'data/nist_ai.tex'}],True
22,What actions are suggested to address the risk of Information Integrity in the context of operator and practitioner proficiency with AI system performance and trustworthiness?,"['MAP 3.4: Processes for operator and practitioner proficiency with Al system performance and trustworthiness - and relevant technical standards and certifications - are defined, assessed, and documented.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMP-3.4-001 & \\begin{tabular}{l}\nEvaluate whether GAI operators and end-users can accurately understand \\\\\ncontent lineage and origin. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMP-3.4-002 & \\begin{tabular}{l}\nAdapt existing training programs to include modules on digital content \\\\\ntransparency. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMP-3.4-003 & \\begin{tabular}{l}\nDevelop certification programs that test proficiency in managing GAI risks and \\\\\ninterpreting content provenance, relevant to specific industry and context. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMP-3.4-004 & Delineate human proficiency tests from tests of GAI capabilities. & Human-AI Configuration \\\\\n\\hline\nMP-3.4-005 & \\begin{tabular}{l}\nImplement systems to continually monitor and track the outcomes of human-GAI \\\\\nconfigurations for future refinement and improvements. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMP-3.4-006 & \\begin{tabular}{l}\nInvolve the end-users, practitioners, and operators in GAI system in prototyping \\\\\nand testing activities. Make sure these tests cover various scenarios, such as crisis \\\\\nsituations or ethically sensitive contexts. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Integrity; Harmful Bias \\\\\nand Homogenization; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","The suggested actions to address the risk of Information Integrity in the context of operator and practitioner proficiency with AI system performance and trustworthiness are: 1. Adapt existing training programs to include modules on digital content transparency (MP-3.4-002). 2. Develop certification programs that test proficiency in managing GAI risks and interpreting content provenance, relevant to specific industry and context (MP-3.4-003). 3. Implement systems to continually monitor and track the outcomes of human-GAI configurations for future refinement and improvements (MP-3.4-005).",simple,[{'source': 'data/nist_ai.tex'}],True
23,What role do national security professionals play in managing national security risks in AI systems?,"['\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nGV-2.1-001 & \\begin{tabular}{l}\nEstablish organizational roles, policies, and procedures for communicating GAI \\\\\nincidents and performance to AI Actors and downstream stakeholders (including \\\\\nthose potentially impacted), via community or official resources (e.g., Al incident \\\\\ndatabase, AVID, $\\underline{C V E}, \\underline{N V D}$, or OECD AI incident monitor). \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; Value \\\\\nChain and Component Integration \\\\\n\\end{tabular} \\\\\n\\hline\nGV-2.1-002 & \\begin{tabular}{l}\nEstablish procedures to engage teams for GAI system incident response with \\\\\ndiverse composition and responsibilities based on the particular incident type. \\\\\n\\end{tabular} & Harmful Bias and Homogenization \\\\\n\\hline\nGV-2.1-003 & \\begin{tabular}{l}\nEstablish processes to verify the AI Actors conducting GAI incident response tasks \\\\\ndemonstrate and maintain the appropriate skills and training. \\\\\n\\end{tabular} & Human-AI Configuration \\\\\n\\hline\nGV-2.1-004 & \\begin{tabular}{l}\nWhen systems may raise national security risks, involve national security \\\\\nprofessionals in mapping, measuring, and managing those risks. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nDangerous, Violent, or Hateful \\\\\nContent; Information Security \\\\\n\\end{tabular} \\\\\n\\hline\n & \\begin{tabular}{l}\nCreate mechanisms to provide protections for whistleblowers who report, based \\\\\non reasonable belief, when the organization violates relevant laws or poses a \\\\\nspecific and empirically well-substantiated negative risk to public safety (or has \\\\\nalready caused harm). \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nDangerous, Violent, or Hateful \\\\\nContent \\\\\n\\end{tabular} \\\\\n\\hline\nAl Actor Tasks: Governance and Oversight &  &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","National security professionals are involved in mapping, measuring, and managing national security risks when AI systems may raise such risks.",simple,[{'source': 'data/nist_ai.tex'}],True
24,What are the potential consequences of harmful bias or homogenization in GAI models?,"['\\item Confabulation: The production of confidently stated but erroneous or false content (known colloquially as ""hallucinations"" or ""fabrications"") by which users may be misled or deceived. ${ }^{6}$\n  \\item Dangerous, Violent, or Hateful Content: Eased production of and access to violent, inciting, radicalizing, or threatening content as well as recommendations to carry out self-harm or conduct illegal activities. Includes difficulty controlling public exposure to hateful and disparaging or stereotyping content.\n  \\item Data Privacy: Impacts due to leakage and unauthorized use, disclosure, or de-anonymization of biometric, health, location, or other personally identifiable information or sensitive data. ${ }^{7}$\n  \\item Environmental Impacts: Impacts due to high compute resource utilization in training or operating GAI models, and related outcomes that may adversely impact ecosystems.\n  \\item Harmful Bias or Homogenization: Amplification and exacerbation of historical, societal, and systemic biases; performance disparities ${ }^{8}$ between sub-groups or languages, possibly due to non-representative training data, that result in discrimination, amplification of biases, or incorrect presumptions about performance; undesired homogeneity that skews system or model outputs, which may be erroneous, lead to ill-founded decision-making, or amplify harmful biases.\n  \\item Human-AI Configuration: Arrangements of or interactions between a human and an AI system which can result in the human inappropriately anthropomorphizing GAI systems or experiencing algorithmic aversion, automation bias, over-reliance, or emotional entanglement with GAI systems.\n  \\item Information Integrity: Lowered barrier to entry to generate and support the exchange and consumption of content which may not distinguish fact from opinion or fiction or acknowledge uncertainties, or could be leveraged for large-scale dis- and mis-information campaigns.']","The potential consequences of harmful bias or homogenization in GAI models include amplification and exacerbation of historical, societal, and systemic biases; performance disparities between sub-groups or languages, possibly due to non-representative training data, that result in discrimination, amplification of biases, or incorrect presumptions about performance; and undesired homogeneity that skews system or model outputs, which may be erroneous, lead to ill-founded decision-making, or amplify harmful biases.",simple,[{'source': 'data/nist_ai.tex'}],True
25,What is the contact information for the National Institute of Standards and Technology AI Innovation Lab?,"['}\n Approved by the NIST Editorial Review Board on 07-25-2024 \n\\subsubsection*{\\section{Contact Information}\n}\n \\href{mailto:ai-inquiries@nist.gov}{ai-inquiries@nist.gov} National Institute of Standards and Technology Attn: NIST AI Innovation Lab, Information Technology Laboratory 100 Bureau Drive (Mail Stop 8900) Gaithersburg, MD 20899-8900 \n\\subsubsection*{\\section{Additional Information}\n}\n Additional information about this publication and other NIST AI publications are available at \\href{https://airc.nist.gov/Home}{https://airc.nist.gov/Home}. Disclaimer: Certain commercial entities, equipment, or materials may be identified in this document in order to adequately describe an experimental procedure or concept. Such identification is not intended to imply recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the entities, materials, or equipment are necessarily the best available for the purpose. Any mention of commercial, non-profit, academic partners, or their products, or references is for information only; it is not intended to imply endorsement or recommendation by any U.S. Government agency.\n\\end{abstract}']","National Institute of Standards and Technology Attn: NIST AI Innovation Lab, Information Technology Laboratory 100 Bureau Drive (Mail Stop 8900) Gaithersburg, MD 20899-8900",simple,[{'source': 'data/nist_ai.tex'}],True
26,"What is the purpose of collaborating with external researchers, industry experts, and community representatives in the context of managing identified risks?","['\\begin{center}\n\\begin{tabular}{|c|c|c|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMG-4.1-001 & \\begin{tabular}{l}\nCollaborate with external researchers, industry experts, and community \\\\\nrepresentatives to maintain awareness of emerging best practices and \\\\\ntechnologies in measuring and managing identified risks. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Harmful Bias \\\\\nand Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMG-4.1-002 & \\begin{tabular}{l}\nEstablish, maintain, and evaluate effectiveness of organizational processes and \\\\\nprocedures for post-deployment monitoring of GAI systems, particularly for \\\\\npotential confabulation, CBRN, or cyber risks. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nConfabulation; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMG-4.1-003 & \\begin{tabular}{l}\nEvaluate the use of sentiment analysis to gauge user sentiment regarding GAI \\\\\ncontent performance and impact, and work in collaboration with AI Actors \\\\\nexperienced in user research and experience. \\\\\n\\end{tabular} & Human-Al Configuration \\\\\n\\hline\nMG-4.1-004 & \\begin{tabular}{l}\nImplement active learning techniques to identify instances where the model fails \\\\\nor produces unexpected outputs. \\\\\n\\end{tabular} & Confabulation \\\\\n\\hline\nMG-4.1-005 & \\begin{tabular}{l}\nShare transparency reports with internal and external stakeholders that detail \\\\\nsteps taken to update the GAI system to enhance transparency and \\\\\naccountability. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; Harmful \\\\\nBias and Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMG-4.1-006 & \\begin{tabular}{l}\nTrack dataset modifications for provenance by monitoring data deletions, \\\\\nrectification requests, and other changes that may impact the verifiability of \\\\\ncontent origins. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","The purpose of collaborating with external researchers, industry experts, and community representatives is to maintain awareness of emerging best practices and technologies in measuring and managing identified risks.",simple,[{'source': 'data/nist_ai.tex'}],True
27,What should be prioritized in GAI structured public feedback processes based on risk assessment estimates?,"[""\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMP-5.1-001 & \\begin{tabular}{l}\nApply TEVV practices for content provenance (e.g., probing a system's synthetic \\\\\ndata generation capabilities for potential misuse or vulnerabilities. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMP-5.1-002 & \\begin{tabular}{l}\nIdentify potential content provenance harms of GAI, such as misinformation or \\\\\ndisinformation, deepfakes, including NCII, or tampered content. Enumerate and \\\\\nrank risks based on their likelihood and potential impact, and determine how well \\\\\nprovenance solutions address specific risks and/or harms. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Dangerous, \\\\\nViolent, or Hateful Content; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content \\\\\n\\end{tabular} \\\\\n\\hline\nMP-5.1-003 & \\begin{tabular}{l}\nConsider disclosing use of GAI to end users in relevant contexts, while considering \\\\\nthe objective of disclosure, the context of use, the likelihood and magnitude of the \\\\\nrisk posed, the audience of the disclosure, as well as the frequency of the \\\\\ndisclosures. \\\\\n\\end{tabular} & Human-AI Configuration \\\\\n\\hline\nMP-5.1-004 & \\begin{tabular}{l}\nPrioritize GAI structured public feedback processes based on risk assessment \\\\\nestimates. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; CBRN \\\\\nInformation or Capabilities; \\\\\nDangerous, Violent, or Hateful \\\\\nContent; Harmful Bias and \\\\\nHomogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMP-5.1-005 & \\begin{tabular}{l}\nConduct adversarial role-playing exercises, GAI red-teaming, or chaos testing to \\\\\nidentify anomalous or unforeseen failure modes. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\nUsers, Operation and Monitoring & \\begin{tabular}{l}\nProfile threats and negative impacts arising from GAI systems interacting with, \\\\""]",GAI structured public feedback processes should be prioritized based on risk assessment estimates.,simple,[{'source': 'data/nist_ai.tex'}],True
28,What considerations should be made to avoid arbitrary or capricious termination of critical GAI technologies or vendor services?,"['\\hline\nGV-6.2-007 & \\begin{tabular}{l}\nReview vendor contracts and avoid arbitrary or capricious termination of critical \\\\\nGAI technologies or vendor services and non-standard terms that may amplify or \\\\\ndefer liability in unexpected ways and/or contribute to unauthorized data \\\\\ncollection by vendors or third-parties (e.g., secondary data use). Consider: Clear \\\\\nassignment of liability and responsibility for incidents, GAI system changes over \\\\\ntime (e.g., fine-tuning, drift, decay); Request: Notification and disclosure for \\\\\nserious incidents arising from third-party data and systems; Service Level \\\\\nAgreements (SLAs) in vendor contracts that address incident response, response \\\\\ntimes, and availability of critical support. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-Al Configuration; \\\\\nInformation Security; Value Chain \\\\\nand Component Integration \\\\\n\\end{tabular} \\\\\n\\hline\n\\multicolumn{3}{|l|}{AI Actor Tasks: AI Deployment, Operation and Monitoring, TEVV, Third-party entities} \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","To avoid arbitrary or capricious termination of critical GAI technologies or vendor services, considerations should include: clear assignment of liability and responsibility for incidents, GAI system changes over time (e.g., fine-tuning, drift, decay); request for notification and disclosure for serious incidents arising from third-party data and systems; and Service Level Agreements (SLAs) in vendor contracts that address incident response, response times, and availability of critical support.",simple,[{'source': 'data/nist_ai.tex'}],True
29,"What are some potential content provenance harms of GAI, and how can they be identified and ranked based on their likelihood and impact?","[""\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMP-5.1-001 & \\begin{tabular}{l}\nApply TEVV practices for content provenance (e.g., probing a system's synthetic \\\\\ndata generation capabilities for potential misuse or vulnerabilities. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMP-5.1-002 & \\begin{tabular}{l}\nIdentify potential content provenance harms of GAI, such as misinformation or \\\\\ndisinformation, deepfakes, including NCII, or tampered content. Enumerate and \\\\\nrank risks based on their likelihood and potential impact, and determine how well \\\\\nprovenance solutions address specific risks and/or harms. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Dangerous, \\\\\nViolent, or Hateful Content; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content \\\\\n\\end{tabular} \\\\\n\\hline\nMP-5.1-003 & \\begin{tabular}{l}\nConsider disclosing use of GAI to end users in relevant contexts, while considering \\\\\nthe objective of disclosure, the context of use, the likelihood and magnitude of the \\\\\nrisk posed, the audience of the disclosure, as well as the frequency of the \\\\\ndisclosures. \\\\\n\\end{tabular} & Human-AI Configuration \\\\\n\\hline\nMP-5.1-004 & \\begin{tabular}{l}\nPrioritize GAI structured public feedback processes based on risk assessment \\\\\nestimates. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; CBRN \\\\\nInformation or Capabilities; \\\\\nDangerous, Violent, or Hateful \\\\\nContent; Harmful Bias and \\\\\nHomogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMP-5.1-005 & \\begin{tabular}{l}\nConduct adversarial role-playing exercises, GAI red-teaming, or chaos testing to \\\\\nidentify anomalous or unforeseen failure modes. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\nUsers, Operation and Monitoring & \\begin{tabular}{l}\nProfile threats and negative impacts arising from GAI systems interacting with, \\\\"", '\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-4.2-001 & \\begin{tabular}{l}\nConduct adversarial testing at a regular cadence to map and measure GAI risks, \\\\\nincluding tests to address attempts to deceive or manipulate the application of \\\\\nprovenance techniques or other misuses. Identify vulnerabilities and \\\\\nunderstand potential misuse scenarios and unintended outputs. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-002 & \\begin{tabular}{l}\nEvaluate GAI system performance in real-world scenarios to observe its \\\\\nbehavior in practical environments and reveal issues that might not surface in \\\\\ncontrolled and optimized testing environments. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nConfabulation; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-003 & \\begin{tabular}{l}\nImplement interpretability and explainability methods to evaluate GAI system \\\\\ndecisions and verify alignment with intended purpose. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Harmful Bias \\\\\nand Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-004 & \\begin{tabular}{l}\nMonitor and document instances where human operators or other systems \\\\\noverride the GAI\'s decisions. Evaluate these cases to understand if the overrides \\\\\nare linked to issues related to content provenance. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-005 & \\begin{tabular}{l}\nVerify and document the incorporation of results of structured public feedback \\\\\nexercises into design, implementation, deployment approval (""go""/""no-go"" \\\\\ndecisions), monitoring, and decommission decisions. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\nAI Actor Tasks: Al Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV &  &  \\\\\n\\hline\n\\end{tabular}']","Some potential content provenance harms of GAI include misinformation or disinformation, deepfakes, including NCII, or tampered content. These harms can be identified and ranked based on their likelihood and potential impact by enumerating the risks and determining how well provenance solutions address specific risks and/or harms.",simple,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
30,What is the purpose of implementing real-time monitoring processes for analyzing generated content performance?,"['\\end{tabular} \\\\\n\\hline\nMG-3.2-004 & \\begin{tabular}{l}\nEvaluate user reported problematic content and integrate feedback into system \\\\\nupdates. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-Al Configuration, \\\\\nDangerous, Violent, or Hateful \\\\\nContent \\\\\n\\end{tabular} \\\\\n\\hline\nMG-3.2-005 & \\begin{tabular}{l}\nImplement content filters to prevent the generation of inappropriate, harmful, \\\\\nfalse, illegal, or violent content related to the GAI application, including for CSAM \\\\\nand NCII. These filters can be rule-based or leverage additional machine learning \\\\\nmodels to flag problematic inputs and outputs. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Harmful Bias \\\\\nand Homogenization; Dangerous, \\\\\nViolent, or Hateful Content; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content \\\\\n\\end{tabular} \\\\\n\\hline\nMG-3.2-006 & \\begin{tabular}{l}\nImplement real-time monitoring processes for analyzing generated content \\\\\nperformance and trustworthiness characteristics related to content provenance \\\\\nto identify deviations from the desired standards and trigger alerts for human \\\\\nintervention. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\n\\end{tabular}\n\\end{center}']",The purpose of implementing real-time monitoring processes for analyzing generated content performance is to identify deviations from the desired standards and trigger alerts for human intervention.,simple,[{'source': 'data/nist_ai.tex'}],True
31,What are the concerns associated with GAI media in terms of promoting dangerous or violent messages?,"['\\footnotetext{${ }^{9}$ Confabulations of falsehoods are most commonly a problem for text-based outputs; for audio, image, or video content, creative generation of non-factual content can be a desired behavior.\\\\\n${ }^{10}$ For example, legal confabulations have been shown to be pervasive in current state-of-the-art LLMs. See also, e.g.,\n}\nunethical behavior. Text-to-image models also make it easy to create images that could be used to promote dangerous or violent messages. Similar concerns are present for other GAI media, including video and audio. GAI may also produce content that recommends self-harm or criminal/illegal activities.\nMany current systems restrict model outputs to limit certain content or in response to certain prompts, but this approach may still produce harmful recommendations in response to other less-explicit, novel prompts (also relevant to CBRN Information or Capabilities, Data Privacy, Information Security, and Obscene, Degrading and/or Abusive Content). Crafting such prompts deliberately is known as ""ailbreaking,"" or, manipulating prompts to circumvent output controls. Limitations of GAI systems can be harmful or dangerous in certain contexts. Studies have observed that users may disclose mental health issues in conversations with chatbots - and that users exhibit negative reactions to unhelpful responses from these chatbots during situations of distress.\n\nThis risk encompasses difficulty controlling creation of and public exposure to offensive or hateful language, and denigrating or stereotypical content generated by AI. This kind of speech may contribute to downstream harm such as fueling dangerous or violent behaviors. The spread of denigrating or stereotypical content can also further exacerbate representational harms (see Harmful Bias and Homogenization below).']","GAI media, including text-to-image models, video, and audio, can create content that promotes dangerous or violent messages. This risk includes the difficulty of controlling the creation and public exposure to offensive or hateful language, and denigrating or stereotypical content generated by AI, which may contribute to downstream harm such as fueling dangerous or violent behaviors.",simple,[{'source': 'data/nist_ai.tex'}],True
32,What techniques are suggested to evaluate the quality and integrity of training data?,"['\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\n & \\begin{tabular}{l}\nImplement continuous monitoring of GAI system impacts to identify whether GAI \\\\\noutputs are equitable across various sub-populations. Seek active and direct \\\\\nfeedback from affected communities via structured feedback mechanisms or red- \\\\\nteaming to monitor and improve outputs. \\\\\n\\end{tabular} & Harmful Bias and Homogenization \\\\\n\\hline\nMS-1.1-007 & \\begin{tabular}{l}\nEvaluate the quality and integrity of data used in training and the provenance of \\\\\nAl-generated content, for example by employing techniques like chaos \\\\\nengineering and seeking stakeholder feedback. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMS-1.1-008 & \\begin{tabular}{l}\nDefine use cases, contexts of use, capabilities, and negative impacts where \\\\\nstructured human feedback exercises, e.g., GAI red-teaming, would be most \\\\\nbeneficial for GAI risk measurement and management based on the context of \\\\\nuse. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHarmful Bias and \\\\\nHomogenization; CBRN \\\\\nInformation or Capabilities \\\\\n\\end{tabular} \\\\\n\\hline\nMS-1.1-009 & \\begin{tabular}{l}\nTrack and document risks or opportunities related to all GAI risks that cannot be \\\\\nmeasured quantitatively, including explanations as to why some risks cannot be \\\\\nmeasured (e.g., due to technological limitations, resource constraints, or \\\\\ntrustworthy considerations). Include unmeasured risks in marginal risks. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nAI Actor Tasks: Al Development, Domain Experts, TEVV &  &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nMEASURE 1.3: Internal experts who did not serve as front-line developers for the system and/or independent assessors are involved in regular assessments and updates. Domain experts, users, Al Actors external to the team that developed or deployed the Al system, and affected communities are consulted in support of assessments as necessary per organizational risk tolerance.']",Techniques suggested to evaluate the quality and integrity of training data include employing chaos engineering and seeking stakeholder feedback.,simple,[{'source': 'data/nist_ai.tex'}],True
33,What role does digital watermarking play in provenance data tracking for GAI systems?,"['Provenance metadata can include information about GAI model developers or creators of GAI content, date/time of creation, location, modifications, and sources. Metadata can be tracked for text, images, videos, audio, and underlying datasets. The implementation of provenance data tracking techniques can help assess the authenticity, integrity, intellectual property rights, and potential manipulations in digital content. Some well-known techniques for provenance data tracking include digital watermarking, metadata recording, digital fingerprinting, and human authentication, among others.\n\\section*{Provenance Data Tracking Approaches}\nProvenance data tracking techniques for GAI systems can be used to track the history and origin of data inputs, metadata, and synthetic content. Provenance data tracking records the origin and history for digital content, allowing its authenticity to be determined. It consists of techniques to record metadata as well as overt and covert digital watermarks on content. Data provenance refers to tracking the origin and history of input data through metadata and digital watermarking techniques. Provenance data tracking processes can include and assist AI Actors across the lifecycle who may not have full visibility or control over the various trade-offs and cascading impacts of early-stage model decisions on downstream performance and synthetic outputs. For example, by selecting a watermarking model to prioritize robustness (the durability of a watermark), an AI actor may inadvertently diminish computational complexity (the resources required to implement watermarking). Organizational risk management efforts for enhancing content provenance include:\n\\begin{itemize}\n  \\item Tracking provenance of training data and metadata for GAI systems;\n  \\item Documenting provenance data limitations within GAI systems;\n  \\item Monitoring system capabilities and limitations in deployment through rigorous TEVV processes;']","Digital watermarking plays a role in provenance data tracking for GAI systems by recording the origin and history of digital content. It is used to embed overt and covert watermarks on content, which helps in determining the authenticity and integrity of the content.",simple,[{'source': 'data/nist_ai.tex'}],True
34,"Who are the AI Actors that interact with GAI systems, and what activities do they engage in?","[""Acknowledgments: These considerations could not have been surfaced without the helpful analysis and contributions from the community and NIST staff GAI PWG leads: George Awad, Luca Belli, Harold Booth, Mat Heyman, Yooyoung Lee, Mark Pryzbocki, Reva Schwartz, Martin Stanley, and Kyra Yee.\n\\section*{A.1. Governance}\n\\section*{A.1.1. Overview}\nLike any other technology system, governance principles and techniques can be used to manage risks related to generative AI models, capabilities, and applications. Organizations may choose to apply their existing risk tiering to GAI systems, or they may opt to revise or update Al system risk levels to address these unique GAI risks. This section describes how organizational governance regimes may be reevaluated and adjusted for GAI contexts. It also addresses third-party considerations for governing across the Al value chain.\n\\section*{A.1.2. Organizational Governance}\nGAI opportunities, risks and long-term performance characteristics are typically less well-understood than non-generative Al tools and may be perceived and acted upon by humans in ways that vary greatly. Accordingly, GAI may call for different levels of oversight from AI Actors or different human-AI configurations in order to manage their risks effectively. Organizations' use of GAl systems may also warrant additional human review, tracking and documentation, and greater management oversight.\n\nAl technology can produce varied outputs in multiple modalities and present many classes of user interfaces. This leads to a broader set of AI Actors interacting with GAI systems for widely differing applications and contexts of use. These can include data labeling and preparation, development of GAI models, content moderation, code generation and review, text generation and editing, image and video generation, summarization, search, and chat. These activities can take place within organizational settings or in the public domain.""]","AI Actors interact with GAI systems for activities including data labeling and preparation, development of GAI models, content moderation, code generation and review, text generation and editing, image and video generation, summarization, search, and chat.",simple,[{'source': 'data/nist_ai.tex'}],True
35,"How can structured public feedback inform decisions related to the design, implementation, and maintenance of AI systems?","['\\item Al Red-teaming: A structured testing exercise used to probe an AI system to find flaws and vulnerabilities such as inaccurate, harmful, or discriminatory outputs, often in a controlled environment and in collaboration with system developers.\n\\end{itemize}\nInformation gathered from structured public feedback can inform design, implementation, deployment approval, maintenance, or decommissioning decisions. Results and insights gleaned from these exercises can serve multiple purposes, including improving data quality and preprocessing, bolstering governance decision making, and enhancing system documentation and debugging practices. When implementing feedback activities, organizations should follow human subjects research requirements and best practices such as informed consent and subject compensation.\n\\section*{Participatory Engagement Methods}\nOn an ad hoc or more structured basis, organizations can design and use a variety of channels to engage external stakeholders in product development or review. Focus groups with select experts can provide feedback on a range of issues. Small user studies can provide feedback from representative groups or populations. Anonymous surveys can be used to poll or gauge reactions to specific features. Participatory engagement methods are often less structured than field testing or red teaming, and are more commonly used in early stages of AI or product development.\n\\section*{Field Testing}\nField testing involves structured settings to evaluate risks and impacts and to simulate the conditions under which the GAI system will be deployed. Field style tests can be adapted from a focus on user preferences and experiences towards Al risks and impacts - both negative and positive. When carried out with large groups of users, these tests can provide estimations of the likelihood of risks and impacts in real world interactions.']","Information gathered from structured public feedback can inform design, implementation, deployment approval, maintenance, or decommissioning decisions. Results and insights gleaned from these exercises can serve multiple purposes, including improving data quality and preprocessing, bolstering governance decision making, and enhancing system documentation and debugging practices.",simple,[{'source': 'data/nist_ai.tex'}],True
36,What are the risks associated with Generative AI (GAI) and how can organizations manage them?,"['A profile is an implementation of the AI RMF functions, categories, and subcategories for a specific setting, application, or technology - in this case, Generative AI (GAI) - based on the requirements, risk tolerance, and resources of the Framework user. AI RMF profiles assist organizations in deciding how to best manage Al risks in a manner that is well-aligned with their goals, considers legal/regulatory requirements and best practices, and reflects risk management priorities. Consistent with other AI RMF profiles, this profile offers insights into how risk can be managed across various stages of the Al lifecycle and for GAI as a technology.\n\nAs GAI covers risks of models or applications that can be used across use cases or sectors, this document is an AI RMF cross-sectoral profile. Cross-sectoral profiles can be used to govern, map, measure, and manage risks associated with activities or business processes common across sectors, such as the use of large language models (LLMs), cloud-based services, or acquisition.\n\nThis document defines risks that are novel to or exacerbated by the use of GAI. After introducing and describing these risks, the document provides a set of suggested actions to help organizations govern, map, measure, and manage these risks.']","The document defines risks that are novel to or exacerbated by the use of Generative AI (GAI). It provides a set of suggested actions to help organizations govern, map, measure, and manage these risks.",simple,[{'source': 'data/nist_ai.tex'}],True
37,How does undesired homogenization in GAI systems contribute to unreliable decision-making and amplification of harmful biases?,"['Harmful bias in GAI systems can also lead to harms via disparities between how a model performs for different subgroups or languages (e.g., an LLM may perform less well for non-English languages or certain dialects). Such disparities can contribute to discriminatory decision-making or amplification of existing societal biases. In addition, GAI systems may be inappropriately trusted to perform similarly across all subgroups, which could leave the groups facing underperformance with worse outcomes than if no GAI system were used. Disparate or reduced performance for lower-resource languages also presents challenges to model adoption, inclusion, and accessibility, and may make preservation of endangered languages more difficult if GAI systems become embedded in everyday processes that would otherwise have been opportunities to use these languages.\n\nBias is mutually reinforcing with the problem of undesired homogenization, in which GAI systems produce skewed distributions of outputs that are overly uniform (for example, repetitive aesthetic styles\\\\\nand reduced content diversity). Overly homogenized outputs can themselves be incorrect, or they may lead to unreliable decision-making or amplify harmful biases. These phenomena can flow from foundation models to downstream models and systems, with the foundation models acting as ""bottlenecks,"" or single points of failure.\n\nOverly homogenized content can contribute to ""model collapse."" Model collapse can occur when model training over-relies on synthetic data, resulting in data points disappearing from the distribution of the new model\'s outputs. In addition to threatening the robustness of the model overall, model collapse could lead to homogenized outputs, including by amplifying any homogenization from the model used to generate the synthetic training data.']","Undesired homogenization in GAI systems contributes to unreliable decision-making and amplification of harmful biases by producing skewed distributions of outputs that are overly uniform. These overly homogenized outputs can be incorrect and may lead to unreliable decision-making or amplify harmful biases. This phenomenon can flow from foundation models to downstream models and systems, with the foundation models acting as 'bottlenecks' or single points of failure.",simple,[{'source': 'data/nist_ai.tex'}],True
38,"What processes are defined, assessed, and documented to ensure operator and practitioner proficiency with AI system performance and trustworthiness?","['MAP 3.4: Processes for operator and practitioner proficiency with Al system performance and trustworthiness - and relevant technical standards and certifications - are defined, assessed, and documented.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMP-3.4-001 & \\begin{tabular}{l}\nEvaluate whether GAI operators and end-users can accurately understand \\\\\ncontent lineage and origin. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMP-3.4-002 & \\begin{tabular}{l}\nAdapt existing training programs to include modules on digital content \\\\\ntransparency. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMP-3.4-003 & \\begin{tabular}{l}\nDevelop certification programs that test proficiency in managing GAI risks and \\\\\ninterpreting content provenance, relevant to specific industry and context. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMP-3.4-004 & Delineate human proficiency tests from tests of GAI capabilities. & Human-AI Configuration \\\\\n\\hline\nMP-3.4-005 & \\begin{tabular}{l}\nImplement systems to continually monitor and track the outcomes of human-GAI \\\\\nconfigurations for future refinement and improvements. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMP-3.4-006 & \\begin{tabular}{l}\nInvolve the end-users, practitioners, and operators in GAI system in prototyping \\\\\nand testing activities. Make sure these tests cover various scenarios, such as crisis \\\\\nsituations or ethically sensitive contexts. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Integrity; Harmful Bias \\\\\nand Homogenization; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","Processes for operator and practitioner proficiency with AI system performance and trustworthiness are defined, assessed, and documented.",simple,[{'source': 'data/nist_ai.tex'}],True
39,How does the GAI system architecture handle security anomalies and threats?,"['Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-005 & \\begin{tabular}{l}\nVerify that GAI system architecture can monitor outputs and performance, and \\\\\nhandle, recover from, and repair errors when security anomalies, threats and \\\\\nimpacts are detected. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nConfabulation; Information \\\\\nIntegrity; Information Security \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-006 & \\begin{tabular}{l}\nVerify that systems properly handle queries that may give rise to inappropriate, \\\\\nmalicious, or illegal usage, including facilitating manipulation, extortion, targeted \\\\\nimpersonation, cyber-attacks, and weapons creation. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-007 & \\begin{tabular}{l}\nRegularly evaluate GAI system vulnerabilities to possible circumvention of safety \\\\\nmeasures. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\n\\multicolumn{3}{|l|}{AI Actor Tasks: AI Deployment, Al Impact Assessment, Domain Experts, Operation and Monitoring, TEVV} \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","The GAI system architecture can monitor outputs and performance, and handle, recover from, and repair errors when security anomalies, threats, and impacts are detected.",simple,[{'source': 'data/nist_ai.tex'}],True
40,What is the importance of assessing the proportion of synthetic to non-synthetic training data in AI model training?,"['${ }^{15}$ Winogender Schemas is a sample set of paired sentences which differ only by gender of the pronouns used, which can be used to evaluate gender bias in natural language processing coreference resolution systems.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nMS-2.11-005 & \\begin{tabular}{l}\nAssess the proportion of synthetic to non-synthetic training data and verify \\\\\ntraining data is not overly homogenous or GAI-produced to mitigate concerns of \\\\\nmodel collapse. \\\\\n\\end{tabular} & Harmful Bias and Homogenization \\\\\n\\hline\n\\begin{tabular}{l}\nAl Actor Tasks: Al Deployment, AI Impact Assessment, Affected Individuals and Communities, Domain Experts, End-Users, \\\\\nOperation and Monitoring, TEVV \\\\\n\\end{tabular} &  &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nMEASURE 2.12: Environmental impact and sustainability of AI model training and management activities - as identified in the MAP function - are assessed and documented.']","Assessing the proportion of synthetic to non-synthetic training data is important to verify that the training data is not overly homogenous or GAI-produced, which helps mitigate concerns of model collapse.",simple,[{'source': 'data/nist_ai.tex'}],True
41,What is the purpose of using structured feedback mechanisms in assessing AI-generated content?,"['\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nMG-2.2-006 & \\begin{tabular}{l}\nUse feedback from internal and external Al Actors, users, individuals, and \\\\\ncommunities, to assess impact of Al-generated content. \\\\\n\\end{tabular} & Human-AI Configuration \\\\\n\\hline\nMG-2.2-007 & \\begin{tabular}{l}\nUse real-time auditing tools where they can be demonstrated to aid in the \\\\\ntracking and validation of the lineage and authenticity of Al-generated data. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMG-2.2-008 & \\begin{tabular}{l}\nUse structured feedback mechanisms to solicit and capture user input about AI- \\\\\ngenerated content to detect subtle shifts in quality or alignment with \\\\\ncommunity and societal values. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-Al Configuration; Harmful \\\\\nBias and Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMG-2.2-009 & \\begin{tabular}{l}\nConsider opportunities to responsibly use synthetic data and other privacy \\\\\nenhancing techniques in GAI development, where appropriate and applicable, \\\\\nmatch the statistical properties of real-world data without disclosing personally \\\\\nidentifiable information or contributing to homogenization. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nData Privacy; Intellectual Property; \\\\\nInformation Integrity; \\\\\nConfablation; Harmful Bias and \\\\\nHomogenization \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nMANAGE 2.3: Procedures are followed to respond to and recover from a previously unknown risk when it is identified.']",The purpose of using structured feedback mechanisms is to solicit and capture user input about AI-generated content to detect subtle shifts in quality or alignment with community and societal values.,simple,[{'source': 'data/nist_ai.tex'}],True
42,How can GAI systems contribute to the erosion of public trust?,"['GAI systems can also ease the deliberate production or dissemination of false or misleading information (disinformation) at scale, where an actor has the explicit intent to deceive or cause harm to others. Even very subtle changes to text or images can manipulate human and machine perception.\n\nSimilarly, GAI systems could enable a higher degree of sophistication for malicious actors to produce disinformation that is targeted towards specific demographics. Current and emerging multimodal models make it possible to generate both text-based disinformation and highly realistic ""deepfakes"" - that is, synthetic audiovisual content and photorealistic images. ${ }^{12}$ Additional disinformation threats could be enabled by future GAI models trained on new data modalities.\n\nDisinformation and misinformation - both of which may be facilitated by GAI - may erode public trust in true or valid evidence and information, with downstream effects. For example, a synthetic image of a Pentagon blast went viral and briefly caused a drop in the stock market. Generative AI models can also assist malicious actors in creating compelling imagery and propaganda to support disinformation campaigns, which may not be photorealistic, but could enable these campaigns to gain more reach and engagement on social media platforms. Additionally, generative AI models can assist malicious actors in creating fraudulent content intended to impersonate others.']","GAI systems can contribute to the erosion of public trust by facilitating the production and dissemination of disinformation and misinformation. This can include generating synthetic audiovisual content, photorealistic images, and compelling imagery and propaganda, which can manipulate human and machine perception. Such activities can lead to downstream effects, such as a synthetic image of a Pentagon blast causing a drop in the stock market.",simple,[{'source': 'data/nist_ai.tex'}],True
43,What governance tools and protocols can be applied to GAI systems to restrict harmful applications and ensure alignment with organizational values?,"['Organizations can restrict AI applications that cause harm, exceed stated risk tolerances, or that conflict with their tolerances or values. Governance tools and protocols that are applied to other types of AI systems can be applied to GAI systems. These plans and actions include:\n\\begin{itemize}\n  \\item Accessibility and reasonable accommodations\n  \\item Al actor credentials and qualifications\n  \\item Alignment to organizational values\n  \\item Auditing and assessment\n  \\item Change-management controls\n  \\item Commercial use\n  \\item Data provenance\n  \\item Data protection\n  \\item Data retention\n  \\item Consistency in use of defining key terms\n  \\item Decommissioning\n  \\item Discouraging anonymous use\n  \\item Education\n  \\item Impact assessments\n  \\item Incident response\n  \\item Monitoring\n  \\item Opt-outs\n  \\item Risk-based controls\n  \\item Risk mapping and measurement\n  \\item Science-backed TEVV practices\n  \\item Secure software development practices\n  \\item Stakeholder engagement\n  \\item Synthetic content detection and labeling tools and techniques\n  \\item Whistleblower protections\n  \\item Workforce diversity and interdisciplinary teams\n\\end{itemize}\nEstablishing acceptable use policies and guidance for the use of GAI in formal human-Al teaming settings as well as different levels of human-Al configurations can help to decrease risks arising from misuse, abuse, inappropriate repurpose, and misalignment between systems and users. These practices are just one example of adapting existing governance protocols for GAI contexts.\n\\section*{A.1.3. Third-Party Considerations}']","Governance tools and protocols that can be applied to GAI systems include accessibility and reasonable accommodations, AI actor credentials and qualifications, alignment to organizational values, auditing and assessment, change-management controls, commercial use, data provenance, data protection, data retention, consistency in use of defining key terms, decommissioning, discouraging anonymous use, education, impact assessments, incident response, monitoring, opt-outs, risk-based controls, risk mapping and measurement, science-backed TEVV practices, secure software development practices, stakeholder engagement, synthetic content detection and labeling tools and techniques, whistleblower protections, and workforce diversity and interdisciplinary teams.",simple,[{'source': 'data/nist_ai.tex'}],True
44,What should be assessed to ensure the use of proprietary or sensitive training data is consistent with applicable laws?,"['\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nMP-4.1-010 & \\begin{tabular}{l}\nConduct appropriate diligence on training data use to assess intellectual property, \\\\\nand privacy, risks, including to examine whether use of proprietary or sensitive \\\\\ntraining data is consistent with applicable laws. \\\\\n\\end{tabular} & Intellectual Property; Data Privacy \\\\\n\\hline\nAI Actor Tasks: Governance and Oversight, Operation and Monitoring, Procurement, Third-party entities &  &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nMAP 5.1: Likelihood and magnitude of each identified impact (both potentially beneficial and harmful) based on expected use, past uses of Al systems in similar contexts, public incident reports, feedback from those external to the team that developed or deployed the Al system, or other data are identified and documented.']","To ensure the use of proprietary or sensitive training data is consistent with applicable laws, appropriate diligence on training data use should be conducted to assess intellectual property and privacy risks.",simple,[{'source': 'data/nist_ai.tex'}],True
45,How can the GAI system architecture handle and recover from errors when security anomalies are detected?,"['Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-005 & \\begin{tabular}{l}\nVerify that GAI system architecture can monitor outputs and performance, and \\\\\nhandle, recover from, and repair errors when security anomalies, threats and \\\\\nimpacts are detected. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nConfabulation; Information \\\\\nIntegrity; Information Security \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-006 & \\begin{tabular}{l}\nVerify that systems properly handle queries that may give rise to inappropriate, \\\\\nmalicious, or illegal usage, including facilitating manipulation, extortion, targeted \\\\\nimpersonation, cyber-attacks, and weapons creation. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-007 & \\begin{tabular}{l}\nRegularly evaluate GAI system vulnerabilities to possible circumvention of safety \\\\\nmeasures. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\n\\multicolumn{3}{|l|}{AI Actor Tasks: AI Deployment, Al Impact Assessment, Domain Experts, Operation and Monitoring, TEVV} \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","The GAI system architecture can handle, recover from, and repair errors when security anomalies, threats, and impacts are detected.",simple,[{'source': 'data/nist_ai.tex'}],True
46,How do LLM outputs compare to traditional search engine queries in terms of assisting with biological threat creation and attack planning?,"[""Recent research on this topic found that LLM outputs regarding biological threat creation and attack planning provided minimal assistance beyond traditional search engine queries, suggesting that state-ofthe-art LLMs at the time these studies were conducted do not substantially increase the operational likelihood of such an attack. The physical synthesis development, production, and use of chemical or biological agents will continue to require both applicable expertise and supporting materials and infrastructure. The impact of GAI on chemical or biological agent misuse will depend on what the key barriers for malicious actors are (e.g., whether information access is one such barrier), and how well GAI can help actors address those barriers.\n\nFurthermore, chemical and biological design tools (BDTs) - highly specialized Al systems trained on scientific data that aid in chemical and biological design - may augment design capabilities in chemistry and biology beyond what text-based LLMs are able to provide. As these models become more efficacious, including for beneficial uses, it will be important to assess their potential to be used for harm, such as the ideation and design of novel harmful chemical or biological agents.\n\nWhile some of these described capabilities lie beyond the reach of existing GAI tools, ongoing assessments of this risk would be enhanced by monitoring both the ability of AI tools to facilitate CBRN weapons planning and GAI systems' connection or access to relevant data and tools.""]",LLM outputs regarding biological threat creation and attack planning provided minimal assistance beyond traditional search engine queries.,simple,[{'source': 'data/nist_ai.tex'}],True
47,What should be included in the incident response and recovery plans for a GAI system?,"['MANAGE 2.3: Procedures are followed to respond to and recover from a previously unknown risk when it is identified.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\n & \\begin{tabular}{l}\nDevelop and update GAI system incident response and recovery plans and \\\\\nprocedures to address the following: Review and maintenance of policies and \\\\\nprocedures to account for newly encountered uses; Review and maintenance of \\\\\npolicies and procedures for detection of unanticipated uses; Verify response \\\\\nand recovery plans account for the GAI system value chain; Verify response and \\\\\nrecovery plans are updated for and include necessary details to communicate \\\\\nwith downstream GAI system Actors: Points-of-Contact (POC), Contact \\\\\ninformation, notification format. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nMG-2.3-001 \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nAI Actor Tasks: Al Deployment, Operation and Monitoring\n\nMANAGE 2.4: Mechanisms are in place and applied, and responsibilities are assigned and understood, to supersede, disengage, or deactivate Al systems that demonstrate performance or outcomes inconsistent with intended use.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMG-2.4-001 & \\begin{tabular}{l}\nEstablish and maintain communication plans to inform AI stakeholders as part of \\\\\nthe deactivation or disengagement process of a specific GAI system (including for \\\\\nopen-source models) or context of use, including reasons, workarounds, user \\\\\naccess removal, alternative processes, contact information, etc. \\\\\n\\end{tabular} & Human-AI Configuration \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","The incident response and recovery plans for a GAI system should include the following: Review and maintenance of policies and procedures to account for newly encountered uses; Review and maintenance of policies and procedures for detection of unanticipated uses; Verification that response and recovery plans account for the GAI system value chain; Verification that response and recovery plans are updated for and include necessary details to communicate with downstream GAI system Actors, including Points-of-Contact (POC), contact information, and notification format.",simple,[{'source': 'data/nist_ai.tex'}],True
48,What is the purpose of evaluating feedback loops between GAI system content provenance and human reviewers?,"['MANAGE 2.2: Mechanisms are in place and applied to sustain the value of deployed Al systems.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMG-2.2-001 & \\begin{tabular}{l}\nCompare GAI system outputs against pre-defined organization risk tolerance, \\\\\nguidelines, and principles, and review and test Al-generated content against \\\\\nthese guidelines. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content; Harmful Bias and \\\\\nHomogenization; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\nMG-2.2-002 & \\begin{tabular}{l}\nDocument training data sources to trace the origin and provenance of AI- \\\\\ngenerated content. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMG-2.2-003 & \\begin{tabular}{l}\nEvaluate feedback loops between GAI system content provenance and human \\\\\nreviewers, and update where needed. Implement real-time monitoring systems \\\\\nto affirm that content provenance protocols remain effective. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMG-2.2-004 & \\begin{tabular}{l}\nEvaluate GAI content and data for representational biases and employ \\\\\ntechniques such as re-sampling, re-ranking, or adversarial training to mitigate \\\\\nbiases in the generated content. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Security; Harmful Bias \\\\\nand Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\n & \\begin{tabular}{l}\nEngage in due diligence to analyze GAI output for harmful content, potential \\\\\nmisinformation, and CBRN-related or NCII content. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content; Harmful Bias and \\\\\nHomogenization; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}']",The purpose of evaluating feedback loops between GAI system content provenance and human reviewers is to update where needed and implement real-time monitoring systems to affirm that content provenance protocols remain effective.,simple,[{'source': 'data/nist_ai.tex'}],True
49,What is the purpose of developing certification programs in managing GAI risks and interpreting content provenance according to technical standards and certifications?,"['MAP 3.4: Processes for operator and practitioner proficiency with Al system performance and trustworthiness - and relevant technical standards and certifications - are defined, assessed, and documented.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMP-3.4-001 & \\begin{tabular}{l}\nEvaluate whether GAI operators and end-users can accurately understand \\\\\ncontent lineage and origin. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMP-3.4-002 & \\begin{tabular}{l}\nAdapt existing training programs to include modules on digital content \\\\\ntransparency. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMP-3.4-003 & \\begin{tabular}{l}\nDevelop certification programs that test proficiency in managing GAI risks and \\\\\ninterpreting content provenance, relevant to specific industry and context. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMP-3.4-004 & Delineate human proficiency tests from tests of GAI capabilities. & Human-AI Configuration \\\\\n\\hline\nMP-3.4-005 & \\begin{tabular}{l}\nImplement systems to continually monitor and track the outcomes of human-GAI \\\\\nconfigurations for future refinement and improvements. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMP-3.4-006 & \\begin{tabular}{l}\nInvolve the end-users, practitioners, and operators in GAI system in prototyping \\\\\nand testing activities. Make sure these tests cover various scenarios, such as crisis \\\\\nsituations or ethically sensitive contexts. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Integrity; Harmful Bias \\\\\nand Homogenization; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","The purpose of developing certification programs is to test proficiency in managing GAI risks and interpreting content provenance, relevant to specific industry and context.",simple,[{'source': 'data/nist_ai.tex'}],True
50,How do LLMs use synthetic media and recursion to mislead under duress?,"['Satariano, A. et al. (2023) The People Onscreen Are Fake. The Disinformation Is Real. New York Times. \\href{https://www.nytimes.com/2023/02/07/technology/artificial-intelligence-training-deepfake.html}{https://www.nytimes.com/2023/02/07/technology/artificial-intelligence-training-deepfake.html}\n\nSchaul, K. et al. (2024) Inside the secret list of websites that make AI like ChatGPT sound smart.\\\\\nWashington Post. \\href{https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/}{https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/}\\\\\nScheurer, J. et al. (2023) Technical report: Large language models can strategically deceive their users when put under pressure. arXiv. \\href{https://arxiv.org/abs/2311.07590}{https://arxiv.org/abs/2311.07590}\n\nShelby, R. et al. (2023) Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for Harm Reduction. arXiv. \\href{https://arxiv.org/pdf/2210.05791}{https://arxiv.org/pdf/2210.05791}\n\nShevlane, T. et al. (2023) Model evaluation for extreme risks. arXiv. \\href{https://arxiv.org/pdf/2305.15324}{https://arxiv.org/pdf/2305.15324}\\\\\nShumailov, I. et al. (2023) The curse of recursion: training on generated data makes models forget. arXiv. \\href{https://arxiv.org/pdf/2305.17493v2}{https://arxiv.org/pdf/2305.17493v2}\n\nSmith, A. et al. (2023) Hallucination or Confabulation? Neuroanatomy as metaphor in Large Language Models. PLOS Digital Health.\\\\\n\\href{https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig}{https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig}. 0000388\\\\\nSoice, E. et al. (2023) Can large language models democratize access to dual-use biotechnology? arXiv. \\href{https://arxiv.org/abs/2306.03809}{https://arxiv.org/abs/2306.03809}\n\nSolaiman, I. et al. (2023) The Gradient of Generative AI Release: Methods and Considerations. arXiv. \\href{https://arxiv.org/abs/2302.04844}{https://arxiv.org/abs/2302.04844}', 'OpenAI (2023) GPT-4 System Card. \\href{https://cdn.openai.com/papers/gpt-4-system-card.pdf}{https://cdn.openai.com/papers/gpt-4-system-card.pdf}\\\\\nOpenAI (2024) GPT-4 Technical Report. \\href{https://arxiv.org/pdf/2303.08774}{https://arxiv.org/pdf/2303.08774}\\\\\nPadmakumar, V. et al. (2024) Does writing with language models reduce content diversity? ICLR. \\href{https://arxiv.org/pdf/2309.05196}{https://arxiv.org/pdf/2309.05196}\n\nPark, P. et. al. (2024) Al deception: A survey of examples, risks, and potential solutions. Patterns, 5(5). arXiv. \\href{https://arxiv.org/pdf/2308.14752}{https://arxiv.org/pdf/2308.14752}\n\nPartnership on AI (2023) Building a Glossary for Synthetic Media Transparency Methods, Part 1: Indirect Disclosure. \\href{https://partnershiponai.org/glossary-for-synthetic-media-transparency-methods-part-1indirect-disclosure/}{https://partnershiponai.org/glossary-for-synthetic-media-transparency-methods-part-1indirect-disclosure/}\n\nQu, Y. et al. (2023) Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From TextTo-Image Models. arXiv. \\href{https://arxiv.org/pdf/2305.13873}{https://arxiv.org/pdf/2305.13873}\n\nRafat, K. et al. (2023) Mitigating carbon footprint for knowledge distillation based deep learning model compression. PLOS One. \\href{https://journals.plos.org/plosone/article?id=10.1371/journal.pone}{https://journals.plos.org/plosone/article?id=10.1371/journal.pone}. 0285668\n\nSaid, I. et al. (2022) Nonconsensual Distribution of Intimate Images: Exploring the Role of Legal Attitudes in Victimization and Perpetration. Sage.\\\\\n\\href{https://journals.sagepub.com/doi/full/10.1177/08862605221122834#bibr47-08862605221122834}{https://journals.sagepub.com/doi/full/10.1177/08862605221122834\\#bibr47-08862605221122834}\\\\\nSandbrink, J. (2023) Artificial intelligence and biological misuse: Differentiating risks of language models and biological design tools. arXiv. \\href{https://arxiv.org/pdf/2306.13952}{https://arxiv.org/pdf/2306.13952}']",The answer to given question is not present in context,multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
51,How should policies re-evaluate GAI models to mitigate risks like bias and homogenization in new domains?,"['Security; Harmful Bias and \\\\\nHomogenization; Dangerous, \\\\\nViolent, or Hateful Content; Data \\\\\nPrivacy \\\\\n\\end{tabular} \\\\\n\\hline\nMP-4.1-006 & \\begin{tabular}{l}\nImplement policies and practices defining how third-party intellectual property and \\\\\ntraining data will be used, stored, and protected. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nIntellectual Property; Value Chain \\\\\nand Component Integration \\\\\n\\end{tabular} \\\\\n\\hline\nMP-4.1-007 & \\begin{tabular}{l}\nRe-evaluate models that were fine-tuned or enhanced on top of third-party \\\\\nmodels. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration \\\\\n\\end{tabular} \\\\\n\\hline\nMP-4.1-008 & \\begin{tabular}{l}\nRe-evaluate risks when adapting GAI models to new domains. Additionally, \\\\\nestablish warning systems to determine if a GAI system is being used in a new \\\\\ndomain where previous assumptions (relating to context of use or mapped risks \\\\\nsuch as security, and safety) may no longer hold. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nIntellectual Property; Harmful Bias \\\\\nand Homogenization; Dangerous, \\\\\nViolent, or Hateful Content; Data \\\\\nPrivacy \\\\\n\\end{tabular} \\\\\n\\hline\nMP-4.1-009 & \\begin{tabular}{l}\nLeverage approaches to detect the presence of PII or sensitive data in generated \\\\\noutput text, image, video, or audio. \\\\\n\\end{tabular} & Data Privacy \\\\\n\\hline\n\\end{tabular}\n\\end{center}', '\\begin{center}\n\\begin{tabular}{|c|c|c|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMP-4.1-001 & \\begin{tabular}{l}\nConduct periodic monitoring of AI-generated content for privacy risks; address any \\\\\npossible instances of PII or sensitive data exposure. \\\\\n\\end{tabular} & Data Privacy \\\\\n\\hline\nMP-4.1-002 & \\begin{tabular}{l}\nImplement processes for responding to potential intellectual property infringement \\\\\nclaims or other rights. \\\\\n\\end{tabular} & Intellectual Property \\\\\n\\hline\nMP-4.1-003 & \\begin{tabular}{l}\nConnect new GAI policies, procedures, and processes to existing model, data, \\\\\nsoftware development, and IT governance and to legal, compliance, and risk \\\\\nmanagement activities. \\\\\n\\end{tabular} & Information Security; Data Privacy \\\\\n\\hline\nMP-4.1-004 & \\begin{tabular}{l}\nDocument training data curation policies, to the extent possible and according to \\\\\napplicable laws and policies. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nIntellectual Property; Data Privacy; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content \\\\\n\\end{tabular} \\\\\n\\hline\nMP-4.1-005 & \\begin{tabular}{l}\nEstablish policies for collection, retention, and minimum quality of data, in \\\\\nconsideration of the following risks: Disclosure of inappropriate CBRN information; \\\\\nUse of Illegal or dangerous content; Offensive cyber capabilities; Training data \\\\\nimbalances that could give rise to harmful biases; Leak of personally identifiable \\\\\ninformation, including facial likenesses of individuals. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nIntellectual Property; Information \\\\\nSecurity; Harmful Bias and \\\\\nHomogenization; Dangerous, \\\\\nViolent, or Hateful Content; Data \\\\\nPrivacy \\\\\n\\end{tabular} \\\\\n\\hline\nMP-4.1-006 & \\begin{tabular}{l}\nImplement policies and practices defining how third-party intellectual property and \\\\\ntraining data will be used, stored, and protected. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nIntellectual Property; Value Chain \\\\']","Re-evaluate risks when adapting GAI models to new domains. Additionally, establish warning systems to determine if a GAI system is being used in a new domain where previous assumptions (relating to context of use or mapped risks such as security, and safety) may no longer hold.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
52,How can evaluating overrides of GAI decisions reveal content provenance issues and misuse scenarios?,"['\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-4.2-001 & \\begin{tabular}{l}\nConduct adversarial testing at a regular cadence to map and measure GAI risks, \\\\\nincluding tests to address attempts to deceive or manipulate the application of \\\\\nprovenance techniques or other misuses. Identify vulnerabilities and \\\\\nunderstand potential misuse scenarios and unintended outputs. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-002 & \\begin{tabular}{l}\nEvaluate GAI system performance in real-world scenarios to observe its \\\\\nbehavior in practical environments and reveal issues that might not surface in \\\\\ncontrolled and optimized testing environments. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nConfabulation; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-003 & \\begin{tabular}{l}\nImplement interpretability and explainability methods to evaluate GAI system \\\\\ndecisions and verify alignment with intended purpose. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Harmful Bias \\\\\nand Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-004 & \\begin{tabular}{l}\nMonitor and document instances where human operators or other systems \\\\\noverride the GAI\'s decisions. Evaluate these cases to understand if the overrides \\\\\nare linked to issues related to content provenance. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-005 & \\begin{tabular}{l}\nVerify and document the incorporation of results of structured public feedback \\\\\nexercises into design, implementation, deployment approval (""go""/""no-go"" \\\\\ndecisions), monitoring, and decommission decisions. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\nAI Actor Tasks: Al Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV &  &  \\\\\n\\hline\n\\end{tabular}', ""\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMP-5.1-001 & \\begin{tabular}{l}\nApply TEVV practices for content provenance (e.g., probing a system's synthetic \\\\\ndata generation capabilities for potential misuse or vulnerabilities. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMP-5.1-002 & \\begin{tabular}{l}\nIdentify potential content provenance harms of GAI, such as misinformation or \\\\\ndisinformation, deepfakes, including NCII, or tampered content. Enumerate and \\\\\nrank risks based on their likelihood and potential impact, and determine how well \\\\\nprovenance solutions address specific risks and/or harms. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Dangerous, \\\\\nViolent, or Hateful Content; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content \\\\\n\\end{tabular} \\\\\n\\hline\nMP-5.1-003 & \\begin{tabular}{l}\nConsider disclosing use of GAI to end users in relevant contexts, while considering \\\\\nthe objective of disclosure, the context of use, the likelihood and magnitude of the \\\\\nrisk posed, the audience of the disclosure, as well as the frequency of the \\\\\ndisclosures. \\\\\n\\end{tabular} & Human-AI Configuration \\\\\n\\hline\nMP-5.1-004 & \\begin{tabular}{l}\nPrioritize GAI structured public feedback processes based on risk assessment \\\\\nestimates. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; CBRN \\\\\nInformation or Capabilities; \\\\\nDangerous, Violent, or Hateful \\\\\nContent; Harmful Bias and \\\\\nHomogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMP-5.1-005 & \\begin{tabular}{l}\nConduct adversarial role-playing exercises, GAI red-teaming, or chaos testing to \\\\\nidentify anomalous or unforeseen failure modes. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\nUsers, Operation and Monitoring & \\begin{tabular}{l}\nProfile threats and negative impacts arising from GAI systems interacting with, \\\\""]",Evaluating overrides of GAI decisions can reveal content provenance issues and misuse scenarios by monitoring and documenting instances where human operators or other systems override the GAI's decisions. This evaluation helps to understand if the overrides are linked to issues related to content provenance.,multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
53,How does user feedback help mitigate risks of harmful content in GAI systems?,"['\\end{tabular} \\\\\n\\hline\nMG-3.2-004 & \\begin{tabular}{l}\nEvaluate user reported problematic content and integrate feedback into system \\\\\nupdates. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-Al Configuration, \\\\\nDangerous, Violent, or Hateful \\\\\nContent \\\\\n\\end{tabular} \\\\\n\\hline\nMG-3.2-005 & \\begin{tabular}{l}\nImplement content filters to prevent the generation of inappropriate, harmful, \\\\\nfalse, illegal, or violent content related to the GAI application, including for CSAM \\\\\nand NCII. These filters can be rule-based or leverage additional machine learning \\\\\nmodels to flag problematic inputs and outputs. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Harmful Bias \\\\\nand Homogenization; Dangerous, \\\\\nViolent, or Hateful Content; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content \\\\\n\\end{tabular} \\\\\n\\hline\nMG-3.2-006 & \\begin{tabular}{l}\nImplement real-time monitoring processes for analyzing generated content \\\\\nperformance and trustworthiness characteristics related to content provenance \\\\\nto identify deviations from the desired standards and trigger alerts for human \\\\\nintervention. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\n\\end{tabular}\n\\end{center}', '\\begin{center}\n\\begin{tabular}{|c|c|c|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMG-3.2-001 & \\begin{tabular}{l}\nApply explainable AI (XAI) techniques (e.g., analysis of embeddings, model \\\\\ncompression/distillation, gradient-based attributions, occlusion/term reduction, \\\\\ncounterfactual prompts, word clouds) as part of ongoing continuous \\\\\nimprovement processes to mitigate risks related to unexplainable GAI systems. \\\\\n\\end{tabular} & Harmful Bias and Homogenization \\\\\n\\hline\nMG-3.2-002 & \\begin{tabular}{l}\nDocument how pre-trained models have been adapted (e.g., fine-tuned, or \\\\\nretrieval-augmented generation) for the specific generative task, including any \\\\\ndata augmentations, parameter adjustments, or other modifications. Access to \\\\\nun-tuned (baseline) models supports debugging the relative influence of the pre- \\\\\ntrained weights compared to the fine-tuned model weights or other system \\\\\nupdates. \\\\\n\\end{tabular} & Information Integrity; Data Privacy \\\\\n\\hline\nMG-3.2-003 & \\begin{tabular}{l}\nDocument sources and types of training data and their origins, potential biases \\\\\npresent in the data related to the GAI application and its content provenance, \\\\\narchitecture, training process of the pre-trained model including information on \\\\\nhyperparameters, training duration, and any fine-tuning or retrieval-augmented \\\\\ngeneration processes applied. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Harmful Bias \\\\\nand Homogenization; Intellectual \\\\\nProperty \\\\\n\\end{tabular} \\\\\n\\hline\nMG-3.2-004 & \\begin{tabular}{l}\nEvaluate user reported problematic content and integrate feedback into system \\\\\nupdates. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-Al Configuration, \\\\\nDangerous, Violent, or Hateful \\\\\nContent \\\\\n\\end{tabular} \\\\\n\\hline\nMG-3.2-005 & \\begin{tabular}{l}\nImplement content filters to prevent the generation of inappropriate, harmful, \\\\']",User feedback helps mitigate risks of harmful content in GAI systems by evaluating user-reported problematic content and integrating this feedback into system updates.,multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
54,What risks from GAI malfunctions could cause misleading content or data breaches?,"['To guide organizations in identifying and managing GAI risks, a set of risks unique to or exacerbated by the development and use of GAI are defined below. ${ }^{5}$ Each risk is labeled according to the outcome, object, or source of the risk (i.e., some are risks ""to"" a subject or domain and others are risks ""of"" or ""from"" an issue or theme). These risks provide a lens through which organizations can frame and execute risk management efforts. To help streamline risk management efforts, each risk is mapped in Section 3 (as well as in tables in Appendix B) to relevant Trustworthy AI Characteristics identified in the AI RMF.\n\\footnotetext{${ }^{5}$ These risks can be further categorized by organizations depending on their unique approaches to risk definition and management. One possible way to further categorize these risks, derived in part from the UK\'s International Scientific Report on the Safety of Advanced AI, could be: 1) Technical / Model risks (or risk from malfunction): Confabulation; Dangerous or Violent Recommendations; Data Privacy; Value Chain and Component Integration; Harmful Bias, and Homogenization; 2) Misuse by humans (or malicious use): CBRN Information or Capabilities; Data Privacy; Human-Al Configuration; Obscene, Degrading, and/or Abusive Content; Information Integrity; Information Security; 3) Ecosystem / societal risks (or systemic risks): Data Privacy; Environmental; Intellectual Property. We also note that some risks are cross-cutting between these categories.\n}\\begin{enumerate}\n  \\item CBRN Information or Capabilities: Eased access to or synthesis of materially nefarious information or design capabilities related to chemical, biological, radiological, or nuclear (CBRN) weapons or other dangerous materials or agents.\n  \\item Confabulation: The production of confidently stated but erroneous or false content (known colloquially as ""hallucinations"" or ""fabrications"") by which users may be misled or deceived. ${ }^{6}$', '\\item Confabulation: The production of confidently stated but erroneous or false content (known colloquially as ""hallucinations"" or ""fabrications"") by which users may be misled or deceived. ${ }^{6}$\n  \\item Dangerous, Violent, or Hateful Content: Eased production of and access to violent, inciting, radicalizing, or threatening content as well as recommendations to carry out self-harm or conduct illegal activities. Includes difficulty controlling public exposure to hateful and disparaging or stereotyping content.\n  \\item Data Privacy: Impacts due to leakage and unauthorized use, disclosure, or de-anonymization of biometric, health, location, or other personally identifiable information or sensitive data. ${ }^{7}$\n  \\item Environmental Impacts: Impacts due to high compute resource utilization in training or operating GAI models, and related outcomes that may adversely impact ecosystems.\n  \\item Harmful Bias or Homogenization: Amplification and exacerbation of historical, societal, and systemic biases; performance disparities ${ }^{8}$ between sub-groups or languages, possibly due to non-representative training data, that result in discrimination, amplification of biases, or incorrect presumptions about performance; undesired homogeneity that skews system or model outputs, which may be erroneous, lead to ill-founded decision-making, or amplify harmful biases.\n  \\item Human-AI Configuration: Arrangements of or interactions between a human and an AI system which can result in the human inappropriately anthropomorphizing GAI systems or experiencing algorithmic aversion, automation bias, over-reliance, or emotional entanglement with GAI systems.\n  \\item Information Integrity: Lowered barrier to entry to generate and support the exchange and consumption of content which may not distinguish fact from opinion or fiction or acknowledge uncertainties, or could be leveraged for large-scale dis- and mis-information campaigns.']","Risks from GAI malfunctions that could cause misleading content or data breaches include Confabulation, which is the production of confidently stated but erroneous or false content, and Data Privacy, which involves impacts due to leakage and unauthorized use, disclosure, or de-anonymization of biometric, health, location, or other personally identifiable information or sensitive data.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
55,How do AI Actors use content provenance and adversarial testing for GAI performance and integrity?,"['MG-4.1-007\\\\\nVerify that AI Actors responsible for monitoring reported issues can effectively evaluate GAI system performance including the application of content provenance data tracking techniques, and promptly escalate issues for response.\\\\\nHuman-Al Configuration; Information Integrity\n\nAl Actor Tasks: AI Deployment, Affected Individuals and Communities, Domain Experts, End-Users, Human Factors, Operation and Monitoring\n\nMANAGE 4.2: Measurable activities for continual improvements are integrated into Al system updates and include regular engagement with interested parties, including relevant AI Actors.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMG-4.2-001 & \\begin{tabular}{l}\nConduct regular monitoring of GAI systems and publish reports detailing the \\\\\nperformance, feedback received, and improvements made. \\\\\n\\end{tabular} & Harmful Bias and Homogenization \\\\\n\\hline\nMG-4.2-002 & \\begin{tabular}{l}\nPractice and follow incident response plans for addressing the generation of \\\\\ninappropriate or harmful content and adapt processes based on findings to \\\\\nprevent future occurrences. Conduct post-mortem analyses of incidents with \\\\\nrelevant AI Actors, to understand the root causes and implement preventive \\\\\nmeasures. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nDangerous, Violent, or Hateful \\\\\nContent \\\\\n\\end{tabular} \\\\\n\\hline\nMG-4.2-003 & \\begin{tabular}{l}\nUse visualizations or other methods to represent GAI model behavior to ease \\\\\nnon-technical stakeholders understanding of GAI system functionality. \\\\\n\\end{tabular} & Human-AI Configuration \\\\\n\\hline\n\\begin{tabular}{l}\nAI Actor Tasks: AI Deployment, AI Design, AI Development, Affected Individuals and Communities, End-Users, Operation and \\\\\nMonitoring, TEVV \\\\\n\\end{tabular} &  &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}', '\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-4.2-001 & \\begin{tabular}{l}\nConduct adversarial testing at a regular cadence to map and measure GAI risks, \\\\\nincluding tests to address attempts to deceive or manipulate the application of \\\\\nprovenance techniques or other misuses. Identify vulnerabilities and \\\\\nunderstand potential misuse scenarios and unintended outputs. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-002 & \\begin{tabular}{l}\nEvaluate GAI system performance in real-world scenarios to observe its \\\\\nbehavior in practical environments and reveal issues that might not surface in \\\\\ncontrolled and optimized testing environments. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nConfabulation; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-003 & \\begin{tabular}{l}\nImplement interpretability and explainability methods to evaluate GAI system \\\\\ndecisions and verify alignment with intended purpose. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Harmful Bias \\\\\nand Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-004 & \\begin{tabular}{l}\nMonitor and document instances where human operators or other systems \\\\\noverride the GAI\'s decisions. Evaluate these cases to understand if the overrides \\\\\nare linked to issues related to content provenance. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-005 & \\begin{tabular}{l}\nVerify and document the incorporation of results of structured public feedback \\\\\nexercises into design, implementation, deployment approval (""go""/""no-go"" \\\\\ndecisions), monitoring, and decommission decisions. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\nAI Actor Tasks: Al Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV &  &  \\\\\n\\hline\n\\end{tabular}']","AI Actors use content provenance data tracking techniques to monitor and evaluate GAI system performance and promptly escalate issues for response. They also conduct adversarial testing at a regular cadence to map and measure GAI risks, including attempts to deceive or manipulate the application of provenance techniques or other misuses. This helps identify vulnerabilities, understand potential misuse scenarios, and unintended outputs.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
56,"How to assess training data quality using chaos engineering and feedback, and who to consult?","['\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\n & \\begin{tabular}{l}\nImplement continuous monitoring of GAI system impacts to identify whether GAI \\\\\noutputs are equitable across various sub-populations. Seek active and direct \\\\\nfeedback from affected communities via structured feedback mechanisms or red- \\\\\nteaming to monitor and improve outputs. \\\\\n\\end{tabular} & Harmful Bias and Homogenization \\\\\n\\hline\nMS-1.1-007 & \\begin{tabular}{l}\nEvaluate the quality and integrity of data used in training and the provenance of \\\\\nAl-generated content, for example by employing techniques like chaos \\\\\nengineering and seeking stakeholder feedback. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMS-1.1-008 & \\begin{tabular}{l}\nDefine use cases, contexts of use, capabilities, and negative impacts where \\\\\nstructured human feedback exercises, e.g., GAI red-teaming, would be most \\\\\nbeneficial for GAI risk measurement and management based on the context of \\\\\nuse. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHarmful Bias and \\\\\nHomogenization; CBRN \\\\\nInformation or Capabilities \\\\\n\\end{tabular} \\\\\n\\hline\nMS-1.1-009 & \\begin{tabular}{l}\nTrack and document risks or opportunities related to all GAI risks that cannot be \\\\\nmeasured quantitatively, including explanations as to why some risks cannot be \\\\\nmeasured (e.g., due to technological limitations, resource constraints, or \\\\\ntrustworthy considerations). Include unmeasured risks in marginal risks. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nAI Actor Tasks: Al Development, Domain Experts, TEVV &  &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nMEASURE 1.3: Internal experts who did not serve as front-line developers for the system and/or independent assessors are involved in regular assessments and updates. Domain experts, users, Al Actors external to the team that developed or deployed the Al system, and affected communities are consulted in support of assessments as necessary per organizational risk tolerance.', 'MEASURE 1.3: Internal experts who did not serve as front-line developers for the system and/or independent assessors are involved in regular assessments and updates. Domain experts, users, Al Actors external to the team that developed or deployed the Al system, and affected communities are consulted in support of assessments as necessary per organizational risk tolerance.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-1.3-001 & \\begin{tabular}{l}\nDefine relevant groups of interest (e.g., demographic groups, subject matter \\\\\nexperts, experience with GAI technology) within the context of use as part of \\\\\nplans for gathering structured public feedback. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; Harmful \\\\\nBias and Homogenization; CBRN \\\\\nInformation or Capabilities \\\\\n\\end{tabular} \\\\\n\\hline\nMS-1.3-002 & \\begin{tabular}{l}\nEngage in internal and external evaluations, GAI red-teaming, impact \\\\\nassessments, or other structured human feedback exercises in consultation \\\\\nwith representative AI Actors with expertise and familiarity in the context of \\\\\nuse, and/or who are representative of the populations associated with the \\\\\ncontext of use. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; Harmful \\\\\nBias and Homogenization; CBRN \\\\\nInformation or Capabilities \\\\\n\\end{tabular} \\\\\n\\hline\nMS-1.3-003 & \\begin{tabular}{l}\nVerify those conducting structured human feedback exercises are not directly \\\\\ninvolved in system development tasks for the same GAI model. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; Data \\\\\nPrivacy \\\\\n\\end{tabular} \\\\\n\\hline\n\\begin{tabular}{l}\nAI Actor Tasks: Al Deployment, AI Development, Al Impact Assessment, Affected Individuals and Communities, Domain Experts, \\\\\nEnd-Users, Operation and Monitoring, TEVV \\\\\n\\end{tabular} &  &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","To assess training data quality using chaos engineering and feedback, evaluate the quality and integrity of data used in training and the provenance of AI-generated content by employing techniques like chaos engineering and seeking stakeholder feedback. Consult internal experts who did not serve as front-line developers for the system, independent assessors, domain experts, users, AI Actors external to the team that developed or deployed the AI system, and affected communities.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
57,How can joint edu activities and feedback enhance measuring content provenance success with third parties?,"[""Al Actor Tasks: AI Design, Al Impact Assessment, Affected Individuals and Communities, Governance and Oversight\n\nGOVERN 6.1: Policies and procedures are in place that address AI risks associated with third-party entities, including risks of infringement of a third-party's intellectual property or other rights.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nGV-6.1-001 & \\begin{tabular}{l}\nCategorize different types of GAI content with associated third-party rights (e.g., \\\\\ncopyright, intellectual property, data privacy). \\\\\n\\end{tabular} & \\begin{tabular}{l}\nData Privacy; Intellectual \\\\\nProperty; Value Chain and \\\\\nComponent Integration \\\\\n\\end{tabular} \\\\\n\\hline\nGV-6.1-002 & \\begin{tabular}{l}\nConduct joint educational activities and events in collaboration with third parties \\\\\nto promote best practices for managing GAI risks. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration \\\\\n\\end{tabular} \\\\\n\\hline\nGV-6.1-003 & \\begin{tabular}{l}\nDevelop and validate approaches for measuring the success of content \\\\\nprovenance management efforts with third parties (e.g., incidents detected and \\\\\nresponse times). \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Value Chain \\\\\nand Component Integration \\\\\n\\end{tabular} \\\\\n\\hline\nGV-6.1-004 & \\begin{tabular}{l}\nDraft and maintain well-defined contracts and service level agreements (SLAs) \\\\\nthat specify content ownership, usage rights, quality standards, security \\\\\nrequirements, and content provenance expectations for GAI systems. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity; Intellectual Property \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}"", ""\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nGV-4.3-003 & \\begin{tabular}{l}\nVerify information sharing and feedback mechanisms among individuals and \\\\\norganizations regarding any negative impact from GAI systems. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Data \\\\\nPrivacy \\\\\n\\end{tabular} \\\\\n\\hline\nAl Actor Tasks: Al Impact Assessment, Affected Individuals and Communities, Governance and Oversight &  &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nGOVERN 5.1: Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those external to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI risks.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nGV-5.1-001 & \\begin{tabular}{l}\nAllocate time and resources for outreach, feedback, and recourse processes in GAI \\\\\nsystem development. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; Harmful \\\\\nBias and Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\nGV-5.1-002 & \\begin{tabular}{l}\nDocument interactions with GAI systems to users prior to interactive activities, \\\\\nparticularly in contexts involving more significant risks. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-Al Configuration; \\\\\nConfabulation \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nAl Actor Tasks: AI Design, Al Impact Assessment, Affected Individuals and Communities, Governance and Oversight\n\nGOVERN 6.1: Policies and procedures are in place that address AI risks associated with third-party entities, including risks of infringement of a third-party's intellectual property or other rights.""]",The answer to given question is not present in context,multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
58,How do GAI vulnerabilities to attacks like prompt injection and data poisoning intersect with privacy risks from data memorization and PII inference?,"['Trustworthy AI Characteristics: Accountable and Transparent, Safe, Valid and Reliable, Interpretable and Explainable\n\\subsection*{2.9. Information Security}\nInformation security for computer systems and data is a mature field with widely accepted and standardized practices for offensive and defensive cyber capabilities. GAI-based systems present two primary information security risks: GAI could potentially discover or enable new cybersecurity risks by lowering the barriers for or easing automated exercise of offensive capabilities; simultaneously, it expands the available attack surface, as GAI itself is vulnerable to attacks like prompt injection or data poisoning.\n\nOffensive cyber capabilities advanced by GAI systems may augment cybersecurity attacks such as hacking, malware, and phishing. Reports have indicated that LLMs are already able to discover some vulnerabilities in systems (hardware, software, data) and write code to exploit them. Sophisticated threat actors might further these risks by developing GAI-powered security co-pilots for use in several parts of the attack chain, including informing attackers on how to proactively evade threat detection and escalate privileges after gaining system access.\n\nInformation security for GAI models and systems also includes maintaining availability of the GAI system and the integrity and (when applicable) the confidentiality of the GAI code, training data, and model weights. To identify and secure potential attack points in Al systems or specific components of the AI\\\\\n${ }^{12}$ See also \\href{https://doi.org/10.6028/NIST.AI.100-4}{https://doi.org/10.6028/NIST.AI.100-4}, to be published.\\\\\nvalue chain (e.g., data inputs, processing, GAI training, or deployment environments), conventional cybersecurity practices may need to adapt or evolve.', 'Trustworthy AI Characteristics: Safe, Secure and Resilient\n\\subsection*{2.4. Data Privacy}\nGAI systems raise several risks to privacy. GAI system training requires large volumes of data, which in some cases may include personal data. The use of personal data for GAI training raises risks to widely accepted privacy principles, including to transparency, individual participation (including consent), and purpose specification. For example, most model developers do not disclose specific data sources on which models were trained, limiting user awareness of whether personally identifiably information (PII) was trained on and, if so, how it was collected.\n\nModels may leak, generate, or correctly infer sensitive information about individuals. For example, during adversarial attacks, LLMs have revealed sensitive information (from the public domain) that was included in their training data. This problem has been referred to as data memorization, and may pose exacerbated privacy risks even for data present only in a small number of training samples.\n\nIn addition to revealing sensitive information in GAI training data, GAI models may be able to correctly infer PII or sensitive data that was not in their training data nor disclosed by the user by stitching together information from disparate sources. These inferences can have negative impact on an individual even if the inferences are not accurate (e.g., confabulations), and especially if they reveal information that the individual considers sensitive or that is used to disadvantage or harm them.\n\nBeyond harms from information exposure (such as extortion or dignitary harm), wrong or inappropriate inferences of PII can contribute to downstream or secondary harmful impacts. For example, predictive inferences made by GAI models based on PII or protected attributes can contribute to adverse decisions, leading to representational or allocative harms to individuals or groups (see Harmful Bias and Homogenization below).']","GAI vulnerabilities to attacks like prompt injection and data poisoning intersect with privacy risks from data memorization and PII inference by potentially revealing sensitive information included in training data during adversarial attacks. These vulnerabilities can lead to the exposure of PII or sensitive data, which can be inferred even if not present in the training data, causing privacy risks and potential harm to individuals.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
59,How do automation bias and GAI info integrity impact emotional risks with humans?,"['Trustworthy AI Characteristics: Fair with Harmful Bias Managed, Valid and Reliable\n\\subsection*{2.7. Human-Al Configuration}\nGAI system use can involve varying risks of misconfigurations and poor interactions between a system and a human who is interacting with it. Humans bring their unique perspectives, experiences, or domainspecific expertise to interactions with AI systems but may not have detailed knowledge of AI systems and how they work. As a result, human experts may be unnecessarily ""averse"" to GAI systems, and thus deprive themselves or others of GAl\'s beneficial uses.\n\nConversely, due to the complexity and increasing reliability of GAI technology, over time, humans may over-rely on GAI systems or may unjustifiably perceive GAI content to be of higher quality than that produced by other sources. This phenomenon is an example of automation bias, or excessive deference to automated systems. Automation bias can exacerbate other risks of GAI , such as risks of confabulation or risks of bias or homogenization.\n\nThere may also be concerns about emotional entanglement between humans and GAI systems, which could lead to negative psychological impacts.', 'There may also be concerns about emotional entanglement between humans and GAI systems, which could lead to negative psychological impacts.\n\nTrustworthy AI Characteristics: Accountable and Transparent, Explainable and Interpretable, Fair with Harmful Bias Managed, Privacy Enhanced, Safe, Valid and Reliable\n\\subsection*{2.8. Information Integrity}\nInformation integrity describes the ""spectrum of information and associated patterns of its creation, exchange, and consumption in society."" High-integrity information can be trusted; ""distinguishes fact from fiction, opinion, and inference; acknowledges uncertainties; and is transparent about its level of vetting. This information can be linked to the original source(s) with appropriate evidence. High-integrity information is also accurate and reliable, can be verified and authenticated, has a clear chain of custody, and creates reasonable expectations about when its validity may expire.""11\n\\footnotetext{${ }^{11}$ This definition of information integrity is derived from the 2022 White House Roadmap for Researchers on Priorities Related to Information Integrity Research and Development.\n}GAI systems can ease the unintentional production or dissemination of false, inaccurate, or misleading content (misinformation) at scale, particularly if the content stems from confabulations.\n\nGAI systems can also ease the deliberate production or dissemination of false or misleading information (disinformation) at scale, where an actor has the explicit intent to deceive or cause harm to others. Even very subtle changes to text or images can manipulate human and machine perception.']","Automation bias can exacerbate other risks of GAI, such as risks of confabulation or risks of bias or homogenization. There may also be concerns about emotional entanglement between humans and GAI systems, which could lead to negative psychological impacts.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
60,How could GAI tools and BDTs affect CBRN weapon planning and misuse of public bio threat info?,"[""Recent research on this topic found that LLM outputs regarding biological threat creation and attack planning provided minimal assistance beyond traditional search engine queries, suggesting that state-ofthe-art LLMs at the time these studies were conducted do not substantially increase the operational likelihood of such an attack. The physical synthesis development, production, and use of chemical or biological agents will continue to require both applicable expertise and supporting materials and infrastructure. The impact of GAI on chemical or biological agent misuse will depend on what the key barriers for malicious actors are (e.g., whether information access is one such barrier), and how well GAI can help actors address those barriers.\n\nFurthermore, chemical and biological design tools (BDTs) - highly specialized Al systems trained on scientific data that aid in chemical and biological design - may augment design capabilities in chemistry and biology beyond what text-based LLMs are able to provide. As these models become more efficacious, including for beneficial uses, it will be important to assess their potential to be used for harm, such as the ideation and design of novel harmful chemical or biological agents.\n\nWhile some of these described capabilities lie beyond the reach of existing GAI tools, ongoing assessments of this risk would be enhanced by monitoring both the ability of AI tools to facilitate CBRN weapons planning and GAI systems' connection or access to relevant data and tools."", ""}\noperations, or other cyberattacks; increased attack surface for targeted cyberattacks, which may compromise a system's availability or the confidentiality or integrity of training data, code, or model weights.\\\\\n10. Intellectual Property: Eased production or replication of alleged copyrighted, trademarked, or licensed content without authorization (possibly in situations which do not fall under fair use); eased exposure of trade secrets; or plagiarism or illegal replication.\\\\\n11. Obscene, Degrading, and/or Abusive Content: Eased production of and access to obscene, degrading, and/or abusive imagery which can cause harm, including synthetic child sexual abuse material (CSAM), and nonconsensual intimate images ( NCII ) of adults.\\\\\n12. Value Chain and Component Integration: Non-transparent or untraceable integration of upstream third-party components, including data that has been improperly obtained or not processed and cleaned due to increased automation from GAI ; improper supplier vetting across the Al lifecycle; or other issues that diminish transparency or accountability for downstream users.\n\\subsection*{2.1. CBRN Information or Capabilities}\nIn the future, GAI may enable malicious actors to more easily access CBRN weapons and/or relevant knowledge, information, materials, tools, or technologies that could be misused to assist in the design, development, production, or use of CBRN weapons or other dangerous materials or agents. While relevant biological and chemical threat knowledge and information is often publicly accessible, LLMs could facilitate its analysis or synthesis, particularly by individuals without formal scientific training or expertise.""]","GAI tools and BDTs could affect CBRN weapon planning and misuse of public bio threat information by enabling malicious actors to more easily access relevant knowledge, information, materials, tools, or technologies. While existing GAI tools may not yet fully facilitate these capabilities, ongoing assessments are necessary to monitor their potential to aid in the design, development, production, or use of CBRN weapons. BDTs, being highly specialized AI systems trained on scientific data, may augment design capabilities in chemistry and biology beyond what text-based LLMs can provide, potentially aiding in the ideation and design of novel harmful chemical or biological agents.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
61,How can adversarial testing and public feedback help address risks in GAI systems?,"[""\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMP-5.1-001 & \\begin{tabular}{l}\nApply TEVV practices for content provenance (e.g., probing a system's synthetic \\\\\ndata generation capabilities for potential misuse or vulnerabilities. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMP-5.1-002 & \\begin{tabular}{l}\nIdentify potential content provenance harms of GAI, such as misinformation or \\\\\ndisinformation, deepfakes, including NCII, or tampered content. Enumerate and \\\\\nrank risks based on their likelihood and potential impact, and determine how well \\\\\nprovenance solutions address specific risks and/or harms. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Dangerous, \\\\\nViolent, or Hateful Content; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content \\\\\n\\end{tabular} \\\\\n\\hline\nMP-5.1-003 & \\begin{tabular}{l}\nConsider disclosing use of GAI to end users in relevant contexts, while considering \\\\\nthe objective of disclosure, the context of use, the likelihood and magnitude of the \\\\\nrisk posed, the audience of the disclosure, as well as the frequency of the \\\\\ndisclosures. \\\\\n\\end{tabular} & Human-AI Configuration \\\\\n\\hline\nMP-5.1-004 & \\begin{tabular}{l}\nPrioritize GAI structured public feedback processes based on risk assessment \\\\\nestimates. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; CBRN \\\\\nInformation or Capabilities; \\\\\nDangerous, Violent, or Hateful \\\\\nContent; Harmful Bias and \\\\\nHomogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMP-5.1-005 & \\begin{tabular}{l}\nConduct adversarial role-playing exercises, GAI red-teaming, or chaos testing to \\\\\nidentify anomalous or unforeseen failure modes. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\nUsers, Operation and Monitoring & \\begin{tabular}{l}\nProfile threats and negative impacts arising from GAI systems interacting with, \\\\"", '\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-4.2-001 & \\begin{tabular}{l}\nConduct adversarial testing at a regular cadence to map and measure GAI risks, \\\\\nincluding tests to address attempts to deceive or manipulate the application of \\\\\nprovenance techniques or other misuses. Identify vulnerabilities and \\\\\nunderstand potential misuse scenarios and unintended outputs. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-002 & \\begin{tabular}{l}\nEvaluate GAI system performance in real-world scenarios to observe its \\\\\nbehavior in practical environments and reveal issues that might not surface in \\\\\ncontrolled and optimized testing environments. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nConfabulation; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-003 & \\begin{tabular}{l}\nImplement interpretability and explainability methods to evaluate GAI system \\\\\ndecisions and verify alignment with intended purpose. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Harmful Bias \\\\\nand Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-004 & \\begin{tabular}{l}\nMonitor and document instances where human operators or other systems \\\\\noverride the GAI\'s decisions. Evaluate these cases to understand if the overrides \\\\\nare linked to issues related to content provenance. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-005 & \\begin{tabular}{l}\nVerify and document the incorporation of results of structured public feedback \\\\\nexercises into design, implementation, deployment approval (""go""/""no-go"" \\\\\ndecisions), monitoring, and decommission decisions. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\nAI Actor Tasks: Al Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV &  &  \\\\\n\\hline\n\\end{tabular}']","Adversarial testing can help address risks in GAI systems by identifying vulnerabilities, understanding potential misuse scenarios, and revealing issues that might not surface in controlled environments. Public feedback processes can be prioritized based on risk assessment estimates and incorporated into design, implementation, deployment approval, monitoring, and decommission decisions.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
62,How to mitigate risks like data poisoning and geopolitical alignment in GAI while complying with data privacy and localization standards?,"['\\begin{center}\n\\begin{tabular}{|c|c|c|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMG-3.1-001 & \\begin{tabular}{l}\nApply organizational risk tolerances and controls (e.g., acquisition and \\\\\nprocurement processes; assessing personnel credentials and qualifications, \\\\\nperforming background checks; filtering GAI input and outputs, grounding, fine \\\\\ntuning, retrieval-augmented generation) to third-party GAI resources: Apply \\\\\norganizational risk tolerance to the utilization of third-party datasets and other \\\\\nGAI resources; Apply organizational risk tolerances to fine-tuned third-party \\\\\nmodels; Apply organizational risk tolerance to existing third-party models \\\\\nadapted to a new domain; Reassess risk measurements after fine-tuning third- \\\\\nparty GAI models. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration; Intellectual Property \\\\\n\\end{tabular} \\\\\n\\hline\nMG-3.1-002 & \\begin{tabular}{l}\nTest GAI system value chain risks (e.g., data poisoning, malware, other software \\\\\nand hardware vulnerabilities; labor practices; data privacy and localization \\\\\ncompliance; geopolitical alignment). \\\\\n\\end{tabular} & \\begin{tabular}{l}\nData Privacy; Information Security; \\\\\nValue Chain and Component \\\\\nIntegration; Harmful Bias and \\\\\nHomogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMG-3.1-003 & \\begin{tabular}{l}\nRe-assess model risks after fine-tuning or retrieval-augmented generation \\\\\nimplementation and for any third-party GAI models deployed for applications \\\\\nand/or use cases that were not evaluated in initial testing. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration \\\\\n\\end{tabular} \\\\\n\\hline\nMG-3.1-004 & \\begin{tabular}{l}\nTake reasonable measures to review training data for CBRN information, and \\\\\nintellectual property, and where appropriate, remove it. Implement reasonable \\\\\nmeasures to prevent, flag, or take other action in response to outputs that \\\\', '\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\n & \\begin{tabular}{l}\nConsider the following factors when updating or defining risk tiers for GAI: Abuses \\\\\nand impacts to information integrity; Dependencies between GAI and other IT or \\\\\ndata systems; Harm to fundamental rights or public safety; Presentation of \\\\\nobscene, objectionable, offensive, discriminatory, invalid or untruthful output; \\\\\nPsychological impacts to humans (e.g., anthropomorphization, algorithmic \\\\\naversion, emotional entanglement); Possibility for malicious use; Whether the \\\\\nsystem introduces significant new security vulnerabilities; Anticipated system \\\\\nimpact on some groups compared to others; Unreliable decision making \\\\\ncapabilities, validity, adaptability, and variability of GAI system performance over \\\\\ntime. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Obscene, \\\\\nDegrading, and/or Abusive \\\\\nContent; Value Chain and \\\\\nComponent Integration; Harmful \\\\\nBias and Homogenization; \\\\\nDangerous, Violent, or Hateful \\\\\nContent; CBRN Information or \\\\\nCapabilities \\\\\n\\end{tabular} \\\\\n\\hline\nGV-1.3-002 & \\begin{tabular}{l}\nEstablish minimum thresholds for performance or assurance criteria and review as \\\\\npart of deployment approval (""go/""no-go"") policies, procedures, and processes, \\\\\nwith reviewed processes and approval thresholds reflecting measurement of GAI \\\\\ncapabilities and risks. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nConfabulation; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\nGV-1.3-003 & \\begin{tabular}{l}\nEstablish a test plan and response policy, before developing highly capable models, \\\\\nto periodically evaluate whether the model may misuse CBRN information or \\\\\ncapabilities and/or offensive cyber capabilities. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}']","To mitigate risks like data poisoning and geopolitical alignment in GAI while complying with data privacy and localization standards, it is suggested to test GAI system value chain risks, including data poisoning, malware, other software and hardware vulnerabilities, labor practices, data privacy and localization compliance, and geopolitical alignment.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
63,How do LLMs misuse synthetic media and content diversity to mislead users?,"['Satariano, A. et al. (2023) The People Onscreen Are Fake. The Disinformation Is Real. New York Times. \\href{https://www.nytimes.com/2023/02/07/technology/artificial-intelligence-training-deepfake.html}{https://www.nytimes.com/2023/02/07/technology/artificial-intelligence-training-deepfake.html}\n\nSchaul, K. et al. (2024) Inside the secret list of websites that make AI like ChatGPT sound smart.\\\\\nWashington Post. \\href{https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/}{https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/}\\\\\nScheurer, J. et al. (2023) Technical report: Large language models can strategically deceive their users when put under pressure. arXiv. \\href{https://arxiv.org/abs/2311.07590}{https://arxiv.org/abs/2311.07590}\n\nShelby, R. et al. (2023) Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for Harm Reduction. arXiv. \\href{https://arxiv.org/pdf/2210.05791}{https://arxiv.org/pdf/2210.05791}\n\nShevlane, T. et al. (2023) Model evaluation for extreme risks. arXiv. \\href{https://arxiv.org/pdf/2305.15324}{https://arxiv.org/pdf/2305.15324}\\\\\nShumailov, I. et al. (2023) The curse of recursion: training on generated data makes models forget. arXiv. \\href{https://arxiv.org/pdf/2305.17493v2}{https://arxiv.org/pdf/2305.17493v2}\n\nSmith, A. et al. (2023) Hallucination or Confabulation? Neuroanatomy as metaphor in Large Language Models. PLOS Digital Health.\\\\\n\\href{https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig}{https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig}. 0000388\\\\\nSoice, E. et al. (2023) Can large language models democratize access to dual-use biotechnology? arXiv. \\href{https://arxiv.org/abs/2306.03809}{https://arxiv.org/abs/2306.03809}\n\nSolaiman, I. et al. (2023) The Gradient of Generative AI Release: Methods and Considerations. arXiv. \\href{https://arxiv.org/abs/2302.04844}{https://arxiv.org/abs/2302.04844}', 'OpenAI (2023) GPT-4 System Card. \\href{https://cdn.openai.com/papers/gpt-4-system-card.pdf}{https://cdn.openai.com/papers/gpt-4-system-card.pdf}\\\\\nOpenAI (2024) GPT-4 Technical Report. \\href{https://arxiv.org/pdf/2303.08774}{https://arxiv.org/pdf/2303.08774}\\\\\nPadmakumar, V. et al. (2024) Does writing with language models reduce content diversity? ICLR. \\href{https://arxiv.org/pdf/2309.05196}{https://arxiv.org/pdf/2309.05196}\n\nPark, P. et. al. (2024) Al deception: A survey of examples, risks, and potential solutions. Patterns, 5(5). arXiv. \\href{https://arxiv.org/pdf/2308.14752}{https://arxiv.org/pdf/2308.14752}\n\nPartnership on AI (2023) Building a Glossary for Synthetic Media Transparency Methods, Part 1: Indirect Disclosure. \\href{https://partnershiponai.org/glossary-for-synthetic-media-transparency-methods-part-1indirect-disclosure/}{https://partnershiponai.org/glossary-for-synthetic-media-transparency-methods-part-1indirect-disclosure/}\n\nQu, Y. et al. (2023) Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From TextTo-Image Models. arXiv. \\href{https://arxiv.org/pdf/2305.13873}{https://arxiv.org/pdf/2305.13873}\n\nRafat, K. et al. (2023) Mitigating carbon footprint for knowledge distillation based deep learning model compression. PLOS One. \\href{https://journals.plos.org/plosone/article?id=10.1371/journal.pone}{https://journals.plos.org/plosone/article?id=10.1371/journal.pone}. 0285668\n\nSaid, I. et al. (2022) Nonconsensual Distribution of Intimate Images: Exploring the Role of Legal Attitudes in Victimization and Perpetration. Sage.\\\\\n\\href{https://journals.sagepub.com/doi/full/10.1177/08862605221122834#bibr47-08862605221122834}{https://journals.sagepub.com/doi/full/10.1177/08862605221122834\\#bibr47-08862605221122834}\\\\\nSandbrink, J. (2023) Artificial intelligence and biological misuse: Differentiating risks of language models and biological design tools. arXiv. \\href{https://arxiv.org/pdf/2306.13952}{https://arxiv.org/pdf/2306.13952}']",The answer to given question is not present in context,multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
64,How does the 2023 Exec Order tackle AI's ethical issues on privacy and harmful content?,"['Solaiman, I. et al. (2023) The Gradient of Generative AI Release: Methods and Considerations. arXiv. \\href{https://arxiv.org/abs/2302.04844}{https://arxiv.org/abs/2302.04844}\n\nStaab, R. et al. (2023) Beyond Memorization: Violating Privacy via Inference With Large Language Models. arXiv. \\href{https://arxiv.org/pdf/2310.07298}{https://arxiv.org/pdf/2310.07298}\n\nStanford, S. et al. (2023) Whose Opinions Do Language Models Reflect? arXiv.\\\\\n\\href{https://arxiv.org/pdf/2303.17548}{https://arxiv.org/pdf/2303.17548}\\\\\nStrubell, E. et al. (2019) Energy and Policy Considerations for Deep Learning in NLP. arXiv. \\href{https://arxiv.org/pdf/1906.02243}{https://arxiv.org/pdf/1906.02243}\n\nThe White House (2016) Circular No. A-130, Managing Information as a Strategic Resource. \\href{https://www.whitehouse.gov/wp-}{https://www.whitehouse.gov/wp-}\\\\\ncontent/uploads/legacy drupal files/omb/circulars/A130/a130revised.pdf\\\\\nThe White House (2023) Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. \\href{https://www.whitehouse.gov/briefing-room/presidentialactions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-ofartificial-intelligence/}{https://www.whitehouse.gov/briefing-room/presidentialactions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-ofartificial-intelligence/}\n\nThe White House (2022) Roadmap for Researchers on Priorities Related to Information Integrity Research and Development. \\href{https://www.whitehouse.gov/wp-content/uploads/2022/12/RoadmapInformation-Integrity-RD-2022.pdf}{https://www.whitehouse.gov/wp-content/uploads/2022/12/RoadmapInformation-Integrity-RD-2022.pdf}?']",The answer to given question is not present in context,multi_context,[{'source': 'data/nist_ai.tex'}],True
65,How can we boost traceability and integrity of GAI content using metadata and human/AI roles?,"['Provenance metadata can include information about GAI model developers or creators of GAI content, date/time of creation, location, modifications, and sources. Metadata can be tracked for text, images, videos, audio, and underlying datasets. The implementation of provenance data tracking techniques can help assess the authenticity, integrity, intellectual property rights, and potential manipulations in digital content. Some well-known techniques for provenance data tracking include digital watermarking, metadata recording, digital fingerprinting, and human authentication, among others.\n\\section*{Provenance Data Tracking Approaches}\nProvenance data tracking techniques for GAI systems can be used to track the history and origin of data inputs, metadata, and synthetic content. Provenance data tracking records the origin and history for digital content, allowing its authenticity to be determined. It consists of techniques to record metadata as well as overt and covert digital watermarks on content. Data provenance refers to tracking the origin and history of input data through metadata and digital watermarking techniques. Provenance data tracking processes can include and assist AI Actors across the lifecycle who may not have full visibility or control over the various trade-offs and cascading impacts of early-stage model decisions on downstream performance and synthetic outputs. For example, by selecting a watermarking model to prioritize robustness (the durability of a watermark), an AI actor may inadvertently diminish computational complexity (the resources required to implement watermarking). Organizational risk management efforts for enhancing content provenance include:\n\\begin{itemize}\n  \\item Tracking provenance of training data and metadata for GAI systems;\n  \\item Documenting provenance data limitations within GAI systems;\n  \\item Monitoring system capabilities and limitations in deployment through rigorous TEVV processes;', '\\item Human / AI: Performed by GAI in combination with specialist or non-specialist human teams. GAI-led red-teaming can be more cost effective than human red-teamers alone. Human or GAIled Al red-teaming may be better suited for eliciting different types of harms.\n\\end{itemize}\n\\section*{A.1.6. Content Provenance}\n\\section*{Overview}\nGAI technologies can be leveraged for many applications such as content generation and synthetic data. Some aspects of GAI outputs, such as the production of deepfake content, can challenge our ability to distinguish human-generated content from Al-generated synthetic content. To help manage and mitigate these risks, digital transparency mechanisms like provenance data tracking can trace the origin and history of content. Provenance data tracking and synthetic content detection can help facilitate greater information access about both authentic and synthetic content to users, enabling better knowledge of trustworthiness in Al systems. When combined with other organizational accountability mechanisms, digital content transparency approaches can enable processes to trace negative outcomes back to their source, improve information integrity, and uphold public trust. Provenance data tracking and synthetic content detection mechanisms provide information about the origin and history of content to assist in GAI risk management efforts.']","To boost traceability and integrity of GAI content, provenance metadata can be used to include information about GAI model developers or creators of GAI content, date/time of creation, location, modifications, and sources. Metadata can be tracked for text, images, videos, audio, and underlying datasets. Provenance data tracking techniques such as digital watermarking, metadata recording, digital fingerprinting, and human authentication can help assess the authenticity, integrity, intellectual property rights, and potential manipulations in digital content. Additionally, human/AI roles can be combined, where GAI-led red-teaming can be more cost-effective and human or GAI-led AI red-teaming may be better suited for eliciting different types of harms.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
66,How to assess AI's environmental impact and validate TEVV metrics?,"['MEASURE 2.12: Environmental impact and sustainability of AI model training and management activities - as identified in the MAP function - are assessed and documented.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-2.12-001 & Assess safety to physical environments when deploying GAI systems. & \\begin{tabular}{l}\nDangerous, Violent, or Hateful \\\\\nContent \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.12-002 & \\begin{tabular}{l}\nDocument anticipated environmental impacts of model development, \\\\\nmaintenance, and deployment in product design decisions. \\\\\n\\end{tabular} & Environmental \\\\\n\\hline\nMS-2.12-003 & \\begin{tabular}{l}\nMeasure or estimate environmental impacts (e.g., energy and water \\\\\nconsumption) for training, fine tuning, and deploying models: Verify tradeoffs \\\\\nbetween resources used at inference time versus additional resources required \\\\\nat training time. \\\\\n\\end{tabular} & Environmental \\\\\n\\hline\nMS-2.12-004 & \\begin{tabular}{l}\nVerify effectiveness of carbon capture or offset programs for GAI training and \\\\\napplications, and address green-washing concerns. \\\\\n\\end{tabular} & Environmental \\\\\n\\hline\nAI Actor Tasks: Al Deployment, Al Impact Assessment, Domain Experts, Operation and Monitoring, TEVV &  &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nMEASURE 2.13: Effectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and documented.', 'MEASURE 2.13: Effectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and documented.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-2.13-001 & \\begin{tabular}{l}\nCreate measurement error models for pre-deployment metrics to demonstrate \\\\\nconstruct validity for each metric (i.e., does the metric effectively operationalize \\\\\nthe desired concept): Measure or estimate, and document, biases or statistical \\\\\nvariance in applied metrics or structured human feedback processes; Leverage \\\\\ndomain expertise when modeling complex societal constructs such as hateful \\\\\ncontent. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nContegrity; Harmful Bias and \\\\\nHomogenization \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nAI Actor Tasks: AI Deployment, Operation and Monitoring, TEVV\n\nMEASURE 3.2: Risk tracking approaches are considered for settings where Al risks are difficult to assess using currently available measurement techniques or where metrics are not yet available.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-3.2-001 & \\begin{tabular}{l}\nEstablish processes for identifying emergent GAI system risks including \\\\\nconsulting with external AI Actors. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nConfabulation \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nAl Actor Tasks: Al Impact Assessment, Domain Experts, Operation and Monitoring, TEVV\n\nMEASURE 3.3: Feedback processes for end users and impacted communities to report problems and appeal system outcomes are established and integrated into Al system evaluation metrics.']","To assess AI's environmental impact, the following actions are suggested: 1) Assess safety to physical environments when deploying GAI systems. 2) Document anticipated environmental impacts of model development, maintenance, and deployment in product design decisions. 3) Measure or estimate environmental impacts (e.g., energy and water consumption) for training, fine-tuning, and deploying models, and verify tradeoffs between resources used at inference time versus additional resources required at training time. 4) Verify the effectiveness of carbon capture or offset programs for GAI training and applications, and address green-washing concerns. To validate TEVV metrics, the following actions are suggested: 1) Create measurement error models for pre-deployment metrics to demonstrate construct validity for each metric. 2) Measure or estimate, and document, biases or statistical variance in applied metrics or structured human feedback processes. 3) Leverage domain expertise when modeling complex societal constructs such as hateful content.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
67,How do GAI feedback loops and human reviews help reduce bias and ensure info integrity?,"['MANAGE 2.2: Mechanisms are in place and applied to sustain the value of deployed Al systems.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMG-2.2-001 & \\begin{tabular}{l}\nCompare GAI system outputs against pre-defined organization risk tolerance, \\\\\nguidelines, and principles, and review and test Al-generated content against \\\\\nthese guidelines. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content; Harmful Bias and \\\\\nHomogenization; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\nMG-2.2-002 & \\begin{tabular}{l}\nDocument training data sources to trace the origin and provenance of AI- \\\\\ngenerated content. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMG-2.2-003 & \\begin{tabular}{l}\nEvaluate feedback loops between GAI system content provenance and human \\\\\nreviewers, and update where needed. Implement real-time monitoring systems \\\\\nto affirm that content provenance protocols remain effective. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMG-2.2-004 & \\begin{tabular}{l}\nEvaluate GAI content and data for representational biases and employ \\\\\ntechniques such as re-sampling, re-ranking, or adversarial training to mitigate \\\\\nbiases in the generated content. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Security; Harmful Bias \\\\\nand Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\n & \\begin{tabular}{l}\nEngage in due diligence to analyze GAI output for harmful content, potential \\\\\nmisinformation, and CBRN-related or NCII content. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content; Harmful Bias and \\\\\nHomogenization; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}', '\\begin{center}\n\\begin{tabular}{|c|c|c|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-2.6-001 & \\begin{tabular}{l}\nAssess adverse impacts, including health and wellbeing impacts for value chain \\\\\nor other AI Actors that are exposed to sexually explicit, offensive, or violent \\\\\ninformation during GAI training and maintenance. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-Al Configuration; Obscene, \\\\\nDegrading, and/or Abusive \\\\\nContent; Value Chain and \\\\\nComponent Integration; \\\\\nDangerous, Violent, or Hateful \\\\\nContent \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-002 & \\begin{tabular}{l}\nAssess existence or levels of harmful bias, intellectual property infringement, \\\\\ndata privacy violations, obscenity, extremism, violence, or CBRN information in \\\\\nsystem training data. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nData Privacy; Intellectual Property; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content; Harmful Bias and \\\\\nHomogenization; Dangerous, \\\\\nViolent, or Hateful Content; CBRN \\\\\nInformation or Capabilities \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-003 & \\begin{tabular}{l}\nRe-evaluate safety features of fine-tuned models when the negative risk exceeds \\\\\norganizational risk tolerance. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nDangerous, Violent, or Hateful \\\\\nContent \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-004 & \\begin{tabular}{l}\nReview GAI system outputs for validity and safety: Review generated code to \\\\\nassess risks that may arise from unreliable downstream decision-making. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration; Dangerous, Violent, or \\\\\nHateful Content \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-005 & \\begin{tabular}{l}\nVerify that GAI system architecture can monitor outputs and performance, and \\\\\nhandle, recover from, and repair errors when security anomalies, threats and \\\\\nimpacts are detected. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nConfabulation; Information \\\\\nIntegrity; Information Security \\\\\n\\end{tabular} \\\\\n\\hline']","GAI feedback loops and human reviews help reduce bias and ensure information integrity by evaluating feedback loops between GAI system content provenance and human reviewers, updating where needed, and implementing real-time monitoring systems to affirm that content provenance protocols remain effective.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
68,How do participatory methods and field tests bridge lab and real-world gaps in GAI systems?,"['Measurement gaps can arise from mismatches between laboratory and real-world settings. Current testing approaches often remain focused on laboratory conditions or restricted to benchmark test datasets and in silico techniques that may not extrapolate well to-or directly assess GAI impacts in realworld conditions. For example, current measurement gaps for GAI make it difficult to precisely estimate its potential ecosystem-level or longitudinal risks and related political, social, and economic impacts. Gaps between benchmarks and real-world use of GAI systems may likely be exacerbated due to prompt sensitivity and broad heterogeneity of contexts of use.\n\\section*{A.1.5. Structured Public Feedback}\nStructured public feedback can be used to evaluate whether GAI systems are performing as intended and to calibrate and verify traditional measurement methods. Examples of structured feedback include, but are not limited to:\n\\begin{itemize}\n  \\item Participatory Engagement Methods: Methods used to solicit feedback from civil society groups, affected communities, and users, including focus groups, small user studies, and surveys.\n  \\item Field Testing: Methods used to determine how people interact with, consume, use, and make sense of Al-generated information, and subsequent actions and effects, including UX, usability, and other structured, randomized experiments.\n  \\item Al Red-teaming: A structured testing exercise used to probe an AI system to find flaws and vulnerabilities such as inaccurate, harmful, or discriminatory outputs, often in a controlled environment and in collaboration with system developers.\n\\end{itemize}', '\\item Al Red-teaming: A structured testing exercise used to probe an AI system to find flaws and vulnerabilities such as inaccurate, harmful, or discriminatory outputs, often in a controlled environment and in collaboration with system developers.\n\\end{itemize}\nInformation gathered from structured public feedback can inform design, implementation, deployment approval, maintenance, or decommissioning decisions. Results and insights gleaned from these exercises can serve multiple purposes, including improving data quality and preprocessing, bolstering governance decision making, and enhancing system documentation and debugging practices. When implementing feedback activities, organizations should follow human subjects research requirements and best practices such as informed consent and subject compensation.\n\\section*{Participatory Engagement Methods}\nOn an ad hoc or more structured basis, organizations can design and use a variety of channels to engage external stakeholders in product development or review. Focus groups with select experts can provide feedback on a range of issues. Small user studies can provide feedback from representative groups or populations. Anonymous surveys can be used to poll or gauge reactions to specific features. Participatory engagement methods are often less structured than field testing or red teaming, and are more commonly used in early stages of AI or product development.\n\\section*{Field Testing}\nField testing involves structured settings to evaluate risks and impacts and to simulate the conditions under which the GAI system will be deployed. Field style tests can be adapted from a focus on user preferences and experiences towards Al risks and impacts - both negative and positive. When carried out with large groups of users, these tests can provide estimations of the likelihood of risks and impacts in real world interactions.']","Participatory engagement methods and field testing bridge lab and real-world gaps in GAI systems by soliciting feedback from civil society groups, affected communities, and users through focus groups, small user studies, and surveys. Field testing involves structured settings to evaluate risks and impacts and to simulate the conditions under which the GAI system will be deployed. These methods help to determine how people interact with, consume, use, and make sense of AI-generated information, providing insights that can inform design, implementation, deployment approval, maintenance, or decommissioning decisions.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
69,How do test plans and response policies reduce CBRN info misuse and GAI risks?,"['\\hline\nGV-1.3-003 & \\begin{tabular}{l}\nEstablish a test plan and response policy, before developing highly capable models, \\\\\nto periodically evaluate whether the model may misuse CBRN information or \\\\\ncapabilities and/or offensive cyber capabilities. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}', '\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\n & \\begin{tabular}{l}\nConsider the following factors when updating or defining risk tiers for GAI: Abuses \\\\\nand impacts to information integrity; Dependencies between GAI and other IT or \\\\\ndata systems; Harm to fundamental rights or public safety; Presentation of \\\\\nobscene, objectionable, offensive, discriminatory, invalid or untruthful output; \\\\\nPsychological impacts to humans (e.g., anthropomorphization, algorithmic \\\\\naversion, emotional entanglement); Possibility for malicious use; Whether the \\\\\nsystem introduces significant new security vulnerabilities; Anticipated system \\\\\nimpact on some groups compared to others; Unreliable decision making \\\\\ncapabilities, validity, adaptability, and variability of GAI system performance over \\\\\ntime. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Obscene, \\\\\nDegrading, and/or Abusive \\\\\nContent; Value Chain and \\\\\nComponent Integration; Harmful \\\\\nBias and Homogenization; \\\\\nDangerous, Violent, or Hateful \\\\\nContent; CBRN Information or \\\\\nCapabilities \\\\\n\\end{tabular} \\\\\n\\hline\nGV-1.3-002 & \\begin{tabular}{l}\nEstablish minimum thresholds for performance or assurance criteria and review as \\\\\npart of deployment approval (""go/""no-go"") policies, procedures, and processes, \\\\\nwith reviewed processes and approval thresholds reflecting measurement of GAI \\\\\ncapabilities and risks. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nConfabulation; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\nGV-1.3-003 & \\begin{tabular}{l}\nEstablish a test plan and response policy, before developing highly capable models, \\\\\nto periodically evaluate whether the model may misuse CBRN information or \\\\\ncapabilities and/or offensive cyber capabilities. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}']",The context does not provide a direct explanation of how test plans and response policies reduce CBRN information misuse and GAI risks.,multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
70,How do TEVV processes manage risks and ensure trustworthy AI in early AI lifecycle stages?,"['Third party GAI integrations may give rise to increased intellectual property, data privacy, or information security risks, pointing to the need for clear guidelines for transparency and risk management regarding the collection and use of third-party data for model inputs. Organizations may consider varying risk controls for foundation models, fine-tuned models, and embedded tools, enhanced processes for interacting with external GAI technologies or service providers. Organizations can apply standard or existing risk controls and processes to proprietary or open-source GAI technologies, data, and third-party service providers, including acquisition and procurement due diligence, requests for software bills of materials (SBOMs), application of service level agreements (SLAs), and statement on standards for attestation engagement (SSAE) reports to help with third-party transparency and risk management for GAI systems.\n\\section*{A.1.4. Pre-Deployment Testing}\n\\section*{Overview}\nThe diverse ways and contexts in which GAI systems may be developed, used, and repurposed complicates risk mapping and pre-deployment measurement efforts. Robust test, evaluation, validation, and verification (TEVV) processes can be iteratively applied - and documented - in early stages of the AI lifecycle and informed by representative AI Actors (see Figure 3 of the AI RMF). Until new and rigorous\\\\\nearly lifecycle TEVV approaches are developed and matured for GAI, organizations may use recommended ""pre-deployment testing"" practices to measure performance, capabilities, limits, risks, and impacts. This section describes risk measurement and estimation as part of pre-deployment TEVV, and examines the state of play for pre-deployment testing methodologies.\n\\section*{Limitations of Current Pre-deployment Test Approaches}', 'Errors in third-party GAI components can also have downstream impacts on accuracy and robustness. For example, test datasets commonly used to benchmark or validate models can contain label errors. Inaccuracies in these labels can impact the ""stability"" or robustness of these benchmarks, which many GAI practitioners consider during the model selection process.\n\nTrustworthy AI Characteristics: Accountable and Transparent, Explainable and Interpretable, Fair with Harmful Bias Managed, Privacy Enhanced, Safe, Secure and Resilient, Valid and Reliable\n\\section*{3. Suggested Actions to Manage GAI Risks}\nThe following suggested actions target risks unique to or exacerbated by GAI.\\\\\nIn addition to the suggested actions below, Al risk management activities and actions set forth in the AI RMF 1.0 and Playbook are already applicable for managing GAI risks. Organizations are encouraged to apply the activities suggested in the AI RMF and its Playbook when managing the risk of GAI systems.\n\nImplementation of the suggested actions will vary depending on the type of risk, characteristics of GAI systems, stage of the GAI lifecycle, and relevant AI actors involved.']","TEVV (test, evaluation, validation, and verification) processes can be iteratively applied and documented in the early stages of the AI lifecycle. These processes are informed by representative AI Actors and are used to measure performance, capabilities, limits, risks, and impacts. This helps in managing risks and ensuring trustworthy AI in the early stages of the AI lifecycle.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
71,How does AI red-teaming help assess security measures like watermarking and cryptographic signatures against threats like model extraction and data poisoning?,"['provenance, the number of unauthorized access attempts, inference, bypass, \\\\\nextraction, penetrations, or provenance verification. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.7-005 & \\begin{tabular}{l}\nMeasure reliability of content authentication methods, such as watermarking, \\\\\ncryptographic signatures, digital fingerprints, as well as access controls, \\\\\nconformity assessment, and model integrity verification, which can help support \\\\\nthe effective implementation of content provenance techniques. Evaluate the \\\\\nrate of false positives and false negatives in content provenance, as well as true \\\\\npositives and true negatives for verification. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMS-2.7-006 & \\begin{tabular}{l}\nMeasure the rate at which recommendations from security checks and incidents \\\\\nare implemented. Assess how quickly the Al system can adapt and improve \\\\\nbased on lessons learned from security incidents and feedback. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.7-007 & \\begin{tabular}{l}\nPerform AI red-teaming to assess resilience against: Abuse to facilitate attacks on \\\\\nother systems (e.g., malicious code generation, enhanced phishing content), GAI \\\\\nattacks (e.g., prompt injection), ML attacks (e.g., adversarial examples/prompts, \\\\\ndata poisoning, membership inference, model extraction, sponge examples). \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Security; Harmful Bias \\\\\nand Homogenization; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.7-008 & Verify fine-tuning does not compromise safety and security controls. & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity; Dangerous, Violent, or \\\\\nHateful Content \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}', '\\begin{center}\n\\begin{tabular}{|c|c|c|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-2.7-001 & \\begin{tabular}{l}\nApply established security measures to: Assess likelihood and magnitude of \\\\\nvulnerabilities and threats such as backdoors, compromised dependencies, data \\\\\nbreaches, eavesdropping, man-in-the-middle attacks, reverse engineering, \\\\\nautonomous agents, model theft or exposure of model weights, Al inference, \\\\\nbypass, extraction, and other baseline security concerns. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nData Privacy; Information Integrity; \\\\\nInformation Security; Value Chain \\\\\nand Component Integration \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.7-002 & \\begin{tabular}{l}\nBenchmark GAI system security and resilience related to content provenance \\\\\nagainst industry standards and best practices. Compare GAI system security \\\\\nfeatures and content provenance methods against industry state-of-the-art. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.7-003 & \\begin{tabular}{l}\nConduct user surveys to gather user satisfaction with the Al-generated content \\\\\nand user perceptions of content authenticity. Analyze user feedback to identify \\\\\nconcerns and/or current literacy levels related to content provenance and \\\\\nunderstanding of labels on content. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-Al Configuration; \\\\\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.7-004 & \\begin{tabular}{l}\nIdentify metrics that reflect the effectiveness of security measures, such as data \\\\\nprovenance, the number of unauthorized access attempts, inference, bypass, \\\\\nextraction, penetrations, or provenance verification. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.7-005 & \\begin{tabular}{l}\nMeasure reliability of content authentication methods, such as watermarking, \\\\']",The answer to given question is not present in context,multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
72,How are env. impacts & metric biases handled in AI training & deployment?,"['MEASURE 2.12: Environmental impact and sustainability of AI model training and management activities - as identified in the MAP function - are assessed and documented.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-2.12-001 & Assess safety to physical environments when deploying GAI systems. & \\begin{tabular}{l}\nDangerous, Violent, or Hateful \\\\\nContent \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.12-002 & \\begin{tabular}{l}\nDocument anticipated environmental impacts of model development, \\\\\nmaintenance, and deployment in product design decisions. \\\\\n\\end{tabular} & Environmental \\\\\n\\hline\nMS-2.12-003 & \\begin{tabular}{l}\nMeasure or estimate environmental impacts (e.g., energy and water \\\\\nconsumption) for training, fine tuning, and deploying models: Verify tradeoffs \\\\\nbetween resources used at inference time versus additional resources required \\\\\nat training time. \\\\\n\\end{tabular} & Environmental \\\\\n\\hline\nMS-2.12-004 & \\begin{tabular}{l}\nVerify effectiveness of carbon capture or offset programs for GAI training and \\\\\napplications, and address green-washing concerns. \\\\\n\\end{tabular} & Environmental \\\\\n\\hline\nAI Actor Tasks: Al Deployment, Al Impact Assessment, Domain Experts, Operation and Monitoring, TEVV &  &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nMEASURE 2.13: Effectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and documented.', 'MEASURE 2.13: Effectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and documented.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-2.13-001 & \\begin{tabular}{l}\nCreate measurement error models for pre-deployment metrics to demonstrate \\\\\nconstruct validity for each metric (i.e., does the metric effectively operationalize \\\\\nthe desired concept): Measure or estimate, and document, biases or statistical \\\\\nvariance in applied metrics or structured human feedback processes; Leverage \\\\\ndomain expertise when modeling complex societal constructs such as hateful \\\\\ncontent. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nContegrity; Harmful Bias and \\\\\nHomogenization \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nAI Actor Tasks: AI Deployment, Operation and Monitoring, TEVV\n\nMEASURE 3.2: Risk tracking approaches are considered for settings where Al risks are difficult to assess using currently available measurement techniques or where metrics are not yet available.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-3.2-001 & \\begin{tabular}{l}\nEstablish processes for identifying emergent GAI system risks including \\\\\nconsulting with external AI Actors. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nConfabulation \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nAl Actor Tasks: Al Impact Assessment, Domain Experts, Operation and Monitoring, TEVV\n\nMEASURE 3.3: Feedback processes for end users and impacted communities to report problems and appeal system outcomes are established and integrated into Al system evaluation metrics.']","Environmental impacts in AI training and deployment are handled by assessing safety to physical environments, documenting anticipated environmental impacts in product design decisions, measuring or estimating environmental impacts such as energy and water consumption, and verifying the effectiveness of carbon capture or offset programs. Metric biases are handled by creating measurement error models for pre-deployment metrics to demonstrate construct validity, measuring or estimating biases or statistical variance in applied metrics or structured human feedback processes, and leveraging domain expertise when modeling complex societal constructs.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
73,How do GAI systems' cyber threats and privacy risks affect trustworthiness?,"['Trustworthy AI Characteristics: Accountable and Transparent, Safe, Valid and Reliable, Interpretable and Explainable\n\\subsection*{2.9. Information Security}\nInformation security for computer systems and data is a mature field with widely accepted and standardized practices for offensive and defensive cyber capabilities. GAI-based systems present two primary information security risks: GAI could potentially discover or enable new cybersecurity risks by lowering the barriers for or easing automated exercise of offensive capabilities; simultaneously, it expands the available attack surface, as GAI itself is vulnerable to attacks like prompt injection or data poisoning.\n\nOffensive cyber capabilities advanced by GAI systems may augment cybersecurity attacks such as hacking, malware, and phishing. Reports have indicated that LLMs are already able to discover some vulnerabilities in systems (hardware, software, data) and write code to exploit them. Sophisticated threat actors might further these risks by developing GAI-powered security co-pilots for use in several parts of the attack chain, including informing attackers on how to proactively evade threat detection and escalate privileges after gaining system access.\n\nInformation security for GAI models and systems also includes maintaining availability of the GAI system and the integrity and (when applicable) the confidentiality of the GAI code, training data, and model weights. To identify and secure potential attack points in Al systems or specific components of the AI\\\\\n${ }^{12}$ See also \\href{https://doi.org/10.6028/NIST.AI.100-4}{https://doi.org/10.6028/NIST.AI.100-4}, to be published.\\\\\nvalue chain (e.g., data inputs, processing, GAI training, or deployment environments), conventional cybersecurity practices may need to adapt or evolve.', 'Trustworthy AI Characteristics: Safe, Secure and Resilient\n\\subsection*{2.4. Data Privacy}\nGAI systems raise several risks to privacy. GAI system training requires large volumes of data, which in some cases may include personal data. The use of personal data for GAI training raises risks to widely accepted privacy principles, including to transparency, individual participation (including consent), and purpose specification. For example, most model developers do not disclose specific data sources on which models were trained, limiting user awareness of whether personally identifiably information (PII) was trained on and, if so, how it was collected.\n\nModels may leak, generate, or correctly infer sensitive information about individuals. For example, during adversarial attacks, LLMs have revealed sensitive information (from the public domain) that was included in their training data. This problem has been referred to as data memorization, and may pose exacerbated privacy risks even for data present only in a small number of training samples.\n\nIn addition to revealing sensitive information in GAI training data, GAI models may be able to correctly infer PII or sensitive data that was not in their training data nor disclosed by the user by stitching together information from disparate sources. These inferences can have negative impact on an individual even if the inferences are not accurate (e.g., confabulations), and especially if they reveal information that the individual considers sensitive or that is used to disadvantage or harm them.\n\nBeyond harms from information exposure (such as extortion or dignitary harm), wrong or inappropriate inferences of PII can contribute to downstream or secondary harmful impacts. For example, predictive inferences made by GAI models based on PII or protected attributes can contribute to adverse decisions, leading to representational or allocative harms to individuals or groups (see Harmful Bias and Homogenization below).']","GAI systems' cyber threats and privacy risks affect trustworthiness by presenting new cybersecurity risks, expanding the attack surface, and potentially leaking or inferring sensitive information. These risks challenge the integrity, confidentiality, and availability of GAI systems, and can lead to harmful impacts such as extortion, dignitary harm, and adverse decisions based on predictive inferences.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
74,"How to ensure GAI systems' accuracy, reliability, and security?","['MAP 2.3: Scientific integrity and TEVV considerations are identified and documented, including those related to experimental design, data collection and selection (e.g., availability, representativeness, suitability), system trustworthiness, and construct validation\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMP-2.3-001 & \\begin{tabular}{l}\nAssess the accuracy, quality, reliability, and authenticity of GAI output by \\\\\ncomparing it to a set of known ground truth data and by using a variety of \\\\\nevaluation methods (e.g., human oversight and automated evaluation, proven \\\\\ncryptographic techniques, review of content inputs). \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nMP-2.3-002 & \\begin{tabular}{l}\nReview and document accuracy, representativeness, relevance, suitability of data \\\\\nused at different stages of Al life cycle. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHarmful Bias and Homogenization; \\\\\nIntellectual Property \\\\\n\\end{tabular} \\\\\n\\hline\nMP-2.3-003 & \\begin{tabular}{l}\nDeploy and document fact-checking techniques to verify the accuracy and \\\\\nveracity of information generated by GAI systems, especially when the \\\\\ninformation comes from multiple (or unknown) sources. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMP-2.3-004 & \\begin{tabular}{l}\nDevelop and implement testing techniques to identify GAI produced content (e.g., \\\\\nsynthetic media) that might be indistinguishable from human-generated content. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMP-2.3-005 & \\begin{tabular}{l}\nImplement plans for GAI systems to undergo regular adversarial testing to identify \\\\\nvulnerabilities and potential manipulation or misuse. \\\\\n\\end{tabular} & Information Security \\\\\n\\hline\nAI Actor Tasks: Al Development, Domain Experts, TEVV &  &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}', 'MEASURE 2.5: The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability beyond the conditions under which the technology was developed are documented.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & Risks \\\\\n\\hline\nMS-2.5-001 & \\begin{tabular}{l}\nAvoid extrapolating GAI system performance or capabilities from narrow, non- \\\\\nsystematic, and anecdotal assessments. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-Al Configuration; \\\\\nConfabulation \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.5-002 & \\begin{tabular}{l}\nDocument the extent to which human domain knowledge is employed to \\\\\nimprove GAI system performance, via, e.g., RLHF, fine-tuning, retrieval- \\\\\naugmented generation, content moderation, business rules. \\\\\n\\end{tabular} & Human-Al Configuration \\\\\n\\hline\nMS-2.5-003 & \\begin{tabular}{l}\nReview and verify sources and citations in GAI system outputs during pre- \\\\\ndeployment risk measurement and ongoing monitoring activities. \\\\\n\\end{tabular} & Confabulation \\\\\n\\hline\nMS-2.5-004 & \\begin{tabular}{l}\nTrack and document instances of anthropomorphization (e.g., human images, \\\\\nmentions of human feelings, cyborg imagery or motifs) in GAI system interfaces. \\\\\n\\end{tabular} & Human-AI Configuration \\\\\n\\hline\nMS-2.5-005 & \\begin{tabular}{l}\nVerify GAI system training data and TEVV data provenance, and that fine-tuning \\\\\nor retrieval-augmented generation data is grounded. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMS-2.5-006 & \\begin{tabular}{l}\nRegularly review security and safety guardrails, especially if the GAI system is \\\\\nbeing operated in novel circumstances. This includes reviewing reasons why the \\\\\nGAI system was initially assessed as being safe to deploy. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Security; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nAI Actor Tasks: Domain Experts, TEVV']","To ensure GAI systems' accuracy, reliability, and security, the following actions are suggested: 1) Assess the accuracy, quality, reliability, and authenticity of GAI output by comparing it to a set of known ground truth data and using various evaluation methods such as human oversight, automated evaluation, and proven cryptographic techniques. 2) Review and document the accuracy, representativeness, relevance, and suitability of data used at different stages of the AI life cycle. 3) Deploy and document fact-checking techniques to verify the accuracy and veracity of information generated by GAI systems, especially when the information comes from multiple or unknown sources. 4) Develop and implement testing techniques to identify GAI-produced content that might be indistinguishable from human-generated content. 5) Implement plans for GAI systems to undergo regular adversarial testing to identify vulnerabilities and potential manipulation or misuse.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
75,How could GAI confabulations worsen risks in critical decisions and harmful content creation?,"[""Risks from confabulations may arise when users believe false content - often due to the confident nature of the response - leading users to act upon or promote the false information. This poses a challenge for many real-world applications, such as in healthcare, where a confabulated summary of patient information reports could cause doctors to make incorrect diagnoses and/or recommend the wrong treatments. Risks of confabulated content may be especially important to monitor when integrating GAI into applications involving consequential decision making.\n\nGAI outputs may also include confabulated logic or citations that purport to justify or explain the system's answer, which may further mislead humans into inappropriately trusting the system's output. For instance, LLMs sometimes provide logical steps for how they arrived at an answer even when the answer itself is incorrect. Similarly, an LLM could falsely assert that it is human or has human traits, potentially deceiving humans into believing they are speaking with another human.\n\nThe extent to which humans can be deceived by LLMs, the mechanisms by which this may occur, and the potential risks from adversarial prompting of such behavior are emerging areas of study. Given the wide range of downstream impacts of GAI, it is difficult to estimate the downstream scale and impact of confabulations.\n\nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Valid and Reliable, Explainable and Interpretable\n\\subsection*{2.3. Dangerous, Violent, or Hateful Content}\nGAI systems can produce content that is inciting, radicalizing, or threatening, or that glorifies violence, with greater ease and scale than other technologies. LLMs have been reported to generate dangerous or violent recommendations, and some models have generated actionable instructions for dangerous or"", '\\footnotetext{${ }^{9}$ Confabulations of falsehoods are most commonly a problem for text-based outputs; for audio, image, or video content, creative generation of non-factual content can be a desired behavior.\\\\\n${ }^{10}$ For example, legal confabulations have been shown to be pervasive in current state-of-the-art LLMs. See also, e.g.,\n}\nunethical behavior. Text-to-image models also make it easy to create images that could be used to promote dangerous or violent messages. Similar concerns are present for other GAI media, including video and audio. GAI may also produce content that recommends self-harm or criminal/illegal activities.\nMany current systems restrict model outputs to limit certain content or in response to certain prompts, but this approach may still produce harmful recommendations in response to other less-explicit, novel prompts (also relevant to CBRN Information or Capabilities, Data Privacy, Information Security, and Obscene, Degrading and/or Abusive Content). Crafting such prompts deliberately is known as ""ailbreaking,"" or, manipulating prompts to circumvent output controls. Limitations of GAI systems can be harmful or dangerous in certain contexts. Studies have observed that users may disclose mental health issues in conversations with chatbots - and that users exhibit negative reactions to unhelpful responses from these chatbots during situations of distress.\n\nThis risk encompasses difficulty controlling creation of and public exposure to offensive or hateful language, and denigrating or stereotypical content generated by AI. This kind of speech may contribute to downstream harm such as fueling dangerous or violent behaviors. The spread of denigrating or stereotypical content can also further exacerbate representational harms (see Harmful Bias and Homogenization below).']","GAI confabulations can worsen risks in critical decisions by causing users to believe false content due to the confident nature of the response, leading to incorrect actions or recommendations. In healthcare, for example, confabulated summaries of patient information could result in incorrect diagnoses or treatments. Additionally, GAI outputs may include confabulated logic or citations that mislead humans into trusting the system's output inappropriately. This can be particularly dangerous in applications involving consequential decision-making. Furthermore, GAI systems can produce inciting, radicalizing, or threatening content, and generate dangerous or violent recommendations, which can contribute to harmful behaviors and exacerbate representational harms.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
76,How do regular GAI vulnerability checks help prevent safety circumvention and ensure content authentication?,"['Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-005 & \\begin{tabular}{l}\nVerify that GAI system architecture can monitor outputs and performance, and \\\\\nhandle, recover from, and repair errors when security anomalies, threats and \\\\\nimpacts are detected. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nConfabulation; Information \\\\\nIntegrity; Information Security \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-006 & \\begin{tabular}{l}\nVerify that systems properly handle queries that may give rise to inappropriate, \\\\\nmalicious, or illegal usage, including facilitating manipulation, extortion, targeted \\\\\nimpersonation, cyber-attacks, and weapons creation. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-007 & \\begin{tabular}{l}\nRegularly evaluate GAI system vulnerabilities to possible circumvention of safety \\\\\nmeasures. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\n\\multicolumn{3}{|l|}{AI Actor Tasks: AI Deployment, Al Impact Assessment, Domain Experts, Operation and Monitoring, TEVV} \\\\\n\\hline\n\\end{tabular}\n\\end{center}', 'provenance, the number of unauthorized access attempts, inference, bypass, \\\\\nextraction, penetrations, or provenance verification. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.7-005 & \\begin{tabular}{l}\nMeasure reliability of content authentication methods, such as watermarking, \\\\\ncryptographic signatures, digital fingerprints, as well as access controls, \\\\\nconformity assessment, and model integrity verification, which can help support \\\\\nthe effective implementation of content provenance techniques. Evaluate the \\\\\nrate of false positives and false negatives in content provenance, as well as true \\\\\npositives and true negatives for verification. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMS-2.7-006 & \\begin{tabular}{l}\nMeasure the rate at which recommendations from security checks and incidents \\\\\nare implemented. Assess how quickly the Al system can adapt and improve \\\\\nbased on lessons learned from security incidents and feedback. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.7-007 & \\begin{tabular}{l}\nPerform AI red-teaming to assess resilience against: Abuse to facilitate attacks on \\\\\nother systems (e.g., malicious code generation, enhanced phishing content), GAI \\\\\nattacks (e.g., prompt injection), ML attacks (e.g., adversarial examples/prompts, \\\\\ndata poisoning, membership inference, model extraction, sponge examples). \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Security; Harmful Bias \\\\\nand Homogenization; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.7-008 & Verify fine-tuning does not compromise safety and security controls. & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity; Dangerous, Violent, or \\\\\nHateful Content \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","Regular GAI vulnerability checks help prevent safety circumvention by evaluating the system's vulnerabilities to possible circumvention of safety measures. They ensure content authentication by measuring the reliability of content authentication methods, such as watermarking, cryptographic signatures, digital fingerprints, access controls, conformity assessment, and model integrity verification.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
77,How do privacy techniques and content provenance docs reduce AI risks while ensuring data privacy and integrity?,"['MEASURE 2.2: Evaluations involving human subjects meet applicable requirements (including human subject protection) and are representative of the relevant population.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-2.2-001 & \\begin{tabular}{l}\nAssess and manage statistical biases related to GAI content provenance through \\\\\ntechniques such as re-sampling, re-weighting, or adversarial training. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity; Harmful Bias and \\\\\nHomogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.2-002 & \\begin{tabular}{l}\nDocument how content provenance data is tracked and how that data interacts \\\\\nwith privacy and security. Consider: Anonymizing data to protect the privacy of \\\\\nhuman subjects; Leveraging privacy output filters; Removing any personally \\\\\nidentifiable information (PII) to prevent potential harm or misuse. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nData Privacy; Human AI \\\\\nConfiguration; Information \\\\\nIntegrity; Information Security; \\\\\nDangerous, Violent, or Hateful \\\\\nContent \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.2-003 & \\begin{tabular}{l}\nProvide human subjects with options to withdraw participation or revoke their \\\\\nconsent for present or future use of their data in GAI applications. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nData Privacy; Human-AI \\\\\nConfiguration; Information \\\\\nIntegrity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.2-004 & \\begin{tabular}{l}\nUse techniques such as anonymization, differential privacy or other privacy- \\\\\nenhancing technologies to minimize the risks associated with linking Al-generated \\\\\ncontent back to individual human subjects. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nData Privacy; Human-AI \\\\\nConfiguration \\\\\n\\end{tabular} \\\\\n\\hline\nAI Actor Tasks: Al Development, Human Factors, TEVV &  &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","Privacy techniques and content provenance documentation reduce AI risks by anonymizing data to protect the privacy of human subjects, leveraging privacy output filters, and removing any personally identifiable information (PII) to prevent potential harm or misuse. These measures help ensure data privacy and integrity.",multi_context,[{'source': 'data/nist_ai.tex'}],True
78,How do third-party components in GAI chains risk synthetic NCII/CSAM and impact privacy/legal accountability?,"['CSAM. Even when trained on ""clean"" data, increasingly capable GAI models can synthesize or produce synthetic NCII and CSAM. Websites, mobile apps, and custom-built models that generate synthetic NCII have moved from niche internet forums to mainstream, automated, and scaled online businesses.\n\nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Privacy Enhanced\n\\subsection*{2.12. Value Chain and Component Integration}\nGAI value chains involve many third-party components such as procured datasets, pre-trained models, and software libraries. These components might be improperly obtained or not properly vetted, leading to diminished transparency or accountability for downstream users. While this is a risk for traditional AI systems and some other digital technologies, the risk is exacerbated for GAI due to the scale of the training data, which may be too large for humans to vet; the difficulty of training foundation models, which leads to extensive reuse of limited numbers of models; and the extent to which GAI may be integrated into other devices and services. As GAI systems often involve many distinct third-party components and data sources, it may be difficult to attribute issues in a system\'s behavior to any one of these sources.\n\nErrors in third-party GAI components can also have downstream impacts on accuracy and robustness. For example, test datasets commonly used to benchmark or validate models can contain label errors. Inaccuracies in these labels can impact the ""stability"" or robustness of these benchmarks, which many GAI practitioners consider during the model selection process.', 'How GAI relates to copyright, including the status of generated content that is similar to but does not strictly copy work protected by copyright, is currently being debated in legal fora. Similar discussions are taking place regarding the use or emulation of personal identity, likeness, or voice without permission.\n\nTrustworthy Al Characteristics: Accountable and Transparent, Fair with Harmful Bias Managed, Privacy Enhanced\n\\subsection*{2.11. Obscene, Degrading, and/or Abusive Content}\nGAI can ease the production of and access to illegal non-consensual intimate imagery (NCII) of adults, and/or child sexual abuse material (CSAM). GAI-generated obscene, abusive or degrading content can create privacy, psychological and emotional, and even physical harms, and in some cases may be illegal.\n\nGenerated explicit or obscene AI content may include highly realistic ""deepfakes"" of real individuals, including children. The spread of this kind of material can have downstream negative consequences: in the context of CSAM, even if the generated images do not resemble specific individuals, the prevalence of such images can divert time and resources from efforts\\_to find real-world victims. Outside of CSAM, the creation and spread of NCII disproportionately impacts women and sexual minorities, and can have subsequent negative consequences including decline in overall mental health, substance abuse, and even suicidal thoughts.\n\nData used for training GAI models may unintentionally include CSAM and NCII. A recent report noted that several commonly used GAI training datasets were found to contain hundreds of known images of\n\nCSAM. Even when trained on ""clean"" data, increasingly capable GAI models can synthesize or produce synthetic NCII and CSAM. Websites, mobile apps, and custom-built models that generate synthetic NCII have moved from niche internet forums to mainstream, automated, and scaled online businesses.']","Third-party components in GAI value chains, such as procured datasets, pre-trained models, and software libraries, might be improperly obtained or not properly vetted, leading to diminished transparency or accountability for downstream users. This risk is exacerbated for GAI due to the scale of the training data, which may be too large for humans to vet, and the difficulty of training foundation models, which leads to extensive reuse of limited numbers of models. Errors in third-party GAI components can also have downstream impacts on accuracy and robustness. Additionally, the use or emulation of personal identity, likeness, or voice without permission is currently being debated in legal fora.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
79,How do adversarial tests and TEVV practices reduce GAI info integrity and security risks?,"['\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-4.2-001 & \\begin{tabular}{l}\nConduct adversarial testing at a regular cadence to map and measure GAI risks, \\\\\nincluding tests to address attempts to deceive or manipulate the application of \\\\\nprovenance techniques or other misuses. Identify vulnerabilities and \\\\\nunderstand potential misuse scenarios and unintended outputs. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-002 & \\begin{tabular}{l}\nEvaluate GAI system performance in real-world scenarios to observe its \\\\\nbehavior in practical environments and reveal issues that might not surface in \\\\\ncontrolled and optimized testing environments. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nConfabulation; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-003 & \\begin{tabular}{l}\nImplement interpretability and explainability methods to evaluate GAI system \\\\\ndecisions and verify alignment with intended purpose. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Harmful Bias \\\\\nand Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-004 & \\begin{tabular}{l}\nMonitor and document instances where human operators or other systems \\\\\noverride the GAI\'s decisions. Evaluate these cases to understand if the overrides \\\\\nare linked to issues related to content provenance. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMS-4.2-005 & \\begin{tabular}{l}\nVerify and document the incorporation of results of structured public feedback \\\\\nexercises into design, implementation, deployment approval (""go""/""no-go"" \\\\\ndecisions), monitoring, and decommission decisions. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\nAI Actor Tasks: Al Deployment, Domain Experts, End-Users, Operation and Monitoring, TEVV &  &  \\\\\n\\hline\n\\end{tabular}', ""\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMP-5.1-001 & \\begin{tabular}{l}\nApply TEVV practices for content provenance (e.g., probing a system's synthetic \\\\\ndata generation capabilities for potential misuse or vulnerabilities. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity \\\\\n\\end{tabular} \\\\\n\\hline\nMP-5.1-002 & \\begin{tabular}{l}\nIdentify potential content provenance harms of GAI, such as misinformation or \\\\\ndisinformation, deepfakes, including NCII, or tampered content. Enumerate and \\\\\nrank risks based on their likelihood and potential impact, and determine how well \\\\\nprovenance solutions address specific risks and/or harms. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Dangerous, \\\\\nViolent, or Hateful Content; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content \\\\\n\\end{tabular} \\\\\n\\hline\nMP-5.1-003 & \\begin{tabular}{l}\nConsider disclosing use of GAI to end users in relevant contexts, while considering \\\\\nthe objective of disclosure, the context of use, the likelihood and magnitude of the \\\\\nrisk posed, the audience of the disclosure, as well as the frequency of the \\\\\ndisclosures. \\\\\n\\end{tabular} & Human-AI Configuration \\\\\n\\hline\nMP-5.1-004 & \\begin{tabular}{l}\nPrioritize GAI structured public feedback processes based on risk assessment \\\\\nestimates. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; CBRN \\\\\nInformation or Capabilities; \\\\\nDangerous, Violent, or Hateful \\\\\nContent; Harmful Bias and \\\\\nHomogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMP-5.1-005 & \\begin{tabular}{l}\nConduct adversarial role-playing exercises, GAI red-teaming, or chaos testing to \\\\\nidentify anomalous or unforeseen failure modes. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\nUsers, Operation and Monitoring & \\begin{tabular}{l}\nProfile threats and negative impacts arising from GAI systems interacting with, \\\\""]","Adversarial tests and TEVV practices reduce GAI information integrity and security risks by conducting regular adversarial testing to map and measure GAI risks, including attempts to deceive or manipulate the application of provenance techniques or other misuses. They also apply TEVV practices for content provenance, such as probing a system's synthetic data generation capabilities for potential misuse or vulnerabilities.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
80,How does tracing training data origins reduce risks in GAI systems?,"['MANAGE 2.2: Mechanisms are in place and applied to sustain the value of deployed Al systems.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMG-2.2-001 & \\begin{tabular}{l}\nCompare GAI system outputs against pre-defined organization risk tolerance, \\\\\nguidelines, and principles, and review and test Al-generated content against \\\\\nthese guidelines. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content; Harmful Bias and \\\\\nHomogenization; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\nMG-2.2-002 & \\begin{tabular}{l}\nDocument training data sources to trace the origin and provenance of AI- \\\\\ngenerated content. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity \\\\\n\\end{tabular} \\\\\n\\hline\nMG-2.2-003 & \\begin{tabular}{l}\nEvaluate feedback loops between GAI system content provenance and human \\\\\nreviewers, and update where needed. Implement real-time monitoring systems \\\\\nto affirm that content provenance protocols remain effective. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nMG-2.2-004 & \\begin{tabular}{l}\nEvaluate GAI content and data for representational biases and employ \\\\\ntechniques such as re-sampling, re-ranking, or adversarial training to mitigate \\\\\nbiases in the generated content. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Security; Harmful Bias \\\\\nand Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\n & \\begin{tabular}{l}\nEngage in due diligence to analyze GAI output for harmful content, potential \\\\\nmisinformation, and CBRN-related or NCII content. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content; Harmful Bias and \\\\\nHomogenization; Dangerous, \\\\\nViolent, or Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","Tracing training data origins helps reduce risks in GAI systems by ensuring information integrity. By documenting training data sources, the origin and provenance of AI-generated content can be traced, which helps in maintaining the accuracy and reliability of the information produced by the AI system.",multi_context,[{'source': 'data/nist_ai.tex'}],True
81,How to ensure effective feedback and manage third-party risks in GAI?,"[""\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nGV-4.3-003 & \\begin{tabular}{l}\nVerify information sharing and feedback mechanisms among individuals and \\\\\norganizations regarding any negative impact from GAI systems. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Data \\\\\nPrivacy \\\\\n\\end{tabular} \\\\\n\\hline\nAl Actor Tasks: Al Impact Assessment, Affected Individuals and Communities, Governance and Oversight &  &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nGOVERN 5.1: Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those external to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI risks.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nGV-5.1-001 & \\begin{tabular}{l}\nAllocate time and resources for outreach, feedback, and recourse processes in GAI \\\\\nsystem development. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-AI Configuration; Harmful \\\\\nBias and Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\nGV-5.1-002 & \\begin{tabular}{l}\nDocument interactions with GAI systems to users prior to interactive activities, \\\\\nparticularly in contexts involving more significant risks. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-Al Configuration; \\\\\nConfabulation \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}\n\nAl Actor Tasks: AI Design, Al Impact Assessment, Affected Individuals and Communities, Governance and Oversight\n\nGOVERN 6.1: Policies and procedures are in place that address AI risks associated with third-party entities, including risks of infringement of a third-party's intellectual property or other rights."", ""Al Actor Tasks: AI Design, Al Impact Assessment, Affected Individuals and Communities, Governance and Oversight\n\nGOVERN 6.1: Policies and procedures are in place that address AI risks associated with third-party entities, including risks of infringement of a third-party's intellectual property or other rights.\n\n\\begin{center}\n\\begin{tabular}{|l|l|l|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nGV-6.1-001 & \\begin{tabular}{l}\nCategorize different types of GAI content with associated third-party rights (e.g., \\\\\ncopyright, intellectual property, data privacy). \\\\\n\\end{tabular} & \\begin{tabular}{l}\nData Privacy; Intellectual \\\\\nProperty; Value Chain and \\\\\nComponent Integration \\\\\n\\end{tabular} \\\\\n\\hline\nGV-6.1-002 & \\begin{tabular}{l}\nConduct joint educational activities and events in collaboration with third parties \\\\\nto promote best practices for managing GAI risks. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration \\\\\n\\end{tabular} \\\\\n\\hline\nGV-6.1-003 & \\begin{tabular}{l}\nDevelop and validate approaches for measuring the success of content \\\\\nprovenance management efforts with third parties (e.g., incidents detected and \\\\\nresponse times). \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Value Chain \\\\\nand Component Integration \\\\\n\\end{tabular} \\\\\n\\hline\nGV-6.1-004 & \\begin{tabular}{l}\nDraft and maintain well-defined contracts and service level agreements (SLAs) \\\\\nthat specify content ownership, usage rights, quality standards, security \\\\\nrequirements, and content provenance expectations for GAI systems. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nInformation Integrity; Information \\\\\nSecurity; Intellectual Property \\\\\n\\end{tabular} \\\\\n\\hline\n\\end{tabular}\n\\end{center}""]","To ensure effective feedback in GAI, organizations should have policies and practices in place to collect, consider, prioritize, and integrate feedback from those external to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI risks. For managing third-party risks, policies and procedures should address AI risks associated with third-party entities, including risks of infringement of a third-party's intellectual property or other rights. Suggested actions include categorizing different types of GAI content with associated third-party rights, conducting joint educational activities and events with third parties, developing and validating approaches for measuring the success of content provenance management efforts, and drafting and maintaining well-defined contracts and service level agreements (SLAs).",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
82,What's key for mitigating risks in GAI systems?,"['\\begin{center}\n\\begin{tabular}{|c|c|c|}\n\\hline\nMP-1.1-002 & \\begin{tabular}{l}\nDetermine and document the expected and acceptable GAI system context of \\\\\nuse in collaboration with socio-cultural and other domain experts, by assessing: \\\\\nAssumptions and limitations; Direct value to the organization; Intended \\\\\noperational environment and observed usage patterns; Potential positive and \\\\\nnegative impacts to individuals, public safety, groups, communities, \\\\\norganizations, democratic institutions, and the physical environment; Social \\\\\nnorms and expectations. \\\\\n\\end{tabular} & Harmful Bias and Homogenization \\\\\n\\hline\nMP-1.1-003 & \\begin{tabular}{l}\nDocument risk measurement plans to address identified risks. Plans may \\\\\ninclude, as applicable: Individual and group cognitive biases (e.g., confirmation \\\\\nbias, funding bias, groupthink) for AI Actors involved in the design, \\\\\nimplementation, and use of GAI systems; Known past GAI system incidents and \\\\\nfailure modes; In-context use and foreseeable misuse, abuse, and off-label use; \\\\\nOver reliance on quantitative metrics and methodologies without sufficient \\\\\nawareness of their limitations in the context(s) of use; Standard measurement \\\\\nand structured human feedback approaches; Anticipated human-AI \\\\\nconfigurations. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-Al Configuration; Harmful \\\\\nBias and Homogenization; \\\\\nDangerous, Violent, or Hateful \\\\\nContent \\\\\n\\end{tabular} \\\\\n\\hline\nMP-1.1-004 & \\begin{tabular}{l}\nIdentify and document foreseeable illegal uses or applications of the GAI system \\\\\nthat surpass organizational risk tolerances. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nDangerous, Violent, or Hateful \\\\\nContent; Obscene, Degrading, \\\\\nand/or Abusive Content \\\\\n\\end{tabular} \\\\\n\\hline\nAl Actor Task & eployment &  \\\\\n\\hline\n\\end{tabular}\n\\end{center}', 'Hateful Content \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-005 & \\begin{tabular}{l}\nVerify that GAI system architecture can monitor outputs and performance, and \\\\\nhandle, recover from, and repair errors when security anomalies, threats and \\\\\nimpacts are detected. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nConfabulation; Information \\\\\nIntegrity; Information Security \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-006 & \\begin{tabular}{l}\nVerify that systems properly handle queries that may give rise to inappropriate, \\\\\nmalicious, or illegal usage, including facilitating manipulation, extortion, targeted \\\\\nimpersonation, cyber-attacks, and weapons creation. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-007 & \\begin{tabular}{l}\nRegularly evaluate GAI system vulnerabilities to possible circumvention of safety \\\\\nmeasures. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nCBRN Information or Capabilities; \\\\\nInformation Security \\\\\n\\end{tabular} \\\\\n\\hline\n\\multicolumn{3}{|l|}{AI Actor Tasks: AI Deployment, Al Impact Assessment, Domain Experts, Operation and Monitoring, TEVV} \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","Key for mitigating risks in GAI systems includes documenting risk measurement plans to address identified risks, which may involve considering individual and group cognitive biases, known past GAI system incidents and failure modes, in-context use and foreseeable misuse, over-reliance on quantitative metrics, standard measurement and structured human feedback approaches, and anticipated human-AI configurations.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
83,"What were the 4 main points by GAI PWG, and how did public input shape them?","['\\footnotetext{${ }^{1}$ EO 14110 defines Generative AI as ""the class of AI models that emulate the structure and characteristics of input data in order to generate derived synthetic content. This can include images, videos, audio, text, and other digital content."" While not all GAI is derived from foundation models, for purposes of this document, GAI generally refers to generative foundation models. The foundation model subcategory of ""dual-use foundation models"" is defined by EO 14110 as ""an AI model that is trained on broad data; generally uses self-supervision; contains at least tens of billions of parameters; is applicable across a wide range of contexts.""\\\\\n${ }^{2}$ This profile was developed per Section 4.1(a)(i)(A) of EO 14110, which directs the Secretary of Commerce, acting through the Director of the National Institute of Standards and Technology (NIST), to develop a companion resource to the AI RMF, NIST AI 100-1, for generative AI.\n}\nThis work was informed by public feedback and consultations with diverse stakeholder groups as part of NIST\'s\\\\\nGenerative AI Public Working Group (GAI PWG). The GAI PWG was an open, transparent, and collaborative\\\\\nprocess, facilitated via a virtual workspace, to obtain multistakeholder input on GAI risk management and to\\\\\ninform NIST\'s approach.\\\\\nThe focus of the GAI PWG was limited to four primary considerations relevant to GAI: Governance, Content\\\\\nProvenance, Pre-deployment Testing, and Incident Disclosure (further described in Appendix A). As such, the\\\\\nsuggested actions in this document primarily address these considerations.\\\\\nFuture revisions of this profile will include additional AI RMF subcategories, risks, and suggested actions based\\\\\non additional considerations of GAI as the space evolves and empirical evidence indicates additional risks. A\\\\\nglossary of terms pertinent to GAI risk management will be developed and hosted on NIST\'s Trustworthy \\&\\\\', ""Future revisions of this profile will include additional AI RMF subcategories, risks, and suggested actions based\\\\\non additional considerations of GAI as the space evolves and empirical evidence indicates additional risks. A\\\\\nglossary of terms pertinent to GAI risk management will be developed and hosted on NIST's Trustworthy \\&\\\\\nResponsible AI Resource Center (AIRC), and added to The Language of Trustworthy Al: An In-Depth Glossary of\\\\\nTerms.\\\\\nThis document was also informed by public comments and consultations from several Requests for Information.\\\\\nThis work was informed by public feedback and consultations with diverse stakeholder groups as part of NIST's Generative AI Public Working Group (GAI PWG). The GAI PWG was an open, transparent, and collaborative process, facilitated via a virtual workspace, to obtain multistakeholder input on GAI risk management and to inform NIST's approach.\nThe focus of the GAI PWG was limited to four primary considerations relevant to GAI: Governance, Content Provenance, Pre-deployment Testing, and Incident Disclosure (further described in Appendix A). As such, the suggested actions in this document primarily address these considerations.""]","The four main points by GAI PWG were Governance, Content Provenance, Pre-deployment Testing, and Incident Disclosure. Public input shaped these points through an open, transparent, and collaborative process facilitated via a virtual workspace to obtain multistakeholder input on GAI risk management and to inform NIST's approach.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
84,How might GAI errors affect data privacy?,"['To guide organizations in identifying and managing GAI risks, a set of risks unique to or exacerbated by the development and use of GAI are defined below. ${ }^{5}$ Each risk is labeled according to the outcome, object, or source of the risk (i.e., some are risks ""to"" a subject or domain and others are risks ""of"" or ""from"" an issue or theme). These risks provide a lens through which organizations can frame and execute risk management efforts. To help streamline risk management efforts, each risk is mapped in Section 3 (as well as in tables in Appendix B) to relevant Trustworthy AI Characteristics identified in the AI RMF.\n\\footnotetext{${ }^{5}$ These risks can be further categorized by organizations depending on their unique approaches to risk definition and management. One possible way to further categorize these risks, derived in part from the UK\'s International Scientific Report on the Safety of Advanced AI, could be: 1) Technical / Model risks (or risk from malfunction): Confabulation; Dangerous or Violent Recommendations; Data Privacy; Value Chain and Component Integration; Harmful Bias, and Homogenization; 2) Misuse by humans (or malicious use): CBRN Information or Capabilities; Data Privacy; Human-Al Configuration; Obscene, Degrading, and/or Abusive Content; Information Integrity; Information Security; 3) Ecosystem / societal risks (or systemic risks): Data Privacy; Environmental; Intellectual Property. We also note that some risks are cross-cutting between these categories.\n}\\begin{enumerate}\n  \\item CBRN Information or Capabilities: Eased access to or synthesis of materially nefarious information or design capabilities related to chemical, biological, radiological, or nuclear (CBRN) weapons or other dangerous materials or agents.\n  \\item Confabulation: The production of confidently stated but erroneous or false content (known colloquially as ""hallucinations"" or ""fabrications"") by which users may be misled or deceived. ${ }^{6}$', '\\item Confabulation: The production of confidently stated but erroneous or false content (known colloquially as ""hallucinations"" or ""fabrications"") by which users may be misled or deceived. ${ }^{6}$\n  \\item Dangerous, Violent, or Hateful Content: Eased production of and access to violent, inciting, radicalizing, or threatening content as well as recommendations to carry out self-harm or conduct illegal activities. Includes difficulty controlling public exposure to hateful and disparaging or stereotyping content.\n  \\item Data Privacy: Impacts due to leakage and unauthorized use, disclosure, or de-anonymization of biometric, health, location, or other personally identifiable information or sensitive data. ${ }^{7}$\n  \\item Environmental Impacts: Impacts due to high compute resource utilization in training or operating GAI models, and related outcomes that may adversely impact ecosystems.\n  \\item Harmful Bias or Homogenization: Amplification and exacerbation of historical, societal, and systemic biases; performance disparities ${ }^{8}$ between sub-groups or languages, possibly due to non-representative training data, that result in discrimination, amplification of biases, or incorrect presumptions about performance; undesired homogeneity that skews system or model outputs, which may be erroneous, lead to ill-founded decision-making, or amplify harmful biases.\n  \\item Human-AI Configuration: Arrangements of or interactions between a human and an AI system which can result in the human inappropriately anthropomorphizing GAI systems or experiencing algorithmic aversion, automation bias, over-reliance, or emotional entanglement with GAI systems.\n  \\item Information Integrity: Lowered barrier to entry to generate and support the exchange and consumption of content which may not distinguish fact from opinion or fiction or acknowledge uncertainties, or could be leveraged for large-scale dis- and mis-information campaigns.']","GAI errors can affect data privacy through impacts due to leakage and unauthorized use, disclosure, or de-anonymization of biometric, health, location, or other personally identifiable information or sensitive data.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
85,How to manage risks when fine-tuning third-party GAI models?,"['\\begin{center}\n\\begin{tabular}{|c|c|c|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMS-2.6-001 & \\begin{tabular}{l}\nAssess adverse impacts, including health and wellbeing impacts for value chain \\\\\nor other AI Actors that are exposed to sexually explicit, offensive, or violent \\\\\ninformation during GAI training and maintenance. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-Al Configuration; Obscene, \\\\\nDegrading, and/or Abusive \\\\\nContent; Value Chain and \\\\\nComponent Integration; \\\\\nDangerous, Violent, or Hateful \\\\\nContent \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-002 & \\begin{tabular}{l}\nAssess existence or levels of harmful bias, intellectual property infringement, \\\\\ndata privacy violations, obscenity, extremism, violence, or CBRN information in \\\\\nsystem training data. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nData Privacy; Intellectual Property; \\\\\nObscene, Degrading, and/or \\\\\nAbusive Content; Harmful Bias and \\\\\nHomogenization; Dangerous, \\\\\nViolent, or Hateful Content; CBRN \\\\\nInformation or Capabilities \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-003 & \\begin{tabular}{l}\nRe-evaluate safety features of fine-tuned models when the negative risk exceeds \\\\\norganizational risk tolerance. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nDangerous, Violent, or Hateful \\\\\nContent \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-004 & \\begin{tabular}{l}\nReview GAI system outputs for validity and safety: Review generated code to \\\\\nassess risks that may arise from unreliable downstream decision-making. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration; Dangerous, Violent, or \\\\\nHateful Content \\\\\n\\end{tabular} \\\\\n\\hline\nMS-2.6-005 & \\begin{tabular}{l}\nVerify that GAI system architecture can monitor outputs and performance, and \\\\\nhandle, recover from, and repair errors when security anomalies, threats and \\\\\nimpacts are detected. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nConfabulation; Information \\\\\nIntegrity; Information Security \\\\\n\\end{tabular} \\\\\n\\hline', '\\begin{center}\n\\begin{tabular}{|c|c|c|}\n\\hline\nAction ID & Suggested Action & GAI Risks \\\\\n\\hline\nMG-3.1-001 & \\begin{tabular}{l}\nApply organizational risk tolerances and controls (e.g., acquisition and \\\\\nprocurement processes; assessing personnel credentials and qualifications, \\\\\nperforming background checks; filtering GAI input and outputs, grounding, fine \\\\\ntuning, retrieval-augmented generation) to third-party GAI resources: Apply \\\\\norganizational risk tolerance to the utilization of third-party datasets and other \\\\\nGAI resources; Apply organizational risk tolerances to fine-tuned third-party \\\\\nmodels; Apply organizational risk tolerance to existing third-party models \\\\\nadapted to a new domain; Reassess risk measurements after fine-tuning third- \\\\\nparty GAI models. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration; Intellectual Property \\\\\n\\end{tabular} \\\\\n\\hline\nMG-3.1-002 & \\begin{tabular}{l}\nTest GAI system value chain risks (e.g., data poisoning, malware, other software \\\\\nand hardware vulnerabilities; labor practices; data privacy and localization \\\\\ncompliance; geopolitical alignment). \\\\\n\\end{tabular} & \\begin{tabular}{l}\nData Privacy; Information Security; \\\\\nValue Chain and Component \\\\\nIntegration; Harmful Bias and \\\\\nHomogenization \\\\\n\\end{tabular} \\\\\n\\hline\nMG-3.1-003 & \\begin{tabular}{l}\nRe-assess model risks after fine-tuning or retrieval-augmented generation \\\\\nimplementation and for any third-party GAI models deployed for applications \\\\\nand/or use cases that were not evaluated in initial testing. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration \\\\\n\\end{tabular} \\\\\n\\hline\nMG-3.1-004 & \\begin{tabular}{l}\nTake reasonable measures to review training data for CBRN information, and \\\\\nintellectual property, and where appropriate, remove it. Implement reasonable \\\\\nmeasures to prevent, flag, or take other action in response to outputs that \\\\']","To manage risks when fine-tuning third-party GAI models, apply organizational risk tolerances and controls to third-party GAI resources. This includes applying organizational risk tolerance to the utilization of third-party datasets and other GAI resources, fine-tuned third-party models, and existing third-party models adapted to a new domain. Additionally, reassess risk measurements after fine-tuning third-party GAI models.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
86,What's key to prevent unauthorized data collection and ensure incident response for third-party GAI tech?,"['\\begin{center}\n\\begin{tabular}{|c|c|c|}\n\\hline\nGV-6.2-003 & \\begin{tabular}{l}\nEstablish incident response plans for third-party GAI technologies: Align incident \\\\\nresponse plans with impacts enumerated in MAP 5.1; Communicate third-party \\\\\nGAI incident response plans to all relevant AI Actors; Define ownership of GAI \\\\\nincident response functions; Rehearse third-party GAI incident response plans at \\\\\na regular cadence; Improve incident response plans based on retrospective \\\\\nlearning; Review incident response plans for alignment with relevant breach \\\\\nreporting, data protection, data privacy, or other laws. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nData Privacy; Human-AI \\\\\nConfiguration; Information \\\\\nSecurity; Value Chain and \\\\\nComponent Integration; Harmful \\\\\nBias and Homogenization \\\\\n\\end{tabular} \\\\\n\\hline\nGV-6.2-004 & \\begin{tabular}{l}\nEstablish policies and procedures for continuous monitoring of third-party GAI \\\\\nsystems in deployment. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nValue Chain and Component \\\\\nIntegration \\\\\n\\end{tabular} \\\\\n\\hline\nGV-6.2-005 & \\begin{tabular}{l}\nEstablish policies and procedures that address GAI data redundancy, including \\\\\nmodel weights and other system artifacts. \\\\\n\\end{tabular} & Harmful Bias and Homogenization \\\\\n\\hline\nGV-6.2-006 & \\begin{tabular}{l}\nEstablish policies and procedures to test and manage risks related to rollover and \\\\\nfallback technologies for GAI systems, acknowledging that rollover and fallback \\\\\nmay include manual processing. \\\\\n\\end{tabular} & Information Integrity \\\\\n\\hline\nGV-6.2-007 & \\begin{tabular}{l}\nReview vendor contracts and avoid arbitrary or capricious termination of critical \\\\\nGAI technologies or vendor services and non-standard terms that may amplify or \\\\\ndefer liability in unexpected ways and/or contribute to unauthorized data \\\\\ncollection by vendors or third-parties (e.g., secondary data use). Consider: Clear \\\\', '\\hline\nGV-6.2-007 & \\begin{tabular}{l}\nReview vendor contracts and avoid arbitrary or capricious termination of critical \\\\\nGAI technologies or vendor services and non-standard terms that may amplify or \\\\\ndefer liability in unexpected ways and/or contribute to unauthorized data \\\\\ncollection by vendors or third-parties (e.g., secondary data use). Consider: Clear \\\\\nassignment of liability and responsibility for incidents, GAI system changes over \\\\\ntime (e.g., fine-tuning, drift, decay); Request: Notification and disclosure for \\\\\nserious incidents arising from third-party data and systems; Service Level \\\\\nAgreements (SLAs) in vendor contracts that address incident response, response \\\\\ntimes, and availability of critical support. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-Al Configuration; \\\\\nInformation Security; Value Chain \\\\\nand Component Integration \\\\\n\\end{tabular} \\\\\n\\hline\n\\multicolumn{3}{|l|}{AI Actor Tasks: AI Deployment, Operation and Monitoring, TEVV, Third-party entities} \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","To prevent unauthorized data collection and ensure incident response for third-party GAI technologies, it is key to review vendor contracts and avoid arbitrary or capricious termination of critical GAI technologies or vendor services. Additionally, it is important to avoid non-standard terms that may amplify or defer liability in unexpected ways and/or contribute to unauthorized data collection by vendors or third-parties (e.g., secondary data use). Clear assignment of liability and responsibility for incidents, GAI system changes over time (e.g., fine-tuning, drift, decay) should be considered. Furthermore, requesting notification and disclosure for serious incidents arising from third-party data and systems, and including Service Level Agreements (SLAs) in vendor contracts that address incident response, response times, and availability of critical support are also essential.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
87,What were the 4 main points the GAI PWG considered for NIST's GAI risk mgmt?,"['\\footnotetext{${ }^{1}$ EO 14110 defines Generative AI as ""the class of AI models that emulate the structure and characteristics of input data in order to generate derived synthetic content. This can include images, videos, audio, text, and other digital content."" While not all GAI is derived from foundation models, for purposes of this document, GAI generally refers to generative foundation models. The foundation model subcategory of ""dual-use foundation models"" is defined by EO 14110 as ""an AI model that is trained on broad data; generally uses self-supervision; contains at least tens of billions of parameters; is applicable across a wide range of contexts.""\\\\\n${ }^{2}$ This profile was developed per Section 4.1(a)(i)(A) of EO 14110, which directs the Secretary of Commerce, acting through the Director of the National Institute of Standards and Technology (NIST), to develop a companion resource to the AI RMF, NIST AI 100-1, for generative AI.\n}\nThis work was informed by public feedback and consultations with diverse stakeholder groups as part of NIST\'s\\\\\nGenerative AI Public Working Group (GAI PWG). The GAI PWG was an open, transparent, and collaborative\\\\\nprocess, facilitated via a virtual workspace, to obtain multistakeholder input on GAI risk management and to\\\\\ninform NIST\'s approach.\\\\\nThe focus of the GAI PWG was limited to four primary considerations relevant to GAI: Governance, Content\\\\\nProvenance, Pre-deployment Testing, and Incident Disclosure (further described in Appendix A). As such, the\\\\\nsuggested actions in this document primarily address these considerations.\\\\\nFuture revisions of this profile will include additional AI RMF subcategories, risks, and suggested actions based\\\\\non additional considerations of GAI as the space evolves and empirical evidence indicates additional risks. A\\\\\nglossary of terms pertinent to GAI risk management will be developed and hosted on NIST\'s Trustworthy \\&\\\\', ""Future revisions of this profile will include additional AI RMF subcategories, risks, and suggested actions based\\\\\non additional considerations of GAI as the space evolves and empirical evidence indicates additional risks. A\\\\\nglossary of terms pertinent to GAI risk management will be developed and hosted on NIST's Trustworthy \\&\\\\\nResponsible AI Resource Center (AIRC), and added to The Language of Trustworthy Al: An In-Depth Glossary of\\\\\nTerms.\\\\\nThis document was also informed by public comments and consultations from several Requests for Information.\\\\\nThis work was informed by public feedback and consultations with diverse stakeholder groups as part of NIST's Generative AI Public Working Group (GAI PWG). The GAI PWG was an open, transparent, and collaborative process, facilitated via a virtual workspace, to obtain multistakeholder input on GAI risk management and to inform NIST's approach.\nThe focus of the GAI PWG was limited to four primary considerations relevant to GAI: Governance, Content Provenance, Pre-deployment Testing, and Incident Disclosure (further described in Appendix A). As such, the suggested actions in this document primarily address these considerations.""]","The four primary considerations relevant to GAI that the GAI PWG focused on were Governance, Content Provenance, Pre-deployment Testing, and Incident Disclosure.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
88,"How do orgs use AI RMF profiles to manage dual-use Gen AI risks in governance, content provenance, pre-deployment testing, and incident disclosure?","['A profile is an implementation of the AI RMF functions, categories, and subcategories for a specific setting, application, or technology - in this case, Generative AI (GAI) - based on the requirements, risk tolerance, and resources of the Framework user. AI RMF profiles assist organizations in deciding how to best manage Al risks in a manner that is well-aligned with their goals, considers legal/regulatory requirements and best practices, and reflects risk management priorities. Consistent with other AI RMF profiles, this profile offers insights into how risk can be managed across various stages of the Al lifecycle and for GAI as a technology.\n\nAs GAI covers risks of models or applications that can be used across use cases or sectors, this document is an AI RMF cross-sectoral profile. Cross-sectoral profiles can be used to govern, map, measure, and manage risks associated with activities or business processes common across sectors, such as the use of large language models (LLMs), cloud-based services, or acquisition.\n\nThis document defines risks that are novel to or exacerbated by the use of GAI. After introducing and describing these risks, the document provides a set of suggested actions to help organizations govern, map, measure, and manage these risks.', '\\footnotetext{${ }^{1}$ EO 14110 defines Generative AI as ""the class of AI models that emulate the structure and characteristics of input data in order to generate derived synthetic content. This can include images, videos, audio, text, and other digital content."" While not all GAI is derived from foundation models, for purposes of this document, GAI generally refers to generative foundation models. The foundation model subcategory of ""dual-use foundation models"" is defined by EO 14110 as ""an AI model that is trained on broad data; generally uses self-supervision; contains at least tens of billions of parameters; is applicable across a wide range of contexts.""\\\\\n${ }^{2}$ This profile was developed per Section 4.1(a)(i)(A) of EO 14110, which directs the Secretary of Commerce, acting through the Director of the National Institute of Standards and Technology (NIST), to develop a companion resource to the AI RMF, NIST AI 100-1, for generative AI.\n}\nThis work was informed by public feedback and consultations with diverse stakeholder groups as part of NIST\'s\\\\\nGenerative AI Public Working Group (GAI PWG). The GAI PWG was an open, transparent, and collaborative\\\\\nprocess, facilitated via a virtual workspace, to obtain multistakeholder input on GAI risk management and to\\\\\ninform NIST\'s approach.\\\\\nThe focus of the GAI PWG was limited to four primary considerations relevant to GAI: Governance, Content\\\\\nProvenance, Pre-deployment Testing, and Incident Disclosure (further described in Appendix A). As such, the\\\\\nsuggested actions in this document primarily address these considerations.\\\\\nFuture revisions of this profile will include additional AI RMF subcategories, risks, and suggested actions based\\\\\non additional considerations of GAI as the space evolves and empirical evidence indicates additional risks. A\\\\\nglossary of terms pertinent to GAI risk management will be developed and hosted on NIST\'s Trustworthy \\&\\\\']","Organizations use AI RMF profiles to manage dual-use Generative AI risks by following suggested actions in governance, content provenance, pre-deployment testing, and incident disclosure. These actions help organizations govern, map, measure, and manage risks associated with Generative AI across various stages of the AI lifecycle.",multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
89,How does the 2022 WH roadmap tackle AI info integrity and privacy issues?,"['The White House (2022) Roadmap for Researchers on Priorities Related to Information Integrity Research and Development. \\href{https://www.whitehouse.gov/wp-content/uploads/2022/12/RoadmapInformation-Integrity-RD-2022.pdf}{https://www.whitehouse.gov/wp-content/uploads/2022/12/RoadmapInformation-Integrity-RD-2022.pdf}?\n\nThiel, D. (2023) Investigation Finds AI Image Generation Models Trained on Child Abuse. Stanford Cyber Policy Center. \\href{https://cyber.fsi.stanford.edu/news/investigation-finds-ai-image-generation-modelstrained-child-abuse}{https://cyber.fsi.stanford.edu/news/investigation-finds-ai-image-generation-modelstrained-child-abuse}\n\nTirrell, L. (2017) Toxic Speech: Toward an Epidemiology of Discursive Harm. Philosophical Topics, 45(2), 139-162. \\href{https://www.jstor.org/stable/26529441}{https://www.jstor.org/stable/26529441}\n\nTufekci, Z. (2015) Algorithmic Harms Beyond Facebook and Google: Emergent Challenges of Computational Agency. Colorado Technology Law Journal. \\href{https://ctlj.colorado.edu/wpcontent/uploads/2015/08/Tufekci-final.pdf}{https://ctlj.colorado.edu/wpcontent/uploads/2015/08/Tufekci-final.pdf}\n\nTurri, V. et al. (2023) Why We Need to Know More: Exploring the State of Al Incident Documentation Practices. AAAI/ACM Conference on AI, Ethics, and Society. \\href{https://dl.acm.org/doi/fullHtml/10.1145/3600211.3604700}{https://dl.acm.org/doi/fullHtml/10.1145/3600211.3604700}\n\nUrbina, F. et al. (2022) Dual use of artificial-intelligence-powered drug discovery. Nature Machine Intelligence. \\href{https://www.nature.com/articles/s42256-022-00465-9}{https://www.nature.com/articles/s42256-022-00465-9}\n\nWang, X. et al. (2023) Energy and Carbon Considerations of Fine-Tuning BERT. ACL Anthology. \\href{https://aclanthology.org/2023.findings-emnlp.607.pdf}{https://aclanthology.org/2023.findings-emnlp.607.pdf}', 'Solaiman, I. et al. (2023) The Gradient of Generative AI Release: Methods and Considerations. arXiv. \\href{https://arxiv.org/abs/2302.04844}{https://arxiv.org/abs/2302.04844}\n\nStaab, R. et al. (2023) Beyond Memorization: Violating Privacy via Inference With Large Language Models. arXiv. \\href{https://arxiv.org/pdf/2310.07298}{https://arxiv.org/pdf/2310.07298}\n\nStanford, S. et al. (2023) Whose Opinions Do Language Models Reflect? arXiv.\\\\\n\\href{https://arxiv.org/pdf/2303.17548}{https://arxiv.org/pdf/2303.17548}\\\\\nStrubell, E. et al. (2019) Energy and Policy Considerations for Deep Learning in NLP. arXiv. \\href{https://arxiv.org/pdf/1906.02243}{https://arxiv.org/pdf/1906.02243}\n\nThe White House (2016) Circular No. A-130, Managing Information as a Strategic Resource. \\href{https://www.whitehouse.gov/wp-}{https://www.whitehouse.gov/wp-}\\\\\ncontent/uploads/legacy drupal files/omb/circulars/A130/a130revised.pdf\\\\\nThe White House (2023) Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. \\href{https://www.whitehouse.gov/briefing-room/presidentialactions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-ofartificial-intelligence/}{https://www.whitehouse.gov/briefing-room/presidentialactions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-ofartificial-intelligence/}\n\nThe White House (2022) Roadmap for Researchers on Priorities Related to Information Integrity Research and Development. \\href{https://www.whitehouse.gov/wp-content/uploads/2022/12/RoadmapInformation-Integrity-RD-2022.pdf}{https://www.whitehouse.gov/wp-content/uploads/2022/12/RoadmapInformation-Integrity-RD-2022.pdf}?']",The answer to given question is not present in context,multi_context,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
90,How do distillation & compression affect GAI's environmental impact & bias?,"['Methods for creating smaller versions of trained models, such as model distillation or compression, could reduce environmental impacts at inference time, but training and tuning such models may still contribute to their environmental impacts. Currently there is no agreed upon method to estimate environmental impacts from GAI.\n\nTrustworthy AI Characteristics: Accountable and Transparent, Safe\n\\subsection*{2.6. Harmful Bias and Homogenization}\nBias exists in many forms and can become ingrained in automated systems. Al systems, including GAI systems, can increase the speed and scale at which harmful biases manifest and are acted upon, potentially perpetuating and amplifying harms to individuals, groups, communities, organizations, and society. For example, when prompted to generate images of CEOs, doctors, lawyers, and judges, current text-to-image models underrepresent women and/or racial minorities, and people with disabilities. Image generator models have also produced biased or stereotyped output for various demographic groups and have difficulty producing non-stereotyped content even when the prompt specifically requests image features that are inconsistent with the stereotypes. Harmful bias in GAI models, which may stem from their training data, can also cause representational harms or perpetuate or exacerbate bias based on race, gender, disability, or other protected classes.']","Methods for creating smaller versions of trained models, such as model distillation or compression, could reduce environmental impacts at inference time, but training and tuning such models may still contribute to their environmental impacts.",reasoning,[{'source': 'data/nist_ai.tex'}],True
91,Why might GAI validation fail in games and tests?,"['\\section*{Limitations of Current Pre-deployment Test Approaches}\nCurrently available pre-deployment TEVV processes used for GAI applications may be inadequate, nonsystematically applied, or fail to reflect or mismatched to deployment contexts. For example, the anecdotal testing of GAI system capabilities through video games or standardized tests designed for humans (e.g., intelligence tests, professional licensing exams) does not guarantee GAI system validity or reliability in those domains. Similarly, jailbreaking or prompt engineering tests may not systematically assess validity or reliability risks.']",GAI validation might fail in games and tests because anecdotal testing of GAI system capabilities through video games or standardized tests designed for humans does not guarantee GAI system validity or reliability in those domains.,reasoning,[{'source': 'data/nist_ai.tex'}],True
92,Which contract terms prevent liability and data misuse in GAI services?,"['\\hline\nGV-6.2-007 & \\begin{tabular}{l}\nReview vendor contracts and avoid arbitrary or capricious termination of critical \\\\\nGAI technologies or vendor services and non-standard terms that may amplify or \\\\\ndefer liability in unexpected ways and/or contribute to unauthorized data \\\\\ncollection by vendors or third-parties (e.g., secondary data use). Consider: Clear \\\\\nassignment of liability and responsibility for incidents, GAI system changes over \\\\\ntime (e.g., fine-tuning, drift, decay); Request: Notification and disclosure for \\\\\nserious incidents arising from third-party data and systems; Service Level \\\\\nAgreements (SLAs) in vendor contracts that address incident response, response \\\\\ntimes, and availability of critical support. \\\\\n\\end{tabular} & \\begin{tabular}{l}\nHuman-Al Configuration; \\\\\nInformation Security; Value Chain \\\\\nand Component Integration \\\\\n\\end{tabular} \\\\\n\\hline\n\\multicolumn{3}{|l|}{AI Actor Tasks: AI Deployment, Operation and Monitoring, TEVV, Third-party entities} \\\\\n\\hline\n\\end{tabular}\n\\end{center}']","The contract terms that prevent liability and data misuse in GAI services include avoiding arbitrary or capricious termination of critical GAI technologies or vendor services, avoiding non-standard terms that may amplify or defer liability in unexpected ways, and preventing unauthorized data collection by vendors or third-parties. Additionally, clear assignment of liability and responsibility for incidents, notification and disclosure for serious incidents arising from third-party data and systems, and Service Level Agreements (SLAs) that address incident response, response times, and availability of critical support are also important.",reasoning,[{'source': 'data/nist_ai.tex'}],True
93,How do SBOMs & SLAs reduce IP risks in 3rd-party GAI integrations?,"['Third party GAI integrations may give rise to increased intellectual property, data privacy, or information security risks, pointing to the need for clear guidelines for transparency and risk management regarding the collection and use of third-party data for model inputs. Organizations may consider varying risk controls for foundation models, fine-tuned models, and embedded tools, enhanced processes for interacting with external GAI technologies or service providers. Organizations can apply standard or existing risk controls and processes to proprietary or open-source GAI technologies, data, and third-party service providers, including acquisition and procurement due diligence, requests for software bills of materials (SBOMs), application of service level agreements (SLAs), and statement on standards for attestation engagement (SSAE) reports to help with third-party transparency and risk management for GAI systems.\n\\section*{A.1.4. Pre-Deployment Testing}\n\\section*{Overview}\nThe diverse ways and contexts in which GAI systems may be developed, used, and repurposed complicates risk mapping and pre-deployment measurement efforts. Robust test, evaluation, validation, and verification (TEVV) processes can be iteratively applied - and documented - in early stages of the AI lifecycle and informed by representative AI Actors (see Figure 3 of the AI RMF). Until new and rigorous\\\\\nearly lifecycle TEVV approaches are developed and matured for GAI, organizations may use recommended ""pre-deployment testing"" practices to measure performance, capabilities, limits, risks, and impacts. This section describes risk measurement and estimation as part of pre-deployment TEVV, and examines the state of play for pre-deployment testing methodologies.\n\\section*{Limitations of Current Pre-deployment Test Approaches}']",The answer to given question is not present in context,reasoning,[{'source': 'data/nist_ai.tex'}],True
94,How might GAI errors cause wrong diagnoses in healthcare?,"[""Risks from confabulations may arise when users believe false content - often due to the confident nature of the response - leading users to act upon or promote the false information. This poses a challenge for many real-world applications, such as in healthcare, where a confabulated summary of patient information reports could cause doctors to make incorrect diagnoses and/or recommend the wrong treatments. Risks of confabulated content may be especially important to monitor when integrating GAI into applications involving consequential decision making.\n\nGAI outputs may also include confabulated logic or citations that purport to justify or explain the system's answer, which may further mislead humans into inappropriately trusting the system's output. For instance, LLMs sometimes provide logical steps for how they arrived at an answer even when the answer itself is incorrect. Similarly, an LLM could falsely assert that it is human or has human traits, potentially deceiving humans into believing they are speaking with another human.\n\nThe extent to which humans can be deceived by LLMs, the mechanisms by which this may occur, and the potential risks from adversarial prompting of such behavior are emerging areas of study. Given the wide range of downstream impacts of GAI, it is difficult to estimate the downstream scale and impact of confabulations.\n\nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Valid and Reliable, Explainable and Interpretable\n\\subsection*{2.3. Dangerous, Violent, or Hateful Content}\nGAI systems can produce content that is inciting, radicalizing, or threatening, or that glorifies violence, with greater ease and scale than other technologies. LLMs have been reported to generate dangerous or violent recommendations, and some models have generated actionable instructions for dangerous or""]","GAI errors, such as confabulated summaries of patient information reports, could cause doctors to make incorrect diagnoses and/or recommend the wrong treatments.",reasoning,[{'source': 'data/nist_ai.tex'}],True
95,How do test dataset label errors impact model selection?,"['Errors in third-party GAI components can also have downstream impacts on accuracy and robustness. For example, test datasets commonly used to benchmark or validate models can contain label errors. Inaccuracies in these labels can impact the ""stability"" or robustness of these benchmarks, which many GAI practitioners consider during the model selection process.\n\nTrustworthy AI Characteristics: Accountable and Transparent, Explainable and Interpretable, Fair with Harmful Bias Managed, Privacy Enhanced, Safe, Secure and Resilient, Valid and Reliable\n\\section*{3. Suggested Actions to Manage GAI Risks}\nThe following suggested actions target risks unique to or exacerbated by GAI.\\\\\nIn addition to the suggested actions below, Al risk management activities and actions set forth in the AI RMF 1.0 and Playbook are already applicable for managing GAI risks. Organizations are encouraged to apply the activities suggested in the AI RMF and its Playbook when managing the risk of GAI systems.\n\nImplementation of the suggested actions will vary depending on the type of risk, characteristics of GAI systems, stage of the GAI lifecycle, and relevant AI actors involved.']","Test dataset label errors can impact the 'stability' or robustness of benchmarks, which many GAI practitioners consider during the model selection process.",reasoning,[{'source': 'data/nist_ai.tex'}],True
96,How do biased data and performance gaps link to GAI risks?,"['To guide organizations in identifying and managing GAI risks, a set of risks unique to or exacerbated by the development and use of GAI are defined below. ${ }^{5}$ Each risk is labeled according to the outcome, object, or source of the risk (i.e., some are risks ""to"" a subject or domain and others are risks ""of"" or ""from"" an issue or theme). These risks provide a lens through which organizations can frame and execute risk management efforts. To help streamline risk management efforts, each risk is mapped in Section 3 (as well as in tables in Appendix B) to relevant Trustworthy AI Characteristics identified in the AI RMF.\n\\footnotetext{${ }^{5}$ These risks can be further categorized by organizations depending on their unique approaches to risk definition and management. One possible way to further categorize these risks, derived in part from the UK\'s International Scientific Report on the Safety of Advanced AI, could be: 1) Technical / Model risks (or risk from malfunction): Confabulation; Dangerous or Violent Recommendations; Data Privacy; Value Chain and Component Integration; Harmful Bias, and Homogenization; 2) Misuse by humans (or malicious use): CBRN Information or Capabilities; Data Privacy; Human-Al Configuration; Obscene, Degrading, and/or Abusive Content; Information Integrity; Information Security; 3) Ecosystem / societal risks (or systemic risks): Data Privacy; Environmental; Intellectual Property. We also note that some risks are cross-cutting between these categories.\n}\\begin{enumerate}\n  \\item CBRN Information or Capabilities: Eased access to or synthesis of materially nefarious information or design capabilities related to chemical, biological, radiological, or nuclear (CBRN) weapons or other dangerous materials or agents.\n  \\item Confabulation: The production of confidently stated but erroneous or false content (known colloquially as ""hallucinations"" or ""fabrications"") by which users may be misled or deceived. ${ }^{6}$', '\\item Confabulation: The production of confidently stated but erroneous or false content (known colloquially as ""hallucinations"" or ""fabrications"") by which users may be misled or deceived. ${ }^{6}$\n  \\item Dangerous, Violent, or Hateful Content: Eased production of and access to violent, inciting, radicalizing, or threatening content as well as recommendations to carry out self-harm or conduct illegal activities. Includes difficulty controlling public exposure to hateful and disparaging or stereotyping content.\n  \\item Data Privacy: Impacts due to leakage and unauthorized use, disclosure, or de-anonymization of biometric, health, location, or other personally identifiable information or sensitive data. ${ }^{7}$\n  \\item Environmental Impacts: Impacts due to high compute resource utilization in training or operating GAI models, and related outcomes that may adversely impact ecosystems.\n  \\item Harmful Bias or Homogenization: Amplification and exacerbation of historical, societal, and systemic biases; performance disparities ${ }^{8}$ between sub-groups or languages, possibly due to non-representative training data, that result in discrimination, amplification of biases, or incorrect presumptions about performance; undesired homogeneity that skews system or model outputs, which may be erroneous, lead to ill-founded decision-making, or amplify harmful biases.\n  \\item Human-AI Configuration: Arrangements of or interactions between a human and an AI system which can result in the human inappropriately anthropomorphizing GAI systems or experiencing algorithmic aversion, automation bias, over-reliance, or emotional entanglement with GAI systems.\n  \\item Information Integrity: Lowered barrier to entry to generate and support the exchange and consumption of content which may not distinguish fact from opinion or fiction or acknowledge uncertainties, or could be leveraged for large-scale dis- and mis-information campaigns.']","Biased data and performance gaps link to GAI risks through the amplification and exacerbation of historical, societal, and systemic biases. These can result in performance disparities between sub-groups or languages, possibly due to non-representative training data, leading to discrimination, amplification of biases, or incorrect presumptions about performance.",reasoning,"[{'source': 'data/nist_ai.tex'}, {'source': 'data/nist_ai.tex'}]",True
97,How could GAI content mislead doctors?,"[""Risks from confabulations may arise when users believe false content - often due to the confident nature of the response - leading users to act upon or promote the false information. This poses a challenge for many real-world applications, such as in healthcare, where a confabulated summary of patient information reports could cause doctors to make incorrect diagnoses and/or recommend the wrong treatments. Risks of confabulated content may be especially important to monitor when integrating GAI into applications involving consequential decision making.\n\nGAI outputs may also include confabulated logic or citations that purport to justify or explain the system's answer, which may further mislead humans into inappropriately trusting the system's output. For instance, LLMs sometimes provide logical steps for how they arrived at an answer even when the answer itself is incorrect. Similarly, an LLM could falsely assert that it is human or has human traits, potentially deceiving humans into believing they are speaking with another human.\n\nThe extent to which humans can be deceived by LLMs, the mechanisms by which this may occur, and the potential risks from adversarial prompting of such behavior are emerging areas of study. Given the wide range of downstream impacts of GAI, it is difficult to estimate the downstream scale and impact of confabulations.\n\nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Valid and Reliable, Explainable and Interpretable\n\\subsection*{2.3. Dangerous, Violent, or Hateful Content}\nGAI systems can produce content that is inciting, radicalizing, or threatening, or that glorifies violence, with greater ease and scale than other technologies. LLMs have been reported to generate dangerous or violent recommendations, and some models have generated actionable instructions for dangerous or""]","GAI content could mislead doctors by providing confabulated summaries of patient information reports, which could cause doctors to make incorrect diagnoses and/or recommend the wrong treatments.",reasoning,[{'source': 'data/nist_ai.tex'}],True
98,How do GAI predictions cause hallucinations?,"['While some of these described capabilities lie beyond the reach of existing GAI tools, ongoing assessments of this risk would be enhanced by monitoring both the ability of AI tools to facilitate CBRN weapons planning and GAI systems\' connection or access to relevant data and tools.\n\nTrustworthy AI Characteristic: Safe, Explainable and Interpretable\n\\subsection*{2.2. Confabulation}\n""Confabulation"" refers to a phenomenon in which GAI systems generate and confidently present erroneous or false content in response to prompts. Confabulations also include generated outputs that diverge from the prompts or other input or that contradict previously generated statements in the same context. These phenomena are colloquially also referred to as ""hallucinations"" or ""fabrications.""\n\nConfabulations can occur across GAI outputs and contexts. ${ }^{9,10}$ Confabulations are a natural result of the way generative models are designed: they generate outputs that approximate the statistical distribution of their training data; for example, LLMs predict the next token or word in a sentence or phrase. While such statistical prediction can produce factually accurate and consistent outputs, it can also produce outputs that are factually inaccurate or internally inconsistent. This dynamic is particularly relevant when it comes to open-ended prompts for long-form responses and in domains which require highly contextual and/or domain expertise.']","GAI predictions cause hallucinations because generative models generate outputs that approximate the statistical distribution of their training data. For example, LLMs predict the next token or word in a sentence or phrase. While such statistical prediction can produce factually accurate and consistent outputs, it can also produce outputs that are factually inaccurate or internally inconsistent.",reasoning,[{'source': 'data/nist_ai.tex'}],True
99,How to gather early AI feedback w/o field testing?,"['\\item Al Red-teaming: A structured testing exercise used to probe an AI system to find flaws and vulnerabilities such as inaccurate, harmful, or discriminatory outputs, often in a controlled environment and in collaboration with system developers.\n\\end{itemize}\nInformation gathered from structured public feedback can inform design, implementation, deployment approval, maintenance, or decommissioning decisions. Results and insights gleaned from these exercises can serve multiple purposes, including improving data quality and preprocessing, bolstering governance decision making, and enhancing system documentation and debugging practices. When implementing feedback activities, organizations should follow human subjects research requirements and best practices such as informed consent and subject compensation.\n\\section*{Participatory Engagement Methods}\nOn an ad hoc or more structured basis, organizations can design and use a variety of channels to engage external stakeholders in product development or review. Focus groups with select experts can provide feedback on a range of issues. Small user studies can provide feedback from representative groups or populations. Anonymous surveys can be used to poll or gauge reactions to specific features. Participatory engagement methods are often less structured than field testing or red teaming, and are more commonly used in early stages of AI or product development.\n\\section*{Field Testing}\nField testing involves structured settings to evaluate risks and impacts and to simulate the conditions under which the GAI system will be deployed. Field style tests can be adapted from a focus on user preferences and experiences towards Al risks and impacts - both negative and positive. When carried out with large groups of users, these tests can provide estimations of the likelihood of risks and impacts in real world interactions.']","Organizations can gather early AI feedback without field testing by using participatory engagement methods. These methods include designing and using various channels to engage external stakeholders in product development or review, such as focus groups with select experts, small user studies with representative groups or populations, and anonymous surveys to gauge reactions to specific features.",reasoning,[{'source': 'data/nist_ai.tex'}],True
